{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenge 1**"
      ],
      "metadata": {
        "id": "xvdtBiPrGllf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "A baseline reference implementation is given. Implement your version to replace it. Rely on the provided host-side function to check the correctness of results.\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. Meanwhile, you may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ],
      "metadata": {
        "id": "xcDOQnQMGwLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General rules and advice:\n",
        "- groups are of 3 members at most, that should, for as much as possible, equally contribute to the project.\n",
        "- you automagically get ~3 points in the 1st part of the exam, taking the form of 1-2 questions that you will be allowed to skip.\n",
        "- deadline is 1 week (as of this writing, you will need to submit your work before october 23 at 23:59).\n",
        "- submissions are to be made on WeBeep, where you need to upload a downloaded copy of this notebook (.ipynb file); for groups of multiple people, it's enough for one member to submit the file in the assignment, other members shall simply write their group's name in their submission, we will then infer groups from what you write in the report section.\n",
        "- your code needs to work here on Colab with the T4 runtime.\n",
        "- do not alter code sections delimited by \"===\"s in the final submission.\n",
        "- we will change around matrix sizes arbitrarily while evaluating your work, so make sure to cover all edge cases and take care that your code is scalable (e.g. execution time grows as expected when doubling all dimensions).\n",
        "- you can get the maximum grade just by using what was discussed during lectures or is present in the glossary shown during exercise sessions; still, if you wanna have \"more fun\" this guide is your best friend https://docs.nvidia.com/cuda/cuda-c-best-practices-guide.\n",
        "- a piece of code that works is better than a supposedly faster piece of code that doesn't, so don't go overboard, but be ambitious.\n",
        "- use LLMs (ChatGPT and friends) responsibly; the purpose of this challenge is for you to get your hands dirty and build up confidence in writing parallel code through trial and error. Having an LLM write your code may get you the challenge's points (unless it's so blatant that we notice), but won't lead you to learn anything and the next time you see some parallel code your mind goes blank. If you wack your head at the problem instead, and solve it, the solution will stick in the back of your mind for a long time. Similarly, if despite pushing yourself you can't find \"that damn bug\", then asking an LLM is fine, so long as you tried first by yourself and just say \"ahhhhhh, so that what it was!\" upon having the LLM help you out. Long story short, AI is fine so long as it's a tool you **learn from** and **not** one you **blindly lean on**.\n",
        "\n",
        "If you need help or anything, please drop us an email:\n",
        "- Dr. M. Ronzani: marco.ronzani@polimi.it\n",
        "- Prof. F. Ferrandi: fabrizio.ferrandi@polimi.it"
      ],
      "metadata": {
        "id": "t0olg8POKfsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab Setup**"
      ],
      "metadata": {
        "id": "iHGYx97cJqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITAYKD7MGcmH"
      },
      "outputs": [],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code**"
      ],
      "metadata": {
        "id": "F9rbKG58Jyw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Original\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < m && col < n) {\n",
        "    for (int b = 0; b < batch; b++) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(16,16); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "6Ys4rptyJ5EJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Group\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "#define BATCH_PER_THREAD 4\n",
        "\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch, int batchesPerBlock) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  int batchStart = blockIdx.z * batchesPerBlock;\n",
        "  int batchEnd   = min(batchStart + batchesPerBlock, batch);\n",
        "\n",
        "   if (row < m && col < n) {\n",
        "        for (int b = batchStart; b < batchEnd; ++b) {\n",
        "            float value = 0.0f;\n",
        "            for (int i = 0; i < k; ++i) {\n",
        "                float a = M[row * k + i];\n",
        "                float c = N[b * (k * n) + i * n + col];\n",
        "                value += a * c;\n",
        "            }\n",
        "            P[b * (m * n) + row * n + col] = value;\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "  const int batchesPerBlock = 1;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(32, 32); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, (batch + batchesPerBlock - 1) / batchesPerBlock);\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch, batchesPerBlock);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "0q_YQCpfCTH2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Claude\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// Optimized kernel with tiling and shared memory\n",
        "#define TILE_DIM 32\n",
        "#define BATCH_PER_THREAD 4\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  __shared__ float tileM[TILE_DIM][TILE_DIM];\n",
        "  __shared__ float tileN[TILE_DIM][TILE_DIM];\n",
        "\n",
        "  int tx = threadIdx.x;\n",
        "  int ty = threadIdx.y;\n",
        "\n",
        "  int row = blockIdx.y * TILE_DIM + ty;\n",
        "  int col = blockIdx.x * TILE_DIM + tx;\n",
        "\n",
        "  // Process multiple batches per thread\n",
        "  int batchStart = blockIdx.z * BATCH_PER_THREAD;\n",
        "  int batchEnd = min(batchStart + BATCH_PER_THREAD, batch);\n",
        "\n",
        "  // Accumulator for each batch\n",
        "  float acc[BATCH_PER_THREAD];\n",
        "  for (int i = 0; i < BATCH_PER_THREAD; i++) {\n",
        "    acc[i] = 0.0f;\n",
        "  }\n",
        "\n",
        "  // Tile across k dimension\n",
        "  int numTiles = (k + TILE_DIM - 1) / TILE_DIM;\n",
        "\n",
        "  for (int t = 0; t < numTiles; t++) {\n",
        "    // Load tile from M (shared across all batches)\n",
        "    int mCol = t * TILE_DIM + tx;\n",
        "    if (row < m && mCol < k) {\n",
        "      tileM[ty][tx] = M[row * k + mCol];\n",
        "    } else {\n",
        "      tileM[ty][tx] = 0.0f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Process each batch\n",
        "    for (int b_idx = 0; b_idx < (batchEnd - batchStart); b_idx++) {\n",
        "      int b = batchStart + b_idx;\n",
        "\n",
        "      // Load tile from N for this batch\n",
        "      int nRow = t * TILE_DIM + ty;\n",
        "      if (nRow < k && col < n) {\n",
        "        tileN[ty][tx] = N[b * (k * n) + nRow * n + col];\n",
        "      } else {\n",
        "        tileN[ty][tx] = 0.0f;\n",
        "      }\n",
        "\n",
        "      __syncthreads();\n",
        "\n",
        "      // Compute partial dot product\n",
        "      if (row < m && col < n) {\n",
        "        for (int i = 0; i < TILE_DIM; i++) {\n",
        "          acc[b_idx] += tileM[ty][i] * tileN[i][tx];\n",
        "        }\n",
        "      }\n",
        "\n",
        "      __syncthreads();\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Write results\n",
        "  if (row < m && col < n) {\n",
        "    for (int b_idx = 0; b_idx < (batchEnd - batchStart); b_idx++) {\n",
        "      int b = batchStart + b_idx;\n",
        "      P[b * (m * n) + row * n + col] = acc[b_idx];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  // Optimized launch configuration\n",
        "  dim3 blockSize(TILE_DIM, TILE_DIM);\n",
        "  dim3 numBlocks(\n",
        "    (n + TILE_DIM - 1) / TILE_DIM,\n",
        "    (m + TILE_DIM - 1) / TILE_DIM,\n",
        "    (batch + BATCH_PER_THREAD - 1) / BATCH_PER_THREAD\n",
        "  );\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        " // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "UYZ25WhBkvqO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Alone 1Ã¹\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__constant__ float Mc[163840];\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int b = blockIdx.z * blockDim.z + threadIdx.z;  // Add batch dimension\n",
        "\n",
        "  if (row < m && col < n && b < batch) {\n",
        "    float value = 0.0f;\n",
        "    for (int i = 0; i < k; i++) {\n",
        "      float a = Mc[row*k + i];\n",
        "      float c = N[b*(k*n) + i*n + col];\n",
        "      value += a * c;\n",
        "    }\n",
        "    P[b*(m*n) + row*n + col] = value;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpyToSymbol(Mc, M, sizeM * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(16, 16, 4); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, (batch + blockSize.z - 1) / blockSize.z);\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KUFhy5ygLwMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Alone 2\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// Tuned kernel: 32x32 tile in shared memory,\n",
        "// but 16x16 threads per block, each computing a 2x2 output fragment.\n",
        "#define TILE_MN 32\n",
        "#define THREADS_PER_DIM 16\n",
        "static_assert(TILE_MN % THREADS_PER_DIM == 0, \"Tile must be multiple of thread dim\");\n",
        "\n",
        "__global__ __launch_bounds__(THREADS_PER_DIM*THREADS_PER_DIM, 2)\n",
        "void batchedMatMul(const float* __restrict__ M,\n",
        "                   const float* __restrict__ N,\n",
        "                         float* __restrict__ P,\n",
        "                   int m, int k, int n, int batch)\n",
        "{\n",
        "  // Which batch do we work on\n",
        "  const int b = blockIdx.z;\n",
        "  if (b >= batch) return;\n",
        "\n",
        "  // Shared tiles\n",
        "  __shared__ float tileM[TILE_MN][TILE_MN];\n",
        "  __shared__ float tileN[TILE_MN][TILE_MN];\n",
        "\n",
        "  // Base pointers for this batch\n",
        "  const float* __restrict__ Nb = N + b * (k * n);\n",
        "  float* __restrict__ Pb = P + b * (m * n);\n",
        "\n",
        "  // 2x2 register-tiling per thread\n",
        "  const int row0 = blockIdx.y * TILE_MN + threadIdx.y;\n",
        "  const int row1 = row0 + THREADS_PER_DIM; // +16\n",
        "  const int col0 = blockIdx.x * TILE_MN + threadIdx.x;\n",
        "  const int col1 = col0 + THREADS_PER_DIM; // +16\n",
        "\n",
        "  float c00 = 0.f, c01 = 0.f, c10 = 0.f, c11 = 0.f;\n",
        "\n",
        "  const int numTiles = (k + TILE_MN - 1) / TILE_MN;\n",
        "\n",
        "  for (int t = 0; t < numTiles; ++t) {\n",
        "    const int kBase = t * TILE_MN;\n",
        "\n",
        "    // --- Load tile of M (two rows per thread) ---\n",
        "    int mCol = kBase + threadIdx.x; // along K\n",
        "    // first half of tile rows\n",
        "    if (row0 < m && mCol < k) {\n",
        "      tileM[threadIdx.y][threadIdx.x] = M[row0 * k + mCol];\n",
        "    } else {\n",
        "      tileM[threadIdx.y][threadIdx.x] = 0.f;\n",
        "    }\n",
        "    // second half of tile rows (+16)\n",
        "    if (row1 < m && mCol < k) {\n",
        "      tileM[threadIdx.y + THREADS_PER_DIM][threadIdx.x] = M[row1 * k + mCol];\n",
        "    } else {\n",
        "      tileM[threadIdx.y + THREADS_PER_DIM][threadIdx.x] = 0.f;\n",
        "    }\n",
        "\n",
        "    // --- Load tile of N (two cols per thread) ---\n",
        "    int nRow = kBase + threadIdx.y; // along K\n",
        "    // first half of tile cols\n",
        "    if (nRow < k && col0 < n) {\n",
        "      tileN[threadIdx.y][threadIdx.x] = Nb[nRow * n + col0];\n",
        "    } else {\n",
        "      tileN[threadIdx.y][threadIdx.x] = 0.f;\n",
        "    }\n",
        "    // second half of tile cols (+16)\n",
        "    if (nRow < k && col1 < n) {\n",
        "      tileN[threadIdx.y][threadIdx.x + THREADS_PER_DIM] = Nb[nRow * n + col1];\n",
        "    } else {\n",
        "      tileN[threadIdx.y][threadIdx.x + THREADS_PER_DIM] = 0.f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // --- Compute this tile's contribution (fully unrolled for 32) ---\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < TILE_MN; ++i) {\n",
        "      float a0 = tileM[threadIdx.y][i];\n",
        "      float a1 = tileM[threadIdx.y + THREADS_PER_DIM][i];\n",
        "      float b0 = tileN[i][threadIdx.x];\n",
        "      float b1 = tileN[i][threadIdx.x + THREADS_PER_DIM];\n",
        "\n",
        "      c00 += a0 * b0;\n",
        "      c01 += a0 * b1;\n",
        "      c10 += a1 * b0;\n",
        "      c11 += a1 * b1;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // --- Write 2x2 results ---\n",
        "  if (row0 < m && col0 < n) Pb[row0 * n + col0] = c00;\n",
        "  if (row0 < m && col1 < n) Pb[row0 * n + col1] = c01;\n",
        "  if (row1 < m && col0 < n) Pb[row1 * n + col0] = c10;\n",
        "  if (row1 < m && col1 < n) Pb[row1 * n + col1] = c11;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d = nullptr;\n",
        "  float *N_d = nullptr;\n",
        "  float *P_d = nullptr;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  // No need to copy P to device; kernel overwrites all computed elements\n",
        "\n",
        "  // Grid/Block configuration: 16x16 threads; 32x32 tile per block\n",
        "  dim3 blockSize(THREADS_PER_DIM, THREADS_PER_DIM);  // 256 threads\n",
        "  dim3 numBlocks(\n",
        "    (n + TILE_MN - 1) / TILE_MN,   // x: columns\n",
        "    (m + TILE_MN - 1) / TILE_MN,   // y: rows\n",
        "    batch                          // z: batches (parallelize!)\n",
        "  );\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YwLDesFkSPO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title small & medium matrices large batch baseline - a matrix sized 2d block for each batch\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "\n",
        "    if (row < m && col < n) {\n",
        "      for (int b = 0; b < batch; b++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(n, m);  // one block of size m Ã n Ã batch\n",
        "  int numBlocks = batch;\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Yb6EuDzf0wml",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title small matrices large batch 90% kernel time cut - single block operation in 3 dimensions with batches on z dimension\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "    int b   = threadIdx.z;  // use z dimension for batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(n, m, batch);  // one block of size m Ã n Ã batch\n",
        "  dim3 gridDim(1,1,1);\n",
        "  batchedMatMul<<<gridDim, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "CnDayy1Cb1xe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title medium matrices large batch - batch number of blocks distributed on grid z dimension\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "    int b   = blockIdx.z;  // use z dimension for batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(n, m);\n",
        "  dim3 gridDim(1, 1, batch);\n",
        "  batchedMatMul<<<gridDim, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UvxanHKbY2S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title medium matrices large batch - M loaded in shared memory (no benefit)\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(const float* M, const float* N, float* P,\n",
        "                   int m, int k, int n, int batch) {\n",
        "\n",
        "    __shared__ float Ms[1024];  // enough space for m*k elements (adjust if needed)\n",
        "\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "    int b   = blockIdx.z;  // batch index (same block, different z threads)\n",
        "\n",
        "    // Step 1. Cooperative load of M into shared memory\n",
        "    // Each thread participates in loading M once\n",
        "    for (int idx = threadIdx.y * blockDim.x + threadIdx.x;\n",
        "         idx < m*k;\n",
        "         idx += blockDim.x * blockDim.y)\n",
        "    {\n",
        "        Ms[idx] = M[idx];\n",
        "    }\n",
        "\n",
        "    __syncthreads();  // ensure M is fully loaded\n",
        "\n",
        "    // Step 2. Compute P for each batch\n",
        "    if (row < m && col < n && b < batch) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; ++i) {\n",
        "            float a = Ms[row*k + i];               // from shared memory\n",
        "            float c = N[b*(k*n) + i*n + col];      // from global memory\n",
        "            value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(n, m);  // one block of size m Ã n Ã batch\n",
        "  dim3 gridSize(1,1,batch);\n",
        "  batchedMatMul<<<gridSize, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XTHe4_l7PncW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title large matrices small batch - failure\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "\n",
        "    int row = threadIdx.x;\n",
        "    int col = blockIdx.y;\n",
        "    int b   = blockIdx.z;  // use z dimension for batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(m, 1, 1);\n",
        "  dim3 gridDim(1, n, batch);\n",
        "  batchedMatMul<<<gridDim, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s3c--uaSnwTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title large\n",
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < m && col < n) {\n",
        "    for (int b = 0; b < batch; b++) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(54,18); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "pwfwwAGI0IBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compile, Run, Profile**"
      ],
      "metadata": {
        "id": "t4eaFcePJ_8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "gXV_BmFYKM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "TlgkX-SPZhqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile:"
      ],
      "metadata": {
        "id": "SBn4aD0gKRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "VRLC6R5AKRTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
        "Checking results on CPU...\n",
        "All results matched, success!Generating '/tmp/nsys-report-3b93.qdstrm'\n",
        "[1/8] [========================100%] report1.nsys-rep\n",
        "[2/8] [========================100%] report1.sqlite\n",
        "[3/8] Executing 'nvtx_sum' stats report\n",
        "SKIPPED: /home/cuda/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
        "[4/8] Executing 'osrt_sum' stats report\n",
        "\n",
        " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
        " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
        "     98.8    5,701,475,935         66  86,385,999.0  100,135,831.5    57,103  284,139,110  43,718,945.9  poll                  \n",
        "      1.2       67,028,728        543     123,441.5       14,584.0       384   17,662,203   1,022,129.7  ioctl                 \n",
        "      0.0        1,897,438         31      61,207.7       11,773.0     7,961    1,199,774     212,278.6  mmap64                \n",
        "      0.0        1,108,245         10     110,824.5       56,247.0    25,249      382,985     116,560.1  sem_timedwait         \n",
        "      0.0          490,750          1     490,750.0      490,750.0   490,750      490,750           0.0  pthread_cond_wait     \n",
        "      0.0          430,390         49       8,783.5        7,673.0     1,866       29,380       4,288.5  open64                \n",
        "      0.0          237,317         40       5,932.9        3,314.5     1,415       33,907       6,802.4  fopen                 \n",
        "      0.0          167,330         15      11,155.3        5,610.0     1,530       57,251      13,956.3  mmap                  \n",
        "      0.0          107,897          2      53,948.5       53,948.5    47,654       60,243       8,901.8  pthread_create        \n",
        "      0.0           71,287         12       5,940.6        6,140.5     3,128        8,985       1,806.8  write                 \n",
        "      0.0           52,856         33       1,601.7        1,177.0       729        6,200       1,268.2  fclose                \n",
        "      0.0           37,455         20       1,872.8           45.5        45       36,517       8,154.4  fgets                 \n",
        "      0.0           35,695         15       2,379.7        1,371.0       769       13,179       3,103.4  read                  \n",
        "      0.0           32,629         64         509.8          510.5       160        1,232         205.2  fcntl                 \n",
        "      0.0           31,885          6       5,314.2        5,309.0     1,402        8,591       2,624.8  open                  \n",
        "      0.0           21,243          2      10,621.5       10,621.5     5,560       15,683       7,158.0  socket                \n",
        "      0.0           20,519          5       4,103.8        4,564.0     2,171        5,648       1,342.4  munmap                \n",
        "      0.0           18,954          3       6,318.0        6,960.0     3,692        8,302       2,371.1  pipe2                 \n",
        "      0.0            9,431          1       9,431.0        9,431.0     9,431        9,431           0.0  connect               \n",
        "      0.0            7,183          2       3,591.5        3,591.5     2,290        4,893       1,840.6  pthread_cond_broadcast\n",
        "      0.0            5,886          2       2,943.0        2,943.0     2,853        3,033         127.3  fwrite                \n",
        "      0.0            2,898          8         362.3          349.0       264          533          78.1  dup                   \n",
        "      0.0            1,464          1       1,464.0        1,464.0     1,464        1,464           0.0  bind                  \n",
        "      0.0              972          1         972.0          972.0       972          972           0.0  listen                \n",
        "\n",
        "[5/8] Executing 'cuda_api_sum' stats report\n",
        "\n",
        " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
        " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
        "     86.2       93,793,097          3  31,264,365.7     89,168.0     75,632  93,628,297  54,008,749.2  cudaMalloc            \n",
        "      6.9        7,521,965          1   7,521,965.0  7,521,965.0  7,521,965   7,521,965           0.0  cudaDeviceSynchronize\n",
        "      5.7        6,247,854          4   1,561,963.5  1,595,584.5    142,418   2,914,267   1,132,750.6  cudaMemcpy            \n",
        "      0.9        1,023,654          3     341,218.0    277,734.0    140,895     605,025     238,488.6  cudaFree              \n",
        "      0.2          179,158          1     179,158.0    179,158.0    179,158     179,158           0.0  cudaLaunchKernel      \n",
        "      0.0            1,820          1       1,820.0      1,820.0      1,820       1,820           0.0  cuModuleGetLoadingMode\n",
        "\n",
        "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
        "\n",
        " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
        " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
        "    100.0        7,513,035          1  7,513,035.0  7,513,035.0  7,513,035  7,513,035          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
        "\n",
        "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
        "\n",
        " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
        " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
        "     78.3        4,121,069      3  1,373,689.7  1,364,036.0     45,983  2,711,050  1,332,559.7  [CUDA memcpy HtoD]\n",
        "     21.7        1,143,626      1  1,143,626.0  1,143,626.0  1,143,626  1,143,626          0.0  [CUDA memcpy DtoH]\n",
        "\n",
        "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
        "\n",
        " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
        " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
        "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
        "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
        "\n",
        "Generated:\n",
        "    /home/cuda/report1.nsys-rep\n",
        "    /home/cuda/report1.sqlite"
      ],
      "metadata": {
        "id": "NBFYgFpvCHs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ],
      "metadata": {
        "id": "edxNOq-8PCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You must fill in this section!!**\n",
        "\n",
        "Group information:\n",
        "- member-1: NAME, SURNAME, PERSON CODE\n",
        "- member-2: NAME, SURNAME, PERSON CODE\n",
        "- member-3: NAME, SURNAME, PERSON CODE\n",
        "- I BULBASAUR<br><img src=\"https://raw.githubusercontent.com/PokeAPI/sprites/master/sprites/pokemon/other/official-artwork/1.png\" alt=\"Bulbasaur\" width=\"120\" border=\"0\">\n",
        "\n",
        "*Note: yes, groups can now have a logo - this is optional and merely for fun, if you don't feel like having one, no worries, in which case you may delete that itemize entry alongside this note :(*\n",
        "<!-- if you reeeeeally don't have ideas for a logo, before giving up, check this out: https://picrew.me/en/image_maker/47882 -->"
      ],
      "metadata": {
        "id": "5COx5mKMPF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bullet points describing what you did with a short motivation (some - arguably stupid ? - examples are given):\n",
        "- used supershared L99 cache: this was the fastest way to raise the temperature and cook an egg on the GPU's heatsink\n",
        "- pinned DRAM chips to the wall and asked them to be faster: they did not comply\n",
        "- missread the assignment and implemented matrix diagonalization: now I have my own version of cuBLAS\n",
        "- relied so heavily on blockIdx.z that the results tried to escape the HBM stack: we politely asked them to stay\n",
        "- broke isolation and achieved priviledge escalation in Colab by kindly asking Google's chief sysadmin, we can now \"tweak\" the profiler's report: this was outside what was discussed during lectures, but social engineering is easier than writing good code\n",
        "\n",
        "*Note: possibly less than 8 entries of ~32 words each. More isn't necessarily better if nobody will read it.*\n",
        "\n",
        "*Note: the subject is \"the main things you came up with to improve the kernel\".*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Block Size Optimization (32x32 = 1024 threads): Maximizes occupancy on modern GPUs (full warp utilization), better SM utilization than 16x16=256 threads"
      ],
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      }
    }
  ]
}