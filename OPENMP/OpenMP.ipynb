{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thomas-Fabbris/parallel-computing-polimi/blob/main/OPENMP/OpenMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOIneyui3qOv"
      },
      "source": [
        "# **OpenMP**\n",
        "A macro-based approach for expressing thread level parallelism in C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rodBOxsf5GC1"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z2nasZca3oIb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install build-essential libomp-dev\n",
        "!mkdir /home/OpenMP\n",
        "%cd /home/OpenMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVfSitYV5Prt"
      },
      "source": [
        "## **Glossary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzzAQA1z5R4Z"
      },
      "source": [
        "OpenMP hides from the programmer all the pedantic complexity of managing POSIX threads, deferring that to the compiler and exposing a simple set of directives/pragmas to specify how the code needs to be parallelized.\n",
        "For instance, in the OpenMP model, the fork and join operations happen automatically at the stard and end of parallel constructs.\n",
        "Similarly, using locks just requires denoting the critical sections of the code that must be protected by one.\n",
        "\n",
        "Some good doc can be found here: https://rookiehpc.org/openmp/docs/index.html\n",
        "<br>\n",
        "Another good introduction is this on: https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html\n",
        "\n",
        "### Pragma Syntax\n",
        "\n",
        "OpenMP directives in C/C++ use the following general syntax (square brackets denote optional parts):\n",
        "\n",
        "```\n",
        "#pragma omp <directive> [clause[[,] clause] ...]\n",
        "```\n",
        "\n",
        "Clauses that refine the behavior of the directive to which they are applied.\n",
        "\n",
        "Common clauses:\n",
        "\n",
        "- `num_threads(n)` : specify the number of threads to use.\n",
        "- `nowait` : remove the implicit barrier at the end of a construct.\n",
        "- `if(cond)` : execute in parallel only if the condition is true.\n",
        "\n",
        "Data sharing clauses:\n",
        "\n",
        "- `private(varlist)` : each thread has its own uninitialized copy of listed variables.\n",
        "- `firstprivate(varlist)` : like `private`, but each copy is initialized with the original value.\n",
        "- `lastprivate(varlist)` : copies the value from the last iteration or section (lexicographically - w.r.t. the order as written in the code) back to the original variable; a very simple way to avoid concurrent writes.\n",
        "- `shared(varlist)` : variables are shared among all threads.\n",
        "- `reduction(operator : varlist)` : perform a reduction operation across threads on the given variables.\n",
        "- `default(shared|private|firstprivate|lastprivate|none)` : defines the default sharing type for variables in the region; if not specified, it is `shared`; if set to `none` the compiler forces you to manually specify a sharing clause for each variable access by the thread.\n",
        "\n",
        "Some pragmas are declarative and can sit anywhere in the serial part of the code (e.g. `omp threadprivate`), others, like worksharing ones (e.g. `omp section`), act on the block that immediately follows.\n",
        "Thus, if such a pragma preceeds a control statement, it acts on its code block, otherwise a block can be induced manually with a pair of `{}`.\n",
        "\n",
        "*Note: in C/C++ a **structured block**, is defined as a single statement or a sequence of statements that is enclosed in curly braces. It shall be such that execution may never branch into the sequence or out of it, going through it entirely after entering it. This is sometimes called \"Single Entry, Single Exit\" (SESE). A multi-statement block often coincides with a scope.*\n",
        "\n",
        "Example:\n",
        "```\n",
        "int a = 1, b, c, d, s;\n",
        "#pragma omp parallel for default(private) firstprivate(a, b) lastprivate(c) shared(s) num_threads(10)\n",
        "for (int i = 0; i < 10; ++i) {\n",
        "  a += 1; // everyone increments their 'a' copy from 1 to 2\n",
        "  c = i;  // the master thread's 'c' will be 9 after the loop\n",
        "  d = 4;  // everyone initializes its copy of 'd' to 4\n",
        "  s = i;  // race condition for who will write the final 's'\n",
        "}\n",
        "```\n",
        "\n",
        "Threadprivate variables:\n",
        "\n",
        "- Declared with: `#pragma omp threadprivate(varlist)`\n",
        "- Define global or file-scope variables private to each thread across parallel regions.\n",
        "- Unlike `private`, their value persists across parallel regions.\n",
        "- Use the `copyin(varlist)` clause to initialize threadprivate variables in each thread from the master thread.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Pragmas\n",
        "\n",
        "The key parallel constructs:\n",
        "\n",
        "- `#pragma omp parallel` : start a parallel region executed by a team of multiple threads.\n",
        "\n",
        "A team is a group of threads that work together to execute a region of code.\n",
        "Each team has one master thread (the thread that encounters the parallel directive) and zero or more additional worker threads.\n",
        "Within a team, threads can execute work concurrently using work-sharing directives or further nested parallel constructs.\n",
        "When the parallel region ends, the team disbands, and execution continues with a single thread (the master).\n",
        "\n",
        "Work-sharing directives:\n",
        "\n",
        "- `#pragma omp for` : distribute loop iterations among threads in a parallel region (\\*). <!--the `omp loop` pragma is its modern, more flexible, counterpart-->\n",
        "- `#pragma omp sections` / `#pragma omp section` : divide work into distinct code blocks, each executed by one thread in a parallel region.\n",
        "\n",
        "These can be combined with `omp parallel` to write just one pragma instead of two to do the same thing, like `omp parallel for` to simultaneously open a parallel region and parallelize a loop.\n",
        "\n",
        "These pragmas terminate in an **implicit barrier** that waits for all threads to complete the work they were assigned as part of the pragma.\n",
        "\n",
        "The parallel directive creates **one** team of threads.\n",
        "Remember, **you can't \"divide twice\" within one team**, therefore you can't nest work-sharing directives inside the same parallel region, you can only issue them sequentially unless you create a nested parallel region that spawns additional teams.\n",
        "<br>\n",
        "In other words, a team can only deal with one work-sharing directive at once.\n",
        "\n",
        "**Nesting parallel regions** provides an immediate way to allow more threads to participate in the computation.<br>\n",
        "Nested parallel region behavior:\n",
        "- if nested parallelism is enabled (see later), each nested region will spawn its defined number of threads each time it is encountered by any thread. This can help programs with limited scalability, but also quickly blow up the number of threads past physical cores and bloat the system (oversubscription) if abused. Enabling dynamic adjustment of the number of threads (see later) can mitigate this.\n",
        "- if nested parallelism is disabled, the nested parallel region executes as if it were a serial block.\n",
        "\n",
        "Synchronization and exclusion:\n",
        "\n",
        "- `#pragma omp single` : only one thread executes the block; other threads wait at an implicit barrier.\n",
        "- `#pragma omp master` : executed only by the master thread (thread 0), with no barrier implied.\n",
        "- `#pragma omp barrier` : explicit synchronization point; all threads wait here.\n",
        "- `#pragma omp critical [(name)]` : only one thread executes the block at a time; multiple critical blocks that have the same name are seen as the same block and use the same lock underneath.\n",
        "- `#pragma omp atomic` : perform a single atomic update on a shared variable.\n",
        "- `#pragma omp flush` : enforce memory consistency (ensures all threads see updated values).\n",
        "<!--- `#pragma omp ordered` : enforce ordered execution of certain loop parts marked as `ordered`.-->\n",
        "\n",
        "(\\*) Using `omp for` requires a canonical OpenMP loop, meaning that:\n",
        "- it is strictly a `for` loop;\n",
        "- it has a single integer loop induction variable;\n",
        "- the loop is countable (finite), with a linear increment or decrement (e.g. `i += 2` is ok, but not `i *= 2`);\n",
        "- the loop variable, bounds, and increment are iteration invariants;\n",
        "\n",
        "Following from the above, the number of iterations can be determined before the loop executes, even if it's not known until run time.\n",
        "In particular, note that the number of iterations doesn't need to be known statially (at compile time). It can be computed at runtime so long as it is fixed by the time the loop is reached and needs to be divided among threads.\n",
        "\n",
        "---\n",
        "\n",
        "### Scheduling\n",
        "\n",
        "When parallelizing unbalanced loops, where some iterations may take more time than others, we can balance the load between threads with a schedule:\n",
        "\n",
        "- `schedule(kind[, chunk_size])` : control iteration scheduling, `kind` can be:\n",
        "  - `static` : iterations are split into chunks of equal size, each thread is assigned its almost evenly chunks before execution begins and those never change afterwards.\n",
        "  - `dynamic` : iterations are split into chunks of equal size and placed in a queue, threads are assigned one chunk at a time from the queue. Adds a slight scheduling overhead, use only if the workload is truly unbalanced.\n",
        "  - `guided` : chunks of iterations are assigned to threads as the loop runs, but the chunk size decreases as the loop progresses. The given `chunk_size` functions as the minimum chunk size reached. Also adds a slight overhead, but less than `dynamic` due to fewer scheduling decisions as per the larger initial chunks.\n",
        "  - `runtime` : delegates the choice of schedule to the environment variable `OMP_SCHEDULE`.\n",
        "  - `auto` : the choice of schedule is delegated to either the compiler or the runtime environment.\n",
        "  - if not specified, the default schedule is implementation-dependent.\n",
        "- `collapse(n)` : collapse (flatten) `n` nested loops into a single loop (and thus single iteration space) for scheduling.\n",
        "\n",
        "**Keep this in mind when parallelizing nested loops:**<br>\n",
        "If execution of any associated loop changes any of the values used to compute any of the **iteration counts** (loop bounds), then the behavior is unspecified.\n",
        "\n",
        "---\n",
        "\n",
        "### Tasks and Related Pragmas\n",
        "\n",
        "Defines asynchronous units of work for fine-grained parallelism:\n",
        "\n",
        "- `#pragma omp task` : define a task for deferred execution.\n",
        "- `#pragma omp taskwait` : wait until all child tasks of the current task complete.\n",
        "- `#pragma omp taskgroup` : group tasks for collective synchronization.\n",
        "- `#pragma omp taskyield` : allow a thread to yield execution to other tasks.\n",
        "\n",
        "Common clauses for tasks:\n",
        "\n",
        "- `if(cond)` : create the task only if the condition is true; otherwise, execute it immediately.\n",
        "- `final(cond)` : mark task as “final,” disallowing creation of child tasks inside it.\n",
        "- `mergeable` : allow the task to be merged with its parent task for optimization.\n",
        "- `depend(in|out|inout : varlist)` : declare task dependencies to control execution order.\n",
        "- `untied` : allow the task to resume on a different thread than the one that started it.\n",
        "\n",
        "Whereas sections define static tasks, statically defined at compile time and whose number cannot change, that are queued up and handled in arbitrary order by threads, these true **tasks** form a graph of execution with dependencies, any task spawning more tasks, and synchronization, more like threads would in a barebones fork-join model, but with cleaner code and higher level abstractions.\n",
        "<br>\n",
        "In brief: tasks can be spawned from any point and thread inside the parallel region!\n",
        "\n",
        "A huge warning: tasks are tied to their thread by default, meaning that if they are suspended, they must resume in the same thread until they finish. This is crucial because private variables (e.g. those specified on the `parallel private(...)` that spawns the team of threads running the tasks) are **per-thread, not per-task**, so if a task relies on the private variables of its thread, and is untied, it may see those randomly changing if it ever gets rescheduled. And the same applies to other per-thread things, like IDs.\n",
        "<br>\n",
        "If you want to untie a task, make sure it only works on shared variables or its own local variables.\n",
        "\n",
        "---\n",
        "\n",
        "### Teams and Offloading (extra)\n",
        "\n",
        "The `pragma opm teams` directive was introduced mainly to support heterogeneous (accelerator/GPU) programming.\n",
        "\n",
        "It takes the place of `parallel`, but unlike it, `teams` creates multiple teams of threads, each with its own master and workers.\n",
        "Each team executes the same code region independently.\n",
        "Within each team, you can then launch further nested parallelism (using `omp parallel`) to create hierarchical parallelism.\n",
        "\n",
        "On a CPU, this may just create a single team (depending on implementation), but on GPUs, **it naturally maps to CUDA thread and blocks** seen as multiple independent teams, each with their own threads.\n",
        "\n",
        "Offloading may look like this:\n",
        "```\n",
        "#pragma omp target teams distribute parallel for device(device_num)\n",
        "for (int i = 0; i < N; ++i) {\n",
        "    A[i] = B[i] + C[i];\n",
        "}\n",
        "```\n",
        "\n",
        "Where the pragma reads as:\n",
        "- target : offload to a set device (e.g. GPU).\n",
        "- teams : create multiple teams (like CUDA thread blocks).\n",
        "- parallel for : within each team, create multiple threads to execute parts of the loop in parallel.\n",
        "\n",
        "---\n",
        "\n",
        "### Routines\n",
        "\n",
        "Functions exposed by the OpenMP API:\n",
        "\n",
        "- `int omp_get_thread_num()` : use inside a parallel region to get the unique identifier of your thread; it will be a number in [0, num_threads).\n",
        "- `int omp_get_num_threads()` : returns the number of threads created in the current parallel region.\n",
        "- `void omp_set_num_threads(int num_threads)` : sets the default number of threads to use in parallel regions.\n",
        "- `int omp_get_max_threads()` : returns the maximum number of threads to use in parallel regions.\n",
        "- `void omp_set_max_threads(int num_threads)` : sets the maximum number of threads to use in parallel regions.\n",
        "- `void omp_set_nested(int nested)` : enables or disables nested parallelism.\n",
        "- `void omp_set_dynamic(int dynamic_threads)` : enables or disables dynamic adjustment of the number of threads available for the execution of subsequent parallel regions.\n",
        "- `double omp_get_wtime()` : returns an absolute time reference, useful to time code execution.\n",
        "\n",
        "Additional routines are available inside the `teams` pragma:\n",
        "- `int omp_get_team_num()` : returns the number of created teams.\n",
        "- `int omp_get_team_num()` : returns the unique identifier of the caller thread's team; it will be a number in [0, num_teams).\n",
        "\n",
        "To access those you need to include the `omp.h` header.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "Control OpenMP runtime behavior without recompiling.\n",
        "\n",
        "- `OMP_NUM_THREADS` : default number of threads to use in parallel regions, if not specified defaults to the system's core count.\n",
        "- `OMP_SCHEDULE` : default loop scheduling policy (e.g. `\"dynamic,4\"`).\n",
        "- `OMP_PROC_BIND` : control thread-core binding (`master|close|spread`).\n",
        "- `OMP_PLACES` : specify hardware places (`threads|cores|sockets` or custom lists).\n",
        "- `OMP_MAX_ACTIVE_LEVELS` : maximum depth of nested parallel regions.\n",
        "- `OMP_WAIT_POLICY` : set thread waiting behavior (`active` or `passive`).\n",
        "- `OMP_DISPLAY_ENV` : print the current OpenMP environment at startup.\n",
        "- `OMP_STACKSIZE` : set the thread stack size.\n",
        "- `OMP_CANCELLATION` : enable or disable cancellation features in tasks or loops.\n",
        "- `OMP_NESTED` : enables nested parallel regions, usually disabled by default.\n",
        "- `OMP_DYNAMIC` : enables or disables dynamic adjustment of the number of threads available for the execution of subsequent parallel regions, usually disabled by default.(`TRUE` or `FALSE`).\n",
        "\n",
        "---\n",
        "\n",
        "### Controlling Affinity and Thread Assignment\n",
        "\n",
        "Mechanisms to control how threads are bound to CPU cores and how their placement affects performance:\n",
        "\n",
        "- `proc_bind(master|close|spread)` : directive clause controlling how threads are distributed within the available places (as defined by `OMP_PLACES`):\n",
        "  - `master` : all threads are placed close to the master thread (usually on the same core group, e.g. socket or NUMA node).\n",
        "  - `close` : threads are packed as near as possible to each other, filling one place before moving to the next (minimize distance, maximize data locality).\n",
        "  - `spread` : threads are distributed as widely as possible across places (maximize distance, maximize resource usage).\n",
        "\n",
        "At the environment level:\n",
        "\n",
        "- `OMP_PROC_BIND` : controls whether and how threads are bound (`TRUE|FALSE|master|close|spread`).\n",
        "- `OMP_PLACES` : specifies the hardware resources threads may be placed on (e.g. `threads`, `cores`, `sockets`, or custom lists like `\"{0,1},{2,3}\"`).\n",
        "\n",
        "Typical uses:\n",
        "- use `close` for threads that frequently share many accesses to the same data or work on contiguous chunks of a shared array, thus improving cache locality.\n",
        "- use `spread` for threads that are largely independent or memory-bound, hence prefer having a lot of hardware resources and may as well fill a cache line by themselves with little data accesses in common with each other.\n",
        "- with nested parallel regions, it's usual to have first a `spread` (to use different sockets or cores) and then a `close` binding (to exploit data reuse within a place), especially when inner threads see more data reuse opportunities than outer ones.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "#pragma omp parallel num_threads(4) proc_bind(spread)\n",
        "{\n",
        "  #pragma omp parallel num_threads(4) proc_bind(close)\n",
        "  {\n",
        "    // Work here\n",
        "  }\n",
        "}\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "### Settings Precedence\n",
        "\n",
        "When setting the same parameter through different means, they override each other in this order from most to least authoritative:\n",
        "\n",
        "1. Explicit clauses in pragmas (`proc_bind`, `num_threads`, ...)\n",
        "2. Explicit routine calls (`omp_set_num_threads`, ...)\n",
        "3. Environment variables (`OMP_PROC_BIND`, `OMP_PLACES`, `OMP_NUM_THREADS`, ...)\n",
        "4. Implementation defaults (compiler/runtime)\n",
        "\n",
        "---\n",
        "\n",
        "### Compiler Commands\n",
        "\n",
        "Compilers (GCC, Clang) require a flag to enable OpenMP support:\n",
        "\n",
        "```bash\n",
        "gcc/clang -fopenmp program.c -o program\n",
        "```\n",
        "\n",
        "To disable OpenMP (e.g. for debugging), you can just omit `-fopenmp`.\n",
        "<br>\n",
        "When disabled, OpenMP pragmas are ignored, and the code runs in serial mode.\n",
        "This is useful for checking correctness and debugging race conditions.\n",
        "\n",
        "---\n",
        "\n",
        "### Notes\n",
        "\n",
        "* OpenMP pragmas are hints to the compiler: if support is disabled, they are ignored and the program remains valid C/C++.\n",
        "* For portable and deterministic parallel programs, always explicitly specify data-sharing attributes and scheduling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km1W492NojXB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvZp4-7XMgTp"
      },
      "source": [
        "## **Simple Examples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Sjhyc_OfLP"
      },
      "source": [
        "### **Exercise 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9f5lhn5OeLg"
      },
      "source": [
        "Multiple concurrent threads printing \"Hello, World!\" (plus meaningless computation to make it run longer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reTXIF6UPUwN",
        "outputId": "292068ef-6328-48c7-d64a-8f1399c03381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/hello_world_0.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/hello_world_0.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main ()\n",
        "{\n",
        "  #pragma omp parallel /*num_threads(2000)*/\n",
        "  {\n",
        "    int id = omp_get_thread_num();\n",
        "    printf(\"Hello World from thread = %d\\n\", id);\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDOVUTKmPbA4"
      },
      "source": [
        "Note how this is different from the Pthreads implementation. Questions:\n",
        "- How many threads will be created? Usually as many as the available CPU cores, but this is implementation dependent. <br/> In this example, two threads are created as the default Colab runtime features a dual-core CPU.\n",
        "- What happens if you ask for a number of threads greater than the number of cores? Oversubscription may happen, leading to lower performance and introducing a scheduling overhead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_ezn2NQsyA"
      },
      "source": [
        "Compile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_VBfqOmJQu6M"
      },
      "outputs": [],
      "source": [
        "!g++ hello_world_0.cpp -fopenmp -o hello_world_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPG0TV29Qt1W"
      },
      "source": [
        "Execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jonu0ntBQ49m",
        "outputId": "ca3e89ab-30d2-4f43-afaf-f621a879a895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World from thread = 1\n",
            "Hello World from thread = 0\n"
          ]
        }
      ],
      "source": [
        "!./hello_world_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJzYN_EF4pVf"
      },
      "source": [
        "### **Exercise 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJlO-P8vN2gO"
      },
      "source": [
        "Introducing parallelism in OpenMP can be as easy as adding pragmas, with no further modifications on the code. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2DBligfSAfM",
        "outputId": "7cee3c62-a94d-4b7e-e330-59294a591f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/hello_world.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/hello_world.cpp\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void print_message(int threadIndex) {\n",
        "  printf(\"Thread number %d\\n\", threadIndex);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  #pragma omp parallel num_threads(4)\n",
        "  {\n",
        "    #pragma omp for schedule(static, 4)\n",
        "    for (int ii = 0; ii < 10; ii++) {\n",
        "      print_message(ii);\n",
        "    }\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUnb63jIdMfU"
      },
      "source": [
        "For this example we need clang if we want to inspect the LLVM-IR..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJRqaP1EdMFT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install clang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWAF2q08NVi6"
      },
      "source": [
        "Compile without the OpenMP flag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vd5hp0-TiH3",
        "outputId": "f077c4ea-66f3-442c-f620-817c4361ac15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/OpenMP\n"
          ]
        }
      ],
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -o hello_world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZp-89r8MLWQ"
      },
      "source": [
        "Inspect the generated LLVM IR:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGtfkM9_WhHP",
        "outputId": "4a0fdad4-ee52-4038-d01b-6262a3820d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/OpenMP\n",
            "; ModuleID = 'hello_world.cpp'\n",
            "source_filename = \"hello_world.cpp\"\n",
            "target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n",
            "target triple = \"x86_64-pc-linux-gnu\"\n",
            "\n",
            "@.str = private unnamed_addr constant [18 x i8] c\"Thread number %d\\0A\\00\", align 1\n",
            "\n",
            "; Function Attrs: mustprogress noinline optnone uwtable\n",
            "define dso_local void @_Z13print_messagei(i32 noundef %0) #0 {\n",
            "  %2 = alloca i32, align 4\n",
            "  store i32 %0, i32* %2, align 4\n",
            "  %3 = load i32, i32* %2, align 4\n",
            "  %4 = call i32 (i8*, ...) @printf(i8* noundef getelementptr inbounds ([18 x i8], [18 x i8]* @.str, i64 0, i64 0), i32 noundef %3)\n",
            "  ret void\n",
            "}\n",
            "\n",
            "declare i32 @printf(i8* noundef, ...) #1\n",
            "\n",
            "; Function Attrs: mustprogress noinline norecurse optnone uwtable\n",
            "define dso_local noundef i32 @main() #2 {\n",
            "  %1 = alloca i32, align 4\n",
            "  %2 = alloca i32, align 4\n",
            "  store i32 0, i32* %1, align 4\n",
            "  store i32 0, i32* %2, align 4\n",
            "  br label %3\n",
            "\n",
            "3:                                                ; preds = %8, %0\n",
            "  %4 = load i32, i32* %2, align 4\n",
            "  %5 = icmp slt i32 %4, 10\n",
            "  br i1 %5, label %6, label %11\n",
            "\n",
            "6:                                                ; preds = %3\n",
            "  %7 = load i32, i32* %2, align 4\n",
            "  call void @_Z13print_messagei(i32 noundef %7)\n",
            "  br label %8\n",
            "\n",
            "8:                                                ; preds = %6\n",
            "  %9 = load i32, i32* %2, align 4\n",
            "  %10 = add nsw i32 %9, 1\n",
            "  store i32 %10, i32* %2, align 4\n",
            "  br label %3, !llvm.loop !6\n",
            "\n",
            "11:                                               ; preds = %3\n",
            "  ret i32 0\n",
            "}\n",
            "\n",
            "attributes #0 = { mustprogress noinline optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #1 = { \"frame-pointer\"=\"all\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #2 = { mustprogress noinline norecurse optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "\n",
            "!llvm.module.flags = !{!0, !1, !2, !3, !4}\n",
            "!llvm.ident = !{!5}\n",
            "\n",
            "!0 = !{i32 1, !\"wchar_size\", i32 4}\n",
            "!1 = !{i32 7, !\"PIC Level\", i32 2}\n",
            "!2 = !{i32 7, !\"PIE Level\", i32 2}\n",
            "!3 = !{i32 7, !\"uwtable\", i32 1}\n",
            "!4 = !{i32 7, !\"frame-pointer\", i32 2}\n",
            "!5 = !{!\"Ubuntu clang version 14.0.0-1ubuntu1.1\"}\n",
            "!6 = distinct !{!6, !7}\n",
            "!7 = !{!\"llvm.loop.mustprogress\"}\n"
          ]
        }
      ],
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -S -emit-llvm\n",
        "!cat hello_world.ll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq4X5DBQNL8p"
      },
      "source": [
        "Compile with the OpenMP flag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP2V0HdzXKIm",
        "outputId": "97906755-50a6-42da-bf49-93f5953fccd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/OpenMP\n"
          ]
        }
      ],
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -fopenmp -lstdc++ -o hello_world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84yjgm_cL8Mh"
      },
      "source": [
        "Inspect the generated LLVM IR:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq9Os-OAX9Oy",
        "outputId": "99f2032a-291d-4db8-9f98-b155f77f617f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/OpenMP\n",
            "; ModuleID = 'hello_world.cpp'\n",
            "source_filename = \"hello_world.cpp\"\n",
            "target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n",
            "target triple = \"x86_64-pc-linux-gnu\"\n",
            "\n",
            "%struct.ident_t = type { i32, i32, i32, i32, i8* }\n",
            "\n",
            "$__clang_call_terminate = comdat any\n",
            "\n",
            "@.str = private unnamed_addr constant [18 x i8] c\"Thread number %d\\0A\\00\", align 1\n",
            "@0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n",
            "@1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 514, i32 0, i32 22, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @0, i32 0, i32 0) }, align 8\n",
            "@2 = private unnamed_addr constant %struct.ident_t { i32 0, i32 66, i32 0, i32 22, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @0, i32 0, i32 0) }, align 8\n",
            "@3 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @0, i32 0, i32 0) }, align 8\n",
            "\n",
            "; Function Attrs: mustprogress noinline optnone uwtable\n",
            "define dso_local void @_Z13print_messagei(i32 noundef %0) #0 {\n",
            "  %2 = alloca i32, align 4\n",
            "  store i32 %0, i32* %2, align 4\n",
            "  %3 = load i32, i32* %2, align 4\n",
            "  %4 = call i32 (i8*, ...) @printf(i8* noundef getelementptr inbounds ([18 x i8], [18 x i8]* @.str, i64 0, i64 0), i32 noundef %3)\n",
            "  ret void\n",
            "}\n",
            "\n",
            "declare i32 @printf(i8* noundef, ...) #1\n",
            "\n",
            "; Function Attrs: mustprogress noinline norecurse nounwind optnone uwtable\n",
            "define dso_local noundef i32 @main() #2 {\n",
            "  %1 = alloca i32, align 4\n",
            "  %2 = call i32 @__kmpc_global_thread_num(%struct.ident_t* @3)\n",
            "  store i32 0, i32* %1, align 4\n",
            "  call void @__kmpc_push_num_threads(%struct.ident_t* @3, i32 %2, i32 4)\n",
            "  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* @3, i32 0, void (i32*, i32*, ...)* bitcast (void (i32*, i32*)* @.omp_outlined. to void (i32*, i32*, ...)*))\n",
            "  ret i32 0\n",
            "}\n",
            "\n",
            "; Function Attrs: noinline norecurse nounwind optnone uwtable\n",
            "define internal void @.omp_outlined.(i32* noalias noundef %0, i32* noalias noundef %1) #3 personality i8* bitcast (i32 (...)* @__gxx_personality_v0 to i8*) {\n",
            "  %3 = alloca i32*, align 8\n",
            "  %4 = alloca i32*, align 8\n",
            "  %5 = alloca i32, align 4\n",
            "  %6 = alloca i32, align 4\n",
            "  %7 = alloca i32, align 4\n",
            "  %8 = alloca i32, align 4\n",
            "  %9 = alloca i32, align 4\n",
            "  %10 = alloca i32, align 4\n",
            "  %11 = alloca i32, align 4\n",
            "  store i32* %0, i32** %3, align 8\n",
            "  store i32* %1, i32** %4, align 8\n",
            "  store i32 0, i32* %7, align 4\n",
            "  store i32 9, i32* %8, align 4\n",
            "  store i32 1, i32* %9, align 4\n",
            "  store i32 0, i32* %10, align 4\n",
            "  %12 = load i32*, i32** %3, align 8\n",
            "  %13 = load i32, i32* %12, align 4\n",
            "  call void @__kmpc_for_static_init_4(%struct.ident_t* @1, i32 %13, i32 33, i32* %10, i32* %7, i32* %8, i32* %9, i32 1, i32 4)\n",
            "  br label %14\n",
            "\n",
            "14:                                               ; preds = %42, %2\n",
            "  %15 = load i32, i32* %8, align 4\n",
            "  %16 = icmp sgt i32 %15, 9\n",
            "  br i1 %16, label %17, label %18\n",
            "\n",
            "17:                                               ; preds = %14\n",
            "  br label %20\n",
            "\n",
            "18:                                               ; preds = %14\n",
            "  %19 = load i32, i32* %8, align 4\n",
            "  br label %20\n",
            "\n",
            "20:                                               ; preds = %18, %17\n",
            "  %21 = phi i32 [ 9, %17 ], [ %19, %18 ]\n",
            "  store i32 %21, i32* %8, align 4\n",
            "  %22 = load i32, i32* %7, align 4\n",
            "  store i32 %22, i32* %5, align 4\n",
            "  %23 = load i32, i32* %5, align 4\n",
            "  %24 = load i32, i32* %8, align 4\n",
            "  %25 = icmp sle i32 %23, %24\n",
            "  br i1 %25, label %26, label %49\n",
            "\n",
            "26:                                               ; preds = %20\n",
            "  br label %27\n",
            "\n",
            "27:                                               ; preds = %38, %26\n",
            "  %28 = load i32, i32* %5, align 4\n",
            "  %29 = load i32, i32* %8, align 4\n",
            "  %30 = icmp sle i32 %28, %29\n",
            "  br i1 %30, label %31, label %41\n",
            "\n",
            "31:                                               ; preds = %27\n",
            "  %32 = load i32, i32* %5, align 4\n",
            "  %33 = mul nsw i32 %32, 1\n",
            "  %34 = add nsw i32 0, %33\n",
            "  store i32 %34, i32* %11, align 4\n",
            "  %35 = load i32, i32* %11, align 4\n",
            "  invoke void @_Z13print_messagei(i32 noundef %35)\n",
            "          to label %36 unwind label %50\n",
            "\n",
            "36:                                               ; preds = %31\n",
            "  br label %37\n",
            "\n",
            "37:                                               ; preds = %36\n",
            "  br label %38\n",
            "\n",
            "38:                                               ; preds = %37\n",
            "  %39 = load i32, i32* %5, align 4\n",
            "  %40 = add nsw i32 %39, 1\n",
            "  store i32 %40, i32* %5, align 4\n",
            "  br label %27\n",
            "\n",
            "41:                                               ; preds = %27\n",
            "  br label %42\n",
            "\n",
            "42:                                               ; preds = %41\n",
            "  %43 = load i32, i32* %7, align 4\n",
            "  %44 = load i32, i32* %9, align 4\n",
            "  %45 = add nsw i32 %43, %44\n",
            "  store i32 %45, i32* %7, align 4\n",
            "  %46 = load i32, i32* %8, align 4\n",
            "  %47 = load i32, i32* %9, align 4\n",
            "  %48 = add nsw i32 %46, %47\n",
            "  store i32 %48, i32* %8, align 4\n",
            "  br label %14\n",
            "\n",
            "49:                                               ; preds = %20\n",
            "  call void @__kmpc_for_static_fini(%struct.ident_t* @1, i32 %13)\n",
            "  call void @__kmpc_barrier(%struct.ident_t* @2, i32 %13)\n",
            "  ret void\n",
            "\n",
            "50:                                               ; preds = %31\n",
            "  %51 = landingpad { i8*, i32 }\n",
            "          catch i8* null\n",
            "  %52 = extractvalue { i8*, i32 } %51, 0\n",
            "  call void @__clang_call_terminate(i8* %52) #7\n",
            "  unreachable\n",
            "}\n",
            "\n",
            "declare void @__kmpc_for_static_init_4(%struct.ident_t*, i32, i32, i32*, i32*, i32*, i32*, i32, i32)\n",
            "\n",
            "declare i32 @__gxx_personality_v0(...)\n",
            "\n",
            "; Function Attrs: noinline noreturn nounwind\n",
            "define linkonce_odr hidden void @__clang_call_terminate(i8* %0) #4 comdat {\n",
            "  %2 = call i8* @__cxa_begin_catch(i8* %0) #5\n",
            "  call void @_ZSt9terminatev() #7\n",
            "  unreachable\n",
            "}\n",
            "\n",
            "declare i8* @__cxa_begin_catch(i8*)\n",
            "\n",
            "declare void @_ZSt9terminatev()\n",
            "\n",
            "; Function Attrs: nounwind\n",
            "declare void @__kmpc_for_static_fini(%struct.ident_t*, i32) #5\n",
            "\n",
            "; Function Attrs: convergent nounwind\n",
            "declare void @__kmpc_barrier(%struct.ident_t*, i32) #6\n",
            "\n",
            "; Function Attrs: nounwind\n",
            "declare i32 @__kmpc_global_thread_num(%struct.ident_t*) #5\n",
            "\n",
            "; Function Attrs: nounwind\n",
            "declare void @__kmpc_push_num_threads(%struct.ident_t*, i32, i32) #5\n",
            "\n",
            "; Function Attrs: nounwind\n",
            "declare !callback !7 void @__kmpc_fork_call(%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) #5\n",
            "\n",
            "attributes #0 = { mustprogress noinline optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #1 = { \"frame-pointer\"=\"all\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #2 = { mustprogress noinline norecurse nounwind optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #3 = { noinline norecurse nounwind optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n",
            "attributes #4 = { noinline noreturn nounwind }\n",
            "attributes #5 = { nounwind }\n",
            "attributes #6 = { convergent nounwind }\n",
            "attributes #7 = { noreturn nounwind }\n",
            "\n",
            "!llvm.module.flags = !{!0, !1, !2, !3, !4, !5}\n",
            "!llvm.ident = !{!6}\n",
            "\n",
            "!0 = !{i32 1, !\"wchar_size\", i32 4}\n",
            "!1 = !{i32 7, !\"openmp\", i32 50}\n",
            "!2 = !{i32 7, !\"PIC Level\", i32 2}\n",
            "!3 = !{i32 7, !\"PIE Level\", i32 2}\n",
            "!4 = !{i32 7, !\"uwtable\", i32 1}\n",
            "!5 = !{i32 7, !\"frame-pointer\", i32 2}\n",
            "!6 = !{!\"Ubuntu clang version 14.0.0-1ubuntu1.1\"}\n",
            "!7 = !{!8}\n",
            "!8 = !{i64 2, i64 -1, i64 -1, i1 true}\n"
          ]
        }
      ],
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -S -emit-llvm -fopenmp\n",
        "!cat hello_world.ll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewd_80SiNc2C"
      },
      "source": [
        "Execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IM8FFhaTtdC",
        "outputId": "c0fdd5e4-630e-47bd-c3b3-6569475eb0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/OpenMP\n",
            "Thread number 0\n",
            "Thread number 1\n",
            "Thread number 8\n",
            "Thread number 9\n",
            "Thread number 4\n",
            "Thread number 5\n",
            "Thread number 6\n",
            "Thread number 7\n",
            "Thread number 2\n",
            "Thread number 3\n"
          ]
        }
      ],
      "source": [
        "%cd /home/OpenMP\n",
        "!./hello_world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QiOjWEyRafk"
      },
      "source": [
        "## **Calculation of pi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nGEtHLqRlZn"
      },
      "source": [
        "###**Exercise 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtcFuEitRnQx"
      },
      "source": [
        "Integral-based method to calculate pi: each thread calculates the heigth of a set of rectangles (map/SIMD pattern), the sum of all heigths is multiplied by the step size to get the area.\n",
        "<img align=\"middle\" src=\"https://drive.google.com/uc?id=17dBhvYY9F5Bl2re_pnmRWiZ717jolCPg\">\n",
        "\n",
        "Why does this work?\n",
        "<br>\n",
        "Recall that: $\\frac{d}{dx} arctan(x) = \\frac{1}{1+x^2}$ and $arctan(0) = 0°$ while $arctan(1) = 45° = \\frac{180°}{4}$, so...\n",
        "\n",
        "*More info here: https://math.stackexchange.com/questions/1085653/geometrical-interpretation-of-pi-int-01-frac41x2dx*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhY5Jlz8HsNy"
      },
      "source": [
        "Basic implementation with manual work-sharing using thread IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IMvx6wvTzh_",
        "outputId": "78236193-b6b6-4ec6-ef37-bc213e968c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/integralpi.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/integralpi.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define MAX_THREADS 4\n",
        "\n",
        "static long num_steps = 100000000;\n",
        "double step;\n",
        "\n",
        "int main() {\n",
        "\tint i, j;\n",
        "\tdouble pi, full_sum = 0.0;\n",
        "\tdouble start_time, run_time;\n",
        "\tdouble sum[MAX_THREADS];\n",
        "\n",
        "\tstep = 1.0/(double) num_steps;\n",
        "\n",
        "\t// measure scalability from 1 to MAX_THREADS threads\n",
        "\tfor (j = 1; j <= MAX_THREADS; j++) {\n",
        "\t\tomp_set_num_threads(j);\n",
        "\t\tfull_sum = 0.0;\n",
        "\t\tstart_time = omp_get_wtime();\n",
        "\n",
        "\t\t#pragma omp parallel\n",
        "\t\t{\n",
        "\t\t\tint i;\n",
        "\t\t\tint id = omp_get_thread_num();\n",
        "\t\t\tint numthreads = omp_get_num_threads();\n",
        "\t\t\tdouble x;\n",
        "\t\t\tsum[id] = 0.0;\n",
        "\t\t\tif (id == 0)\n",
        "\t\t\t\tprintf(\" num_threads = %d\", numthreads);\n",
        "\n",
        "\t\t\t// manual work allocation\n",
        "\t\t\tfor (i = id; i < num_steps; i += numthreads) {\n",
        "\t\t\t\tx = (i + 0.5)*step;\n",
        "\t\t\t\tsum[id] = sum[id] + 4.0/(1.0 + x*x);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t\tfor(full_sum = 0.0, i = 0; i < j; i++)\n",
        "\t\t\tfull_sum += sum[i];\n",
        "\n",
        "\t\tpi = step * full_sum;\n",
        "\t\trun_time = omp_get_wtime() - start_time;\n",
        "\t\tprintf(\"\\n pi is %f in %f seconds %d threads \\n\", pi, run_time, j);\n",
        "\t}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96IpoqAdcnd"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmXE19iPdeCI",
        "outputId": "92d49c36-e60f-42cb-a0f1-7523fbb5d0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " num_threads = 1\n",
            " pi is 3.141593 in 0.580693 seconds 1 threads \n",
            " num_threads = 2\n",
            " pi is 3.141593 in 0.492389 seconds 2 threads \n",
            " num_threads = 3\n",
            " pi is 3.141593 in 0.537672 seconds 3 threads \n",
            " num_threads = 4\n",
            " pi is 3.141593 in 0.530303 seconds 4 threads \n"
          ]
        }
      ],
      "source": [
        "!g++ integralpi.cpp -fopenmp -o integralpi\n",
        "!./integralpi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2ebqOckUGop"
      },
      "source": [
        "Questions:\n",
        "- How is work distributed among threads?\n",
        "<!--using thread IDs, each step every \"numthreads\" is assigned to a different thread-->\n",
        "- Is the result deterministic?\n",
        "<!--yes-->\n",
        "- How do you expect performance to scale with the number of threads?\n",
        "<!--ideally, linearly, as we will almost always have far more iterations to distribute than threads and there is little overhead for the creation of additional threads, aside from their creation itself; on Colab tho, anything could happen-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOLx6_aAVHyX"
      },
      "source": [
        "Implementation with the `parallel for` work-sharing construct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd1Z5QUUVPiw",
        "outputId": "f2ca7a33-378b-46b2-9fa7-6d24f8e728a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/integralpi2.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/integralpi2.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "static long num_steps = 100000000;\n",
        "double step;\n",
        "\n",
        "int main() {\n",
        "\tint i, j;\n",
        "\tdouble x, pi, sum = 0.0;\n",
        "\tdouble start_time, run_time;\n",
        "\n",
        "\tstep = 1.0/(double) num_steps;\n",
        "\n",
        "\tfor (j = 1; j <= 4; j++) {\n",
        "\t\tsum = 0.0;\n",
        "\t\tomp_set_num_threads(i);\n",
        "\t\tstart_time = omp_get_wtime();\n",
        "\n",
        "\t\t#pragma omp parallel for private(x) reduction(+:sum)\n",
        "\t\tfor (i = 1; i <= num_steps; i++) {\n",
        "\t\t\tx = (i-0.5)*step;\n",
        "\t\t\tsum = sum + 4.0/(1.0+x*x);\n",
        "\t\t}\n",
        "\n",
        "\t\tpi = step * sum;\n",
        "\t\trun_time = omp_get_wtime() - start_time;\n",
        "\t\tprintf(\"\\n pi is %f in %f seconds and %d threads\\n\", pi, run_time, j);\n",
        "\t}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGGnxOj5dg54"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXDcU4fqdh70",
        "outputId": "a58c4137-a7aa-4ea7-f2e9-b213b2f73757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " pi is 3.141593 in 0.475248 seconds and 1 threads\n",
            "\n",
            " pi is 3.141593 in 0.346572 seconds and 2 threads\n",
            "\n",
            " pi is 3.141593 in 0.350604 seconds and 3 threads\n",
            "\n",
            " pi is 3.141593 in 0.364947 seconds and 4 threads\n"
          ]
        }
      ],
      "source": [
        "!g++ integralpi2.cpp -fopenmp -o integralpi2\n",
        "!./integralpi2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crk4g21GVUe5"
      },
      "source": [
        "Questions:\n",
        "- How is work distributed among threads?\n",
        "<!--each thread is statically assigned some of the loop's iterations by the \"for\" pragma-->\n",
        "- Are there other ways of resolving the access to the shared variable?\n",
        "<!--yes, atomically incrementing \"sum\", even tho it's a poor idea performance-wise-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9y1aoBb7jWg"
      },
      "source": [
        "## **Variables Initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5rtihG370_-"
      },
      "source": [
        "### **FirstPrivate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K81ROlH7916"
      },
      "source": [
        "Whenever we need to quickly give a copy of a value to each thread, we use `firstprivate`, this saves us the time needed for each thread to go and fetch a copy of an otherwise shared variable.\n",
        "\n",
        "Say that we have an array of elements and an initial value.\n",
        "We need to find subsequences of 3 contigous values in the array that such that, if added to the initial one, overflow.\n",
        "We can dispatch the initial value to threads as a firstprivate variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTZeBe0u7-FM"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/fistprivate.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#include <limits.h>\n",
        "\n",
        "int main(void) {\n",
        "  const int N = 12;\n",
        "  unsigned int data[N] = {10, 20, 30, 250, 5, 10, 100, 200, 50, 90, 200, 40};\n",
        "  unsigned int init_value = 1<<30;\n",
        "\n",
        "  #pragma omp parallel for firstprivate(init_value)\n",
        "  for (int i = 0; i < N - 2; i++) {\n",
        "    unsigned int a = data[i];\n",
        "    unsigned int b = data[i + 1];\n",
        "    unsigned int c = data[i + 2];\n",
        "\n",
        "    unsigned int sum = init_value;\n",
        "    int overflow = 0;\n",
        "\n",
        "    if (sum > UINT_MAX - a) overflow = 1;\n",
        "    else sum += a;\n",
        "    if (!overflow && sum > UINT_MAX - b) overflow = 1;\n",
        "    else sum += b;\n",
        "    if (!overflow && sum > UINT_MAX - c) overflow = 1;\n",
        "    else sum += c;\n",
        "\n",
        "    if (overflow)\n",
        "      printf(\"Thread %d found overflow at subsequence [%d,%d,%d]\\n\", omp_get_thread_num(), i, i+1, i+2);\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNuktT9DM4pw"
      },
      "source": [
        "Questions:\n",
        "- could we do this without `firstprivate`?\n",
        "<!--obviously yes, we could just have each thread copy the content of the then-shared \"init_value\" into its own local variable-->\n",
        "- what is the advantage of using `firstprivate`?\n",
        "<!--firstprivate guarantees that threads will not alter the global instance of the variable. It is mainly a semantical tool to ensure that when comparing the code between a parallel and serial execution the existence of threads doesn't unpredictably change the content of the thus-private variable. Ultimately, firstprivate clearly explicitate how the program intends the variable to be (safely) operated upon by threads: each thread receives its own copy of this initial value.-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zxfZG6L7mfe"
      },
      "source": [
        "### **LastPrivate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zqy7uZG7q1k"
      },
      "source": [
        "With sections (or iterations of a for loop) and `lastprivate`, we can ensure that a variable is updated by the last section (or iterations) in serial program order, not whichever finishes last in real time.\n",
        "\n",
        "Arguably, lastprivate is very rarely useful..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8S2VxRE7qOI"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/lastprivate.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  int x = 0;\n",
        "\n",
        "  #pragma omp parallel sections lastprivate(x)\n",
        "  {\n",
        "    #pragma omp section\n",
        "    { x = 1; printf(\"Section 1: x=%d\\n\", x); }\n",
        "\n",
        "    #pragma omp section\n",
        "    { x = 2; printf(\"Section 2: x=%d\\n\", x); }\n",
        "\n",
        "    #pragma omp section\n",
        "    { x = 3; printf(\"Section 3: x=%d\\n\", x); }\n",
        "  }\n",
        "\n",
        "  printf(\"After sections, x=%d (from the *last* section)\\n\", x);\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzk_KelOV0r8"
      },
      "source": [
        "## **Linked List Traversal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPkcvbyk_oad"
      },
      "source": [
        "###**Exercise 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVCRiCt_oaf"
      },
      "source": [
        "Parallelizing the traversal of a linked list with OpenMP can be highly inefficient.\n",
        "<img align=\"middle\" src=\"https://drive.google.com/uc?id=1BrtuiwIzR2Y-xGPUt3lIX88zEfVF7e_L\">\n",
        "The *task* construct provides a better way to dynamically create concurrent work units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMp2ochj_oag"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/linkedlist.cpp\n",
        "#include <omp.h>\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "struct node {\n",
        "  int data;\n",
        "  int fibdata;\n",
        "  struct node* next;\n",
        "};\n",
        "\n",
        "struct node* init_list(struct node* p);\n",
        "void processwork(struct node* p);\n",
        "int fib(int n);\n",
        "\n",
        "int fib(int n) {\n",
        "  int x, y;\n",
        "  if (n < 2) {\n",
        "    return (n);\n",
        "  } else {\n",
        "    x = fib(n - 1);\n",
        "    y = fib(n - 2);\n",
        "    return (x + y);\n",
        "  }\n",
        "}\n",
        "\n",
        "void processwork(struct node* p) {\n",
        "  int n, temp;\n",
        "  n = p->data;\n",
        "  temp = fib(n);\n",
        "  p->fibdata = temp;\n",
        "}\n",
        "\n",
        "struct node* init_list(struct node* p) {\n",
        "  int i;\n",
        "  struct node* head = NULL;\n",
        "  struct node* temp = NULL;\n",
        "\n",
        "  head = malloc(sizeof(struct node));\n",
        "  p = head;\n",
        "  p->data = 38;\n",
        "  p->fibdata = 0;\n",
        "  for (i = 0; i < 5; i++) {\n",
        "    temp  = malloc(sizeof(struct node));\n",
        "    p->next = temp;\n",
        "    p = temp;\n",
        "    p->data = 38 + i + 1;\n",
        "    p->fibdata = i + 1;\n",
        "  }\n",
        "  p->next = NULL;\n",
        "  return head;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  double start, end;\n",
        "  struct node *p=NULL;\n",
        "  struct node *temp=NULL;\n",
        "  struct node *head=NULL;\n",
        "\n",
        "  printf(\"Process linked list\\n\");\n",
        "  printf(\"  Each linked list node will be processed by function 'processwork()'\\n\");\n",
        "  printf(\"  Each node will compute a subsequent Fibonacci number starting from the 38th\\n\");\n",
        "\n",
        "  p = init_list(p);\n",
        "  head = p;\n",
        "\n",
        "  start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel num_threads(4)\n",
        "  {\n",
        "    #pragma omp master\n",
        "    printf(\"Threads: %d\\n\", omp_get_num_threads());\n",
        "    #pragma omp single\n",
        "    {\n",
        "      printf(\"I am thread %d and I am creating tasks\\n\", omp_get_thread_num());\n",
        "      p = head;\n",
        "      while (p) {\n",
        "        #pragma omp task firstprivate(p) // each task gets is own copy of the pointer to the current list node\n",
        "        {\n",
        "          processwork(p);\n",
        "          printf(\"I am thread %d\\n\", omp_get_thread_num());\n",
        "        }\n",
        "        p = p->next;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  end = omp_get_wtime();\n",
        "  p = head;\n",
        "  while (p != NULL) {\n",
        "    printf(\"%d : %d\\n\",p->data, p->fibdata);\n",
        "    temp = p->next;\n",
        "    free (p);\n",
        "    p = temp;\n",
        "  }\n",
        "  free (p);\n",
        "\n",
        "  printf(\"Compute Time: %f seconds\\n\", end - start);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3DtkUky_oai"
      },
      "source": [
        "Questions:\n",
        "- How many threads create tasks?\n",
        "<!--one, it's inside the \"single\" pragma-->\n",
        "- How are tasks distributed among threads?\n",
        "<!--workpile-style, each thread fetches a new task every time it finishes the current one-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMCfutQOMK33"
      },
      "source": [
        "## **Task Graphs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_q0ejhNQkNg"
      },
      "source": [
        "Recall:\n",
        "- $work$ : total amount of work\n",
        "- $span$ : work on the critical path\n",
        "- $parallelism = work / span$\n",
        "\n",
        "Room for optimization:\n",
        "- reducing the critical path\n",
        "- reducing overhead for anything that is not on the critical path\n",
        "\n",
        "Representation:\n",
        "- nodes: tasks with a certain amount of work to do\n",
        "- directed edges: dependencies between tasks (inbound arrows: what the current task needs to wait for)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X299cr7iiQPg"
      },
      "source": [
        "For completeness, let's also recap a bit of terminology:\n",
        "- when a task creates another task, the creating task becomes the *parent task* of the new task. The new task then is called a *child task* of its parent task. - the term *sibling tasks* refers to all tasks that have the same parent.\n",
        "- a *descendant task* is a task in the ancestor chain of a parent, so either a child task or a task created by a descendant task (e.g., a child task of a child task).\n",
        "- if a new task is put into the task pool, it is said to be *deferred* while if it is executed straight away, it is *undeferred*.\n",
        "- a task is described as *completed* when it has been scheduled for execution and that execution has finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsA7Q777MRxd"
      },
      "source": [
        "### **Exercise 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCVVBl9QMXv_"
      },
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=17PNFB2oQAFEHfvQPflSieUmjQpmLGAmn\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "- Calculate work and parallelism. <!--span=100+250+300, work=860, parallelism=1.32-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are active during the execution of Task 5? <!--1 or 2, depending on the state of T4-->\n",
        "- Is there a better parallel implementation (considering both performance and resource usage)? I.e. do we really need to exploit all parallelism? <!--since W(T4) > W(T2) + W(T3), there is no need to run T2 and T3 in parallel (with the overhead of constructing one more thread), we could just run the sequentially T2 -> T3 and we would still be bond by the amount of work done by T4-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hsINTvraOMq"
      },
      "source": [
        "### **Exercise 6**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWEG4xwoaRuk"
      },
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=15gkHj2zWAZQnuLhmPMWWSjWZqOfkj3MB\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "- Calculate work and span. <!--span=670, work=1045, parallelism=1.56-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- Is this implementation faster than a sequential one? <!--yes, we have parallelism > 1 after all-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jWBaggVd7z0"
      },
      "source": [
        "### **Exercise 7**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8MU2k-Vd7E0"
      },
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1PXlO9Eaxu1lI27k2hYIaWD6y6tLZqDi9\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=100, W(T2)=100, W(T3)=75, W(T4)=50, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "- Calculate work and span. <!--span=400, work=800, parallelism=2-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are needed to achieve the maximum theoretical parallelism? <!--\n",
        "2 threads!\n",
        "For example, thread 1 handles T1, thread 2 handles T2; then thread 1 does T3, T4, and T5 while thread 2 handle T8, then thread 1 handles T6 and thread 2 finishes T7.\n",
        "Note: you can always get away with \"ceil(parallelism)\" threads so long as you assume that a thread can \"yield\" a task, go do some other work, and resume it later. In this case this was not needed since luckly W(T3)+W(T4)+W(T5) = W(T8).\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48oUgM9OorTU"
      },
      "source": [
        "### **Exercise 8**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R96cl-eo8Jz"
      },
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=14BBD6_ctJ-IUH1ubnaF4pMh99LNjX818\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=50, W(T2)=50, W(T3)=200, W(T4)=75, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "- Which dashed arrow prevents from implementing such a task structure with OpenMP? <!--none, there are still no cycles, though the dependency T6->T8 is redundant with T6->T7->T8-->\n",
        "- Remove the dashed arrows, calculate work and span. <!--span=600, work=850, parallelism=1.42-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNlob9bqqybW"
      },
      "source": [
        "### **Exercise 9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGOjC5igqxZ5"
      },
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1flOJlQU5-jo4kLeJmBBuou7HqYYYvQZI\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=50, W(T2)=50, W(T3)=50, W(T4)=75, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "<!--note the redundant dependency T3->T7-->\n",
        "- Calculate work and span. <!--span=500, work=700, parallelism=1.4-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are needed to run the program? <!--ceil(1.4)=2, while T3 and T7 run by themselves in the above path, T4 can be handled by the other thread-->\n",
        "- How many threads could be active during the execution of Task 3? How many during Task 5? <!--during T3, at most 2 (another on T4), during T5, at most 3 (another on T6 and another on T4)-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRiaOvXFMAD5"
      },
      "source": [
        "## **Deadlocks 1o1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8863nHR6MgZG"
      },
      "source": [
        "A very simple example of how NOT to use barriers.\n",
        "<br>\n",
        "Also not how each thread can acquire its own ID. This is written in C++ just to spice things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSCyMObEMGld",
        "outputId": "7cf7133b-8771-4c6e-fc0e-6b9275544e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/deadlock.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/deadlock.cpp\n",
        "#include <iostream>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  #pragma omp parallel default(none) shared(std::cout) num_threads(4)\n",
        "  {\n",
        "    const int thread_num = omp_get_thread_num();\n",
        "\n",
        "    if(thread_num == 0) {\n",
        "      std::cout << \"I'm thread 0 and I caused a deadlock!\" << std::endl;\n",
        "    } else {\n",
        "      #pragma omp barrier\n",
        "    }\n",
        "\n",
        "    #pragma omp critical\n",
        "    std::cout << \"I'm thread \" << thread_num << std::endl;\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hih3-MuMYpj"
      },
      "source": [
        "Compile and run with a timeout (since we know it will deadlock):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zoPuo_NMYSZ",
        "outputId": "42ede49a-7a47-4e5b-b92f-a55be1633e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm thread 0 and I caused a deadlock!\n",
            "I'm thread 0\n",
            "Program was killed by timeout.\n"
          ]
        }
      ],
      "source": [
        "!g++ deadlock.cpp -fopenmp -o deadlock\n",
        "!timeout 4s ./deadlock && echo \"Program finished normally.\" || echo \"Program was killed by timeout.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT3_gN50NYvq"
      },
      "source": [
        "The compile often helps you, however. Something this horrific will not even compile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvyqbpGwNdFw",
        "outputId": "cfc24f21-426a-42a5-e56f-1a58ccaf1a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/horrific_example.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/horrific_example.cpp\n",
        "#include <iostream>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "#pragma omp parallel\n",
        "{\n",
        "#pragma omp single\n",
        "{\n",
        "  std::cout << \"I've caused a deadlock!\\n\";\n",
        "  #pragma omp barrier\n",
        "}\n",
        "}\n",
        "return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXtFElMrF_m8",
        "outputId": "7c19d6ee-4529-43e7-a8b3-31acc740b817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Khorrific_example.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Khorrific_example.cpp:10:61:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kbarrier region may not be closely nested inside of work-sharing, ‘\u001b[01m\u001b[Kloop\u001b[m\u001b[K’, ‘\u001b[01m\u001b[Kcritical\u001b[m\u001b[K’, ‘\u001b[01m\u001b[Kordered\u001b[m\u001b[K’, ‘\u001b[01m\u001b[Kmaster\u001b[m\u001b[K’, explicit ‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ or ‘\u001b[01m\u001b[Ktaskloop\u001b[m\u001b[K’ region\n",
            "   10 |   #pragma omp barrier // <-- the compiler errors out on this\n",
            "      |                                                             \u001b[01;31m\u001b[K^\u001b[m\u001b[K\n"
          ]
        }
      ],
      "source": [
        "!g++ horrific_example.cpp -fopenmp -o horror"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bO_AOMOXxPS"
      },
      "source": [
        "## **Loop Schedules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhUmX8gFX4Qc"
      },
      "source": [
        "Let's see the effect of different loops schedules on an unbalanced loop nest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb_ukK7fX0k0",
        "outputId": "f3c8246e-8242-4fe8-e224-19bcde21da20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/schedules.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/schedules.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 5000 // outer loop iterations\n",
        "#define M 5000 // inner loop iterations\n",
        "\n",
        "#define CHUNK 1000 // chunk size\n",
        "#define THREADS 4  // number of threads\n",
        "\n",
        "int main(void) {\n",
        "  omp_set_nested(true); // enable nested parallelism\n",
        "  omp_set_num_threads(THREADS);\n",
        "\n",
        "  double start, end;\n",
        "  double total = 0.0;\n",
        "\n",
        "  printf(\"OpenMP Nested Loop Parallelization Comparison\\n\");\n",
        "  printf(\"N = %d, M = %d\\nCHUNK = %d, THREADS = %d \\n\\n\", N, M, CHUNK, THREADS);\n",
        "\n",
        "  // nested work-sharing with static schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(static)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    double local_sum = 0.0;\n",
        "    // Note: we CANNOT put another work-sharing construct here like:\n",
        "    // #pragma omp for schedule(static)\n",
        "    // OpenMP forbids two work-sharing constructs back-to-back without a barrier or parallel pragma in-between!\n",
        "    // The intended way to achieve the same result is the 'collapse' clause, see version 4.\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      // fake unbalance: the amount of work depends on i\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        local_sum += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "    #pragma omp atomic\n",
        "    total += local_sum;\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 1 (static, nested for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // nested work-sharing with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(dynamic)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    double local_sum = 0.0;\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        local_sum += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "    #pragma omp atomic\n",
        "    total += local_sum;\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 2 (dynamic, nested for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // nested parallelization with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(static) reduction(+:total)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    // Note: here we can do this, becase we create a nested parallel region\n",
        "    #pragma omp parallel for schedule(static) reduction(+:total)\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 3 (dynamic, nested parallel for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // collapse clause, single parallel for with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for collapse(2) schedule(dynamic, CHUNK)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 4 (dynamic(CHUNK), collapse(2)): %f seconds\\n\", end - start);\n",
        "\n",
        "  // collapse clause, single parallel for with guided schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for collapse(2) schedule(guided)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 5 (guided, collapse(2)): %f seconds\\n\", end - start);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Gk6i7ncx0w"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkeViKuicyEt",
        "outputId": "cc89026a-3123-42c2-e4bf-584e2b55f836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenMP Nested Loop Parallelization Comparison\n",
            "N = 5000, M = 5000\n",
            "CHUNK = 1000, THREADS = 4 \n",
            "\n",
            "Version 1 (static, nested for): 1.903383 seconds\n",
            "Version 2 (dynamic, nested for): 1.865860 seconds\n",
            "Version 3 (dynamic, nested parallel for): 3.145333 seconds\n",
            "Version 4 (dynamic(CHUNK), collapse(2)): 4.494570 seconds\n",
            "Version 5 (guided, collapse(2)): 4.864830 seconds\n"
          ]
        }
      ],
      "source": [
        "!g++ schedules.cpp -fopenmp -o schedules\n",
        "!./schedules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZ6mMXbc8RN"
      },
      "source": [
        "**Version 1**:\n",
        "<br>\n",
        "Only the outer parallel creates a team of threads.\n",
        "\n",
        "- Low overhead, only one parallel team created.\n",
        "- Static scheduling ensures predictable, reproducible thread assignment.\n",
        "- Poor load balancing.\n",
        "\n",
        "**Version 2**:\n",
        "<br>\n",
        "Still a single team of threads.\n",
        "Dynamic scheduling lets threads grab new chunks as they finish, compensates the unbalaned workload.\n",
        "\n",
        "- Load balancing.\n",
        "- More scheduling overhead than static.\n",
        "\n",
        "**Version 3**:\n",
        "<br>\n",
        "The outer parallel for creates one team of threads.\n",
        "Each outer thread that hits the inner parallel for spawns a new inner team.\n",
        "\n",
        "- Full control of both loop levels, each loop can scale and be scheduled independently.\n",
        "- Can exploit more cores if your hardware and OpenMP's runtime support nested teams efficiently.\n",
        "- High overhead: every outer iteration spawns a new parallel region.\n",
        "- Memory and scheduling overhead outweigh benefits for small and medium-sized workloads.\n",
        "- Likely to oversubscribe cores.\n",
        "\n",
        "**Version 4**:\n",
        "<br>\n",
        "The two loops are merged into a single iteration space of size N*M.\n",
        "One parallel team handles all (i, j) pairs directly.\n",
        "\n",
        "- More fine grained load balancing when combined with the dynamic schedule.\n",
        "- Scheduling overhead can be more effectively controlled with larger chunk sizes.\n",
        "- Only works if the nested loops are perfectly nested and independent.\n",
        "\n",
        "**Version 5**:\n",
        "<br>\n",
        "Same as version 4, but guided scheduling means that when there is a lot of work left to do, each thread is assigned a larger portion of it, with smaller and smaller portions being assigned as less work remains.\n",
        "\n",
        "- Very fine grained load balancing.\n",
        "- Extremely reduced scheduling overhead, as we issue more jobs only towards the end, to avoid leaving some threads at idle.\n",
        "\n",
        "Version 5 is the cleanest and often fastest option.\n",
        "<br>\n",
        "*Note: this may not show on Colab, as it gives us only 2 mere cores...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkSRzD3MZvHO"
      },
      "source": [
        "*Note: in version 1 and 2, a reduction over `local_sum` is not needed, because the inner `#pragma omp for` is not creating a new parallel region, it is simply another work-sharing construct within the same parallel team spawned by the outer pragma.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUbYFwk8ktfn"
      },
      "source": [
        "Questions:\n",
        "\n",
        "If you are told that exactly one iteration every 10 needs to do 5x the work, and you have thousands of iterations, what is the best schedule and why?\n",
        "<!--\"static\", with batch multiple of 10, large enough to exploit caches, but not too large as not to cause severe unbalance in the number iterations given to each thread-->\n",
        "\n",
        "Assume now a workload consisting of roughly 2-3x as many iterations as there will be threads (e.g. 25 iterations, 10 threads), you know that each iteration has a one-in-three chance of requiring twice the amount of work as would a normal iteration. What schedule would work best and why?\n",
        "<!--\"dynamic\", chunks of 1, atmost 2, iterations, because with this little iterations the scheduling overhead is negligible and using larger chunks or \"guided\" could not effectively mitigate the unbalance due to the high likelyhood of heavier iterations-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG9wQSjQou7U"
      },
      "source": [
        "## **Déjà-vu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHrlSXMjov0a"
      },
      "source": [
        "### **Vector Product**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIylO0rkoy86"
      },
      "source": [
        "Just another way to see yet another reduce, but now with OpenMP pragmas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI0oQxLCo1Qc"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/vector_vector_prod.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  int N = 1000000;\n",
        "\n",
        "  double *A = (double*) malloc(N * sizeof(double));\n",
        "  double *B = (double*) malloc(N * sizeof(double));\n",
        "\n",
        "  // initialize vectors\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    A[i] = i * 0.001;\n",
        "    B[i] = (N - i) * 0.002;\n",
        "  }\n",
        "\n",
        "  double dot = 0.0;\n",
        "  double start_time = omp_get_wtime();\n",
        "\n",
        "  // parallel reduce\n",
        "  #pragma omp parallel for reduction(+ : dot) schedule(static)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    dot += A[i] * B[i];\n",
        "  }\n",
        "\n",
        "  double end_time = omp_get_wtime();\n",
        "\n",
        "  printf(\"Dot product = %.5f\\n\", dot);\n",
        "  printf(\"Computed in %.5f seconds using %d threads.\\n\",\n",
        "  end_time - start_time, omp_get_max_threads());\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdP1O3Oeo88A"
      },
      "source": [
        "### **MatMul**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qClT8PhKo_WJ"
      },
      "source": [
        "Let's just make each thread perform a vector-vector product!\n",
        "<br>\n",
        "However, we can try to exploit thread affinity to slightly improve cache performance by manually tiling the loop!\n",
        "<br>\n",
        "Say for example that we have a dual-socket motherboard with two 8-core CPUs, with each CPU itself divided in two NUMA nodes (e.g. each group of 4 cores has its own L2 cache and dedicated DRAM channel).\n",
        "<br>\n",
        "It's better if we split the work in 16 chunks, space far and wide 4 groups of those chunks, and then pull close the 4 chunks in each group.\n",
        "This equates to cutting the output matrix's rows and cols in 4 equi-sized groups. This results in 16 chunks. Then we give each of the 4 quadrants of the output matrix to a different NUMA node and each quarter of the quadrant to a thread.\n",
        "\n",
        "*Note: this is FAR from the best way to do a matmul on CPU! There are several improvements among which \"blocking\", a similar idea to tiling to better exploit caches!*\n",
        "<br>\n",
        "*More on this here:*\n",
        "- *BLAS: https://www.netlib.org/blas/*\n",
        "- *BLIS: https://dl.acm.org/doi/10.1145/2764454*\n",
        "- *More on BLIS: https://dl.acm.org/doi/10.1145/2925987*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8QC-CbXrMuy",
        "outputId": "249a9b57-b27b-47a4-e558-c88c880a29ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/matmul.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/matmul.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "void init_matrix(double *M, int N, double scale) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      M[i * N + j] = scale * ((i + j) % 100);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "  int N = 1024;\n",
        "  if (argc > 1) N = atoi(argv[1]);\n",
        "\n",
        "  printf(\"Matrix size: %d x %d\\n\", N, N);\n",
        "\n",
        "  double *A = (double*) malloc(N * N * sizeof(double));\n",
        "  double *B = (double*) malloc(N * N * sizeof(double));\n",
        "  double *C = (double*) calloc(N * N, sizeof(double));\n",
        "\n",
        "  init_matrix(A, N, 0.01);\n",
        "  init_matrix(B, N, 0.02);\n",
        "\n",
        "  int groups = 4;            // outer level (NUMA groups)\n",
        "  int threads_per_group = 4; // inner level threads per group\n",
        "  int tile_size = N / 4;     // 4 x 4 grid of tiles\n",
        "\n",
        "  omp_set_nested(1); // enable nested parallelism\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  // outer parallel region: spread affinity\n",
        "  #pragma omp parallel num_threads(groups) proc_bind(spread)\n",
        "  {\n",
        "    int gi = omp_get_thread_num(); // group index\n",
        "    int i_start = gi * tile_size;\n",
        "    int i_end   = (gi + 1) * tile_size;\n",
        "\n",
        "    // inner parallel region: close affinity within group\n",
        "    #pragma omp parallel num_threads(threads_per_group) proc_bind(close)\n",
        "    {\n",
        "      int gj = omp_get_thread_num(); // tile index within group\n",
        "      int j_start = gj * tile_size;\n",
        "      int j_end = (gj + 1) * tile_size;\n",
        "\n",
        "      for (int i = i_start; i < i_end; i++) {\n",
        "        for (int j = j_start; j < j_end; j++) {\n",
        "          double sum = 0.0;\n",
        "          for (int k = 0; k < N; k++) {\n",
        "            sum += A[i * N + k] * B[k * N + j];\n",
        "          }\n",
        "          C[i * N + j] = sum;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.3f s\\n\", t2 - t1);\n",
        "\n",
        "  free(A); free(B); free(C);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICxNoujdDnYB"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7o7HTGADno8",
        "outputId": "6470bb51-1e06-4215-a651-1fefb58b888d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix size: 1024 x 1024\n",
            "Execution time: 14.621 s\n"
          ]
        }
      ],
      "source": [
        "!g++ matmul.cpp -fopenmp -o matmul\n",
        "!./matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F8IVKyzrPQa"
      },
      "source": [
        "### **Histogram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjgZFPH4rT7m"
      },
      "source": [
        "Shared memory and atomics, here we go again!\n",
        "<br>\n",
        "Remember to rely on privatization!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65hQ1asHrhbx",
        "outputId": "a0669080-dd1c-470d-a734-0bcbcd9f5416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/histogram.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/histogram.cpp\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define NUM_LETTERS 26\n",
        "#define BIN_SIZE 4\n",
        "#define NUM_BINS ((NUM_LETTERS + BIN_SIZE - 1) / BIN_SIZE)\n",
        "\n",
        "void generate_random_string(char *s, size_t len) {\n",
        "  for (size_t i = 0; i < len; i++)\n",
        "    s[i] = 'a' + (rand() % NUM_LETTERS);\n",
        "  s[len] = '\\0';\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "  size_t N = 10000000;\n",
        "\n",
        "  char *text = (char*) malloc(N + 1);\n",
        "  srand(42);\n",
        "  generate_random_string(text, N);\n",
        "\n",
        "  // global copy\n",
        "  int global_hist[NUM_BINS] = {0};\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    // private copy\n",
        "    int local_hist[NUM_BINS] = {0};\n",
        "\n",
        "    #pragma omp for\n",
        "    for (size_t i = 0; i < N; i++) {\n",
        "      char c = text[i];\n",
        "      if (c >= 'a' && c <= 'z') {\n",
        "        int bin = (c - 'a') / BIN_SIZE;\n",
        "        local_hist[bin]++;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // merge private copies atomically\n",
        "    for (int b = 0; b < NUM_BINS; b++) {\n",
        "      #pragma omp atomic\n",
        "      global_hist[b] += local_hist[b];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.4f s using %d threads\\n\", t2 - t1, omp_get_max_threads());\n",
        "\n",
        "  // print histogram\n",
        "  printf(\"\\nHistogram (%u-letter bins):\\n\", BIN_SIZE);\n",
        "  for (int b = 0; b < NUM_BINS; b++) {\n",
        "    char start = 'a' + b * BIN_SIZE;\n",
        "    char end   = start + BIN_SIZE - 1;\n",
        "    if (end > 'z') end = 'z';\n",
        "      printf(\"  %c-%c : %d\\n\", start, end, global_hist[b]);\n",
        "  }\n",
        "\n",
        "  free(text);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IpzOOeWGhnd"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKJxdBCwGfVx",
        "outputId": "866a4134-4b96-4c96-f02c-fe73ca60b2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 0.0370 s using 2 threads\n",
            "\n",
            "Histogram (4-letter bins):\n",
            "  a-d : 1538630\n",
            "  e-h : 1538792\n",
            "  i-l : 1538822\n",
            "  m-p : 1539186\n",
            "  q-t : 1536136\n",
            "  u-x : 1539567\n",
            "  y-z : 768867\n"
          ]
        }
      ],
      "source": [
        "!g++ histogram.cpp -fopenmp -o histogram\n",
        "!./histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSo7Gd7yiB7s"
      },
      "source": [
        "Question:\n",
        "- what would be a good schedule for the parallel for? <!--static (at most guided) since all iterations are perfectly balanced, we just need to worry about maximizing the cache hits of each thread, so chunks should be reasonably larger while not becoming uneven-->\n",
        "- is privatization always the best option? <!--no, say that you are counting the occurrencies of 10M items, that are uniformly distributed in the input, then atomic updates on a single shared copy could be fast enough to justify saving the cost of replicating all counters-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnYd8YG3Bqyk"
      },
      "source": [
        "## **Sections and Critical Sections**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F97zRCGhCFWA"
      },
      "source": [
        "There are two very apparent ways to optimize this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JavpJFVCDwM",
        "outputId": "927eb061-de4f-4b7d-e0be-5bab9243d5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/two_loops.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/two_loops.cpp\n",
        "#include <cstdio>\n",
        "#include <omp.h>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "  const int N = 10000000;\n",
        "  std::vector<double> A(N, 0.0), B(N, 0.0);\n",
        "  double global_sum_A = 0.0;\n",
        "  double global_sum_B = 0.0;\n",
        "\n",
        "  double start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel default(none) shared(A, B, N, global_sum_A, global_sum_B)\n",
        "  {\n",
        "    double local_sum = 0.0;\n",
        "\n",
        "    // first loop: compute A[i]\n",
        "    #pragma omp for schedule(static)\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      A[i] = std::sin(i * 0.001);\n",
        "      local_sum += A[i];\n",
        "    }\n",
        "\n",
        "    // conditional accumulation on A\n",
        "    #pragma omp critical\n",
        "    {\n",
        "      if (global_sum_A < 100.0)\n",
        "        global_sum_A += local_sum;\n",
        "    }\n",
        "\n",
        "    // synchronize before the next loop\n",
        "    #pragma omp barrier\n",
        "    local_sum = 0.0;\n",
        "\n",
        "    // second loop: compute B[i]\n",
        "    #pragma omp for schedule(static)\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      B[i] = std::cos(i * 0.001);\n",
        "      local_sum += B[i];\n",
        "    }\n",
        "\n",
        "    // conditional accumulation on B\n",
        "    #pragma omp critical\n",
        "    {\n",
        "      if (global_sum_B < 100.0)\n",
        "        global_sum_B += local_sum;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double end = omp_get_wtime();\n",
        "\n",
        "  printf(\"Global sums: A = %.3f, B = %.3f\\n\", global_sum_A, global_sum_B);\n",
        "  printf(\"Total time: %.4f seconds\\n\", end - start);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31K4qkd1Djxy"
      },
      "source": [
        "Observe that the two loops (and subsequent critical sections) have no dependency with one-another, they could run concurrently!\n",
        "\n",
        "What we need to do is:\n",
        "- run each loop in its own parallel section\n",
        "- remove the barrier between them\n",
        "- name the two critical sections to make them independent\n",
        "- now however we shared work among sections, to also share the work of each loop's iterations, we need a nested parallel region for each loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LLbr0JpEumU",
        "outputId": "504b3c7b-dfed-42e6-d373-0e44ce6247e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/two_loops.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/two_loops.cpp\n",
        "#include <cstdio>\n",
        "#include <omp.h>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "  omp_set_nested(true);\n",
        "\n",
        "  const int N = 10000000;\n",
        "  std::vector<double> A(N, 0.0), B(N, 0.0);\n",
        "  double global_sum_A = 0.0;\n",
        "  double global_sum_B = 0.0;\n",
        "\n",
        "  double start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel default(none) shared(A, B, N, global_sum_A, global_sum_B)\n",
        "  {\n",
        "    #pragma omp sections\n",
        "    {\n",
        "      // first loop: compute A[i]\n",
        "      #pragma omp section\n",
        "      {\n",
        "        double local_sum_A = 0.0;\n",
        "        #pragma omp parallel for reduction(+:local_sum_A) schedule(static)\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "          A[i] = std::sin(i * 0.001);\n",
        "          local_sum_A += A[i];\n",
        "        }\n",
        "\n",
        "        #pragma omp critical (acc_A)\n",
        "        {\n",
        "          if (global_sum_A < 100.0)\n",
        "            global_sum_A += local_sum_A;\n",
        "        }\n",
        "      }\n",
        "\n",
        "      // second loop: compute B[i]\n",
        "      #pragma omp section\n",
        "      {\n",
        "        double local_sum_B = 0.0;\n",
        "        #pragma omp parallel for reduction(+:local_sum_B) schedule(static)\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "          B[i] = std::cos(i * 0.001);\n",
        "          local_sum_B += B[i];\n",
        "        }\n",
        "\n",
        "        #pragma omp critical (acc_B)\n",
        "        {\n",
        "          if (global_sum_B < 100.0)\n",
        "            global_sum_B += local_sum_B;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double end = omp_get_wtime();\n",
        "\n",
        "  printf(\"Global sums: A = %.3f, B = %.3f\\n\", global_sum_A, global_sum_B);\n",
        "  printf(\"Total time: %.4f seconds\\n\", end - start);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K_AXl_pF7EH"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GeweCvNF7Ut",
        "outputId": "8eb85080-e2fa-44ff-a467-e635f0d7cdc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global sums: A = 1952.308, B = -304.638\n",
            "Total time: 0.4028 seconds\n"
          ]
        }
      ],
      "source": [
        "!g++ two_loops.cpp -fopenmp -o two_loops\n",
        "!./two_loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVxLNFXusHkQ"
      },
      "source": [
        "## **Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oYN1FLfsKDt"
      },
      "source": [
        "### **MergeSort**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnz7stMsLsc"
      },
      "source": [
        "### **Algebraic Expression Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo-Eulc7eOlu"
      },
      "source": [
        "Let's evaluate this linar algebra expression with OpenMP tasks:\n",
        "\n",
        "$A, B, C, D, E \\in \\mathbb{R}^{4 \\times 4}$\n",
        "\n",
        "$A \\cdot (A \\cdot B - C \\cdot D) - E^2 \\cdot D^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bl4ZHYOejeZ"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/algebra.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 4\n",
        "\n",
        "void matmul(double A[N][N], double B[N][N], double C[N][N]) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      double s = 0.0;\n",
        "      for (int k = 0; k < N; k++) s += A[i][k] * B[k][j];\n",
        "      C[i][j] = s;\n",
        "    }\n",
        "}\n",
        "\n",
        "// alpha = +1 => add, alpha = -1 => sub\n",
        "void matadd(double A[N][N], double B[N][N], double C[N][N], double alpha) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      C[i][j] = A[i][j] + alpha * B[i][j];\n",
        "}\n",
        "\n",
        "void init(double M[N][N], double scale) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      M[i][j] = scale * ((i + j) % 7);\n",
        "}\n",
        "\n",
        "void printmat(const char *name, double M[N][N]) {\n",
        "  printf(\"%s:\\n\", name);\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < N; j++)\n",
        "      printf(\"%6.2f \", M[i][j]);\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  double A[N][N], B[N][N], C[N][N], D[N][N], E[N][N];\n",
        "  double AB[N][N], CD[N][N], inner[N][N], Aterm[N][N];\n",
        "  double E2[N][N], D2[N][N], E2D2[N][N], Result[N][N];\n",
        "\n",
        "  init(A, 0.5); init(B, 0.7); init(C, 0.9);\n",
        "  init(D, 1.1); init(E, 1.3);\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  #pragma omp single\n",
        "  {\n",
        "    // AB = A*B\n",
        "    #pragma omp task depend(out:AB)\n",
        "    matmul(A, B, AB);\n",
        "\n",
        "    // CD = C*D\n",
        "    #pragma omp task depend(out:CD)\n",
        "    matmul(C, D, CD);\n",
        "\n",
        "    // inner = AB - CD\n",
        "    #pragma omp task depend(in:AB, CD) depend(out:inner)\n",
        "    matadd(AB, CD, inner, -1.0);\n",
        "\n",
        "    // Aterm = A * inner\n",
        "    #pragma omp task depend(in:A, inner) depend(out:Aterm)\n",
        "    matmul(A, inner, Aterm);\n",
        "\n",
        "    // E2 = E*E\n",
        "    #pragma omp task depend(out:E2)\n",
        "    matmul(E, E, E2);\n",
        "\n",
        "    // D2 = D*D\n",
        "    #pragma omp task depend(out:D2)\n",
        "    matmul(D, D, D2);\n",
        "\n",
        "    // E2D2 = E2 * D2\n",
        "    #pragma omp task depend(in:E2, D2) depend(out:E2D2)\n",
        "    matmul(E2, D2, E2D2);\n",
        "\n",
        "    // Result = Aterm - E2D2\n",
        "    #pragma omp task depend(in:Aterm, E2D2) depend(out:Result)\n",
        "    matadd(Aterm, E2D2, Result, -1.0);\n",
        "\n",
        "    #pragma omp taskwait\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.3f ms\\n\", 1e3 * (t2 - t1));\n",
        "\n",
        "  printmat(\"Result\", Result);\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q2xJu8zejsI"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYGgIxHjej6O"
      },
      "outputs": [],
      "source": [
        "!g++ algebra.cpp -fopenmp -o algebra\n",
        "!./algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhkFvLvdf1cO"
      },
      "source": [
        "### **Sum of Products**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jss7qwm0f4M2"
      },
      "source": [
        "Assume you are given strings repersenting algebraic expressions in the form of a sums of products with only 2 possible variable, \"a\" and \"b\", like:\n",
        "<br>\n",
        "\"a+a\\*b\\*b+b\\*b\\*a\\*b\\*a+b\\*b\\*b\"\n",
        "<br>\n",
        "The following program parses the expression and dynamically spawns an OpenMP task to solve each product, then reducing among those tasks to do the sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3BEHehGgr0N"
      },
      "outputs": [],
      "source": [
        "%%writefile /home/OpenMP/sop.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <omp.h>\n",
        "\n",
        "// Evaluate a product term like \"a*b*b*a\"\n",
        "float eval_product(const char *term, float a, float b) {\n",
        "    float res = 1.0f;\n",
        "    for (const char *p = term; *p; p++) {\n",
        "        if (*p == 'a') res *= a;\n",
        "        else if (*p == 'b') res *= b;\n",
        "    }\n",
        "    return res;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const char *expr = \"a*b*b*a+a*a+b*b\";\n",
        "    float a = 2.0f, b = 3.0f;\n",
        "\n",
        "    printf(\"Expression: %s\\n\", expr);\n",
        "    printf(\"a = %.2f, b = %.2f\\n\", a, b);\n",
        "\n",
        "    // Split the expression into product terms by '+'\n",
        "    char *expr_copy = strdup(expr);\n",
        "    char *terms[64];\n",
        "    int n_terms = 0;\n",
        "    for (char *tok = strtok(expr_copy, \"+\"); tok; tok = strtok(NULL, \"+\"))\n",
        "        terms[n_terms++] = tok;\n",
        "\n",
        "    float partials[64] = {0.0f};\n",
        "    float result = 0.0f;\n",
        "\n",
        "    double t1 = omp_get_wtime();\n",
        "\n",
        "    #pragma omp parallel\n",
        "    #pragma omp single\n",
        "    {\n",
        "        // spawn a parsing task that spawns product tasks\n",
        "        #pragma omp task depend(out:partials)\n",
        "        {\n",
        "            for (int i = 0; i < n_terms; i++) {\n",
        "                int idx = i;\n",
        "                const char *term = terms[i];\n",
        "                #pragma omp task firstprivate(idx, term) depend(out:partials[idx])\n",
        "                {\n",
        "                    partials[idx] = eval_product(term, a, b);\n",
        "                    // Optional diagnostic:\n",
        "                    // printf(\"Term %d (%s) = %.2f\\n\", idx, term, partials[idx]);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // reduction task that depends on all partials\n",
        "        #pragma omp task depend(in:partials) depend(out:result)\n",
        "        {\n",
        "            float sum = 0.0f;\n",
        "            for (int i = 0; i < n_terms; i++) sum += partials[i];\n",
        "            result = sum;\n",
        "        }\n",
        "\n",
        "        #pragma omp taskwait\n",
        "    }\n",
        "\n",
        "    double t2 = omp_get_wtime();\n",
        "\n",
        "    printf(\"Result = %.2f\\n\", result);\n",
        "    printf(\"Execution time: %.3f ms\\n\", 1e3 * (t2 - t1));\n",
        "\n",
        "    free(expr_copy);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMtqdvwCgsCo"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgGxzZhPgscB"
      },
      "outputs": [],
      "source": [
        "!g++ sop.cpp -fopenmp -o sop\n",
        "!./sop a*b*b*a+a*a+b*b+a*b*b*a*a*a+b*b*b*b*b+a*b*a*a*b*a+a*a+b*b*a*a*a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWCs-sLdjTx-"
      },
      "source": [
        "### **MatMul with Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6WkolDPjaIT"
      },
      "source": [
        "Here we see a slightly new pragma, `taskloop`: splits the iteration space of a loop into OpenMP tasks.\n",
        "While this looks similar to the `for` worksharing construct, the behavior is fundamentally different.\n",
        "When using the `for` construct, all threads in the parallel region have to encounter the construct so that they can split up the work, whereas the taskloop construct needs only be executed by a single thread (e.g. the master thread).\n",
        "\n",
        "*Note: the taskloop construct is defined in a way that is similar to the definition of a regular task.\n",
        "This means that if N threads encounter the construct, each of the threads will start executing the same loop, so the loop will be executed N times rather than having a single incarnation of the loop split between them.*\n",
        "\n",
        "The `grainsize` clause defines how many iterations should be executed per task.\n",
        "Funnily enough, the OpenMP standard allows this to be an interval, which in this example will be eight to sixteen iterations, so an implementation has some flexibility in choosing the exact number of iterations (and, therefore, tasks).\n",
        "The construct also supports the `num_tasks` clause, which specifies exactly how many tasks should be created, and then adjusts the chunk size accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6zEnj11jZqJ",
        "outputId": "fa07bbdd-fa75-4932-9f63-b35092491632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/OpenMP/tmatmul.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/tmatmul.cpp\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <omp.h>\n",
        "\n",
        "void matmul_taskloop(float *C, const float *A, const float *B, size_t n) {\n",
        "  #pragma omp parallel firstprivate(n)\n",
        "  {\n",
        "    #pragma omp master\n",
        "    {\n",
        "      #pragma omp taskloop firstprivate(n) grainsize(8)\n",
        "      for (int i = 0; i < n; ++i)\n",
        "        for (int k = 0; k < n; ++k)\n",
        "          for (int j = 0; j < n; ++j)\n",
        "            C[i * n + j] += A[i * n + k] * B[k * n + j];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void init_matrix(float *M, int n, float scale) {\n",
        "  for (int i = 0; i < n; i++)\n",
        "    for (int j = 0; j < n; j++)\n",
        "      M[i*n + j] = scale * float((i + j) % 13);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int n = 512;\n",
        "\n",
        "  printf(\"Matrix size: %d x %d\\n\", n, n);\n",
        "\n",
        "  float *A = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "  float *B = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "  float *C = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "\n",
        "  init_matrix(A, n, 0.01f);\n",
        "  init_matrix(B, n, 0.02f);\n",
        "  init_matrix(C, n, 0.0f);\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "  matmul_taskloop(C, A, B, n);\n",
        "  double t2 = omp_get_wtime();\n",
        "\n",
        "  printf(\"Time: %.3f sec\\n\", t2 - t1);\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "  free(C);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pxw3kqU_toA"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uLocZpO_tIS",
        "outputId": "dda6a8d3-eb19-4e25-d3e2-e89579cb108e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix size: 512 x 512\n",
            "Time: 0.707 sec\n"
          ]
        }
      ],
      "source": [
        "!g++ tmatmul.cpp -fopenmp -o tmatmul\n",
        "!./tmatmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBPzM7QgNMx"
      },
      "source": [
        "## **Inspect The Hardware**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSsfp_hzgQW4"
      },
      "source": [
        "See what the machine we are using (here on Colab) is capable of:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVxC1SOogTon",
        "outputId": "fb75f056-f27a-4ff6-9f1b-650014aff115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /home/OpenMP/inspect_hw.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile /home/OpenMP/inspect_hw.cpp\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "#include <omp.h>\n",
        "#include <unistd.h>\n",
        "#include <thread>\n",
        "\n",
        "// helper to read cache info from Linux sysfs\n",
        "long read_cache_size(int level) {\n",
        "    std::string path = \"/sys/devices/system/cpu/cpu0/cache/index\" + std::to_string(level) + \"/size\";\n",
        "    std::ifstream file(path);\n",
        "    if (!file.is_open()) return -1;\n",
        "    std::string value;\n",
        "    file >> value;\n",
        "    long size = std::stol(value);\n",
        "    if (value.find('K') != std::string::npos) size *= 1024;\n",
        "    if (value.find('M') != std::string::npos) size *= 1024 * 1024;\n",
        "    return size;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // CPU and threading info\n",
        "  int omp_procs = omp_get_num_procs();\n",
        "  int omp_max_threads = omp_get_max_threads();\n",
        "  unsigned int hw_threads = std::thread::hardware_concurrency();\n",
        "\n",
        "  std::cout << \"=== System Info (OpenMP + Hardware) ===\\n\";\n",
        "  std::cout << \"Logical processors available (OpenMP): \" << omp_procs << \"\\n\";\n",
        "  std::cout << \"Max OpenMP threads: \" << omp_max_threads << \"\\n\";\n",
        "  std::cout << \"Hardware concurrency (std::thread): \" << hw_threads << \"\\n\";\n",
        "\n",
        "  // hyperthreading available if hardware threads > physical cores\n",
        "  if (hw_threads > omp_procs / 2)\n",
        "    std::cout << \"Hyperthreading likely enabled.\\n\";\n",
        "  else\n",
        "    std::cout << \"No hyperthreading detected (or not applicable).\\n\";\n",
        "\n",
        "  // cache info\n",
        "  for (int i = 0; i < 3; ++i) {\n",
        "    long size = read_cache_size(i);\n",
        "    if (size > 0)\n",
        "      std::cout << \"L\" << (i + 1) << \" cache size: \" << size / 1024 << \" KB\\n\";\n",
        "  }\n",
        "\n",
        "  // RAM info\n",
        "  long pages = sysconf(_SC_PHYS_PAGES);\n",
        "  long page_size = sysconf(_SC_PAGE_SIZE);\n",
        "  double total_ram_gb = (double)pages * page_size / (1024.0 * 1024.0 * 1024.0);\n",
        "  std::cout << \"Total physical memory: \" << total_ram_gb << \" GB\\n\";\n",
        "\n",
        "  // OpenMP runtime confirmation\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    #pragma omp single\n",
        "    std::cout << \"Actual threads used by default by OpenMP: \" << omp_get_num_threads() << \"\\n\";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lwy9qAHhBfq"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKCbPzBphCzq",
        "outputId": "82a8b74d-7ee8-4e97-ff08-6ce79e8a7082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== System Info (OpenMP + Hardware) ===\n",
            "Logical processors available (OpenMP): 2\n",
            "Max OpenMP threads: 2\n",
            "Hardware concurrency (std::thread): 2\n",
            "Hyperthreading likely enabled.\n",
            "L1 cache size: 32 KB\n",
            "L2 cache size: 32 KB\n",
            "L3 cache size: 256 KB\n",
            "Total physical memory: 12.6714 GB\n",
            "Actual threads used by default by OpenMP: 2\n"
          ]
        }
      ],
      "source": [
        "!g++ inspect_hw.cpp -fopenmp -o inspect_hw\n",
        "!./inspect_hw"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rodBOxsf5GC1",
        "nVfSitYV5Prt",
        "gvZp4-7XMgTp",
        "44Sjhyc_OfLP",
        "gJzYN_EF4pVf",
        "4QiOjWEyRafk",
        "_nGEtHLqRlZn",
        "E9y1aoBb7jWg",
        "Bzk_KelOV0r8",
        "UMCfutQOMK33",
        "tRiaOvXFMAD5",
        "7bO_AOMOXxPS",
        "XG9wQSjQou7U",
        "cdP1O3Oeo88A",
        "NnYd8YG3Bqyk",
        "dLnz7stMsLsc",
        "UhkFvLvdf1cO",
        "CWCs-sLdjTx-",
        "kGBPzM7QgNMx"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
