{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rodBOxsf5GC1",
        "nVfSitYV5Prt",
        "gvZp4-7XMgTp",
        "44Sjhyc_OfLP",
        "gJzYN_EF4pVf",
        "4QiOjWEyRafk",
        "_nGEtHLqRlZn",
        "E9y1aoBb7jWg",
        "Bzk_KelOV0r8",
        "UMCfutQOMK33",
        "tRiaOvXFMAD5",
        "7bO_AOMOXxPS",
        "XG9wQSjQou7U",
        "PHrlSXMjov0a",
        "cdP1O3Oeo88A",
        "1F8IVKyzrPQa",
        "NnYd8YG3Bqyk",
        "ZVxLNFXusHkQ",
        "4oYN1FLfsKDt",
        "dLnz7stMsLsc",
        "UhkFvLvdf1cO",
        "CWCs-sLdjTx-",
        "FWXcfjqOZbId",
        "4Z6q5N1tL_T5",
        "BI3NPZLOXD_z",
        "km7PBnWkoGyz",
        "lq5LODvF9bE7",
        "OLFlk37vB5ju",
        "SELVL1FtttFr",
        "CdXuPYMmhlL5",
        "g3H6zk5BMJW6",
        "zHsMPZZ2SJsE",
        "BOJ_-nXTP_eh",
        "SCUk7-QeEXiE",
        "8KLXCFxzpub9",
        "yK94no68BRls",
        "gg5jAluslAFz",
        "iyQQDcDr14D-",
        "kGBPzM7QgNMx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thomas-Fabbris/parallel-computing-polimi/blob/main/OPENMP/OpenMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOIneyui3qOv"
      },
      "source": [
        "# **OpenMP**\n",
        "A macro-based approach for expressing thread level parallelism in C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "rodBOxsf5GC1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2nasZca3oIb"
      },
      "source": [
        "%%capture\n",
        "!apt install build-essential libomp-dev\n",
        "!mkdir /home/OpenMP\n",
        "%cd /home/OpenMP"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Glossary**"
      ],
      "metadata": {
        "id": "nVfSitYV5Prt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenMP hides from the programmer all the pedantic complexity of managing POSIX threads, deferring that to the compiler and exposing a simple set of directives/pragmas to specify how the code needs to be parallelized.\n",
        "For instance, in the OpenMP model, the fork and join operations happen automatically at the stard and end of parallel constructs.\n",
        "Similarly, using locks just requires denoting the critical sections of the code that must be protected by one.\n",
        "\n",
        "Some good doc can be found here: https://rookiehpc.org/openmp/docs/index.html\n",
        "<br>\n",
        "Another good introduction is this on: https://github-pages.ucl.ac.uk/research-computing-with-cpp/08openmp/02_intro_openmp.html\n",
        "\n",
        "### Pragma Syntax\n",
        "\n",
        "OpenMP directives in C/C++ use the following general syntax (square brackets denote optional parts):\n",
        "\n",
        "```\n",
        "#pragma omp <directive> [clause[[,] clause] ...]\n",
        "```\n",
        "\n",
        "Clauses that refine the behavior of the directive to which they are applied.\n",
        "\n",
        "Common clauses:\n",
        "\n",
        "- `num_threads(n)` : specify the number of threads to use.\n",
        "- `nowait` : remove the implicit barrier at the end of a construct.\n",
        "- `if(cond)` : execute in parallel only if the condition is true.\n",
        "\n",
        "Data sharing clauses:\n",
        "\n",
        "- `private(varlist)` : each thread has its own uninitialized copy of listed variables.\n",
        "- `firstprivate(varlist)` : like `private`, but each copy is initialized with the original value.\n",
        "- `lastprivate(varlist)` : copies the value from the last iteration or section (lexicographically - w.r.t. the order as written in the code) back to the original variable; a very simple way to avoid concurrent writes.\n",
        "- `shared(varlist)` : variables are shared among all threads.\n",
        "- `reduction(operator : varlist)` : perform a reduction operation across threads on the given variables.\n",
        "- `default(shared|private|firstprivate|lastprivate|none)` : defines the default sharing type for variables in the region; if not specified, it is `shared`; if set to `none` the compiler forces you to manually specify a sharing clause for each variable access by the thread.\n",
        "\n",
        "Some pragmas are declarative and can sit anywhere in the serial part of the code (e.g. `omp threadprivate`), others, like worksharing ones (e.g. `omp section`), act on the block that immediately follows.\n",
        "Thus, if such a pragma preceeds a control statement, it acts on its code block, otherwise a block can be induced manually with a pair of `{}`.\n",
        "\n",
        "*Note: in C/C++ a **structured block**, is defined as a single statement or a sequence of statements that is enclosed in curly braces. It shall be such that execution may never branch into the sequence or out of it, going through it entirely after entering it. This is sometimes called \"Single Entry, Single Exit\" (SESE). A multi-statement block often coincides with a scope.*\n",
        "\n",
        "Example:\n",
        "```\n",
        "int a = 1, b, c, d, s;\n",
        "#pragma omp parallel for default(private) firstprivate(a, b) lastprivate(c) shared(s) num_threads(10)\n",
        "for (int i = 0; i < 10; ++i) {\n",
        "  a += 1; // everyone increments their 'a' copy from 1 to 2\n",
        "  c = i;  // the master thread's 'c' will be 9 after the loop\n",
        "  d = 4;  // everyone initializes its copy of 'd' to 4\n",
        "  s = i;  // race condition for who will write the final 's'\n",
        "}\n",
        "```\n",
        "\n",
        "Threadprivate variables:\n",
        "\n",
        "- Declared with: `#pragma omp threadprivate(varlist)`\n",
        "- Define global or file-scope variables private to each thread across parallel regions.\n",
        "- Unlike `private`, their value persists across parallel regions.\n",
        "- Use the `copyin(varlist)` clause to initialize threadprivate variables in each thread from the master thread.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Pragmas\n",
        "\n",
        "The key parallel constructs:\n",
        "\n",
        "- `#pragma omp parallel` : start a parallel region executed by a team of multiple threads.\n",
        "\n",
        "A team is a group of threads that work together to execute a region of code.\n",
        "Each team has one master thread (the thread that encounters the parallel directive) and zero or more additional worker threads.\n",
        "Within a team, threads can execute work concurrently using work-sharing directives or further nested parallel constructs.\n",
        "When the parallel region ends, the team disbands, and execution continues with a single thread (the master).\n",
        "\n",
        "Work-sharing directives:\n",
        "\n",
        "- `#pragma omp for` : distribute loop iterations among threads in a parallel region (\\*). <!--the `omp loop` pragma is its modern, more flexible, counterpart-->\n",
        "- `#pragma omp sections` / `#pragma omp section` : divide work into distinct code blocks, each executed by one thread in a parallel region.\n",
        "\n",
        "These can be combined with `omp parallel` to write just one pragma instead of two to do the same thing, like `omp parallel for` to simultaneously open a parallel region and parallelize a loop.\n",
        "\n",
        "These pragmas terminate in an **implicit barrier** that waits for all threads to complete the work they were assigned as part of the pragma.\n",
        "\n",
        "The parallel directive creates **one** team of threads.\n",
        "Remember, **you can't \"divide twice\" within one team**, therefore you can't nest work-sharing directives inside the same parallel region, you can only issue them sequentially unless you create a nested parallel region that spawns additional teams.\n",
        "<br>\n",
        "In other words, a team can only deal with one work-sharing directive at once.\n",
        "\n",
        "**Nesting parallel regions** provides an immediate way to allow more threads to participate in the computation.<br>\n",
        "Nested parallel region behavior:\n",
        "- if nested parallelism is enabled (see later), each nested region will spawn its defined number of threads each time it is encountered by any thread. This can help programs with limited scalability, but also quickly blow up the number of threads past physical cores and bloat the system (oversubscription) if abused. Enabling dynamic adjustment of the number of threads (see later) can mitigate this.\n",
        "- if nested parallelism is disabled, the nested parallel region executes as if it were a serial block.\n",
        "\n",
        "Synchronization and exclusion:\n",
        "\n",
        "- `#pragma omp single` : only one thread executes the block; other threads wait at an implicit barrier.\n",
        "- `#pragma omp master` : executed only by the master thread (thread 0), with no barrier implied.\n",
        "- `#pragma omp barrier` : explicit synchronization point; all threads wait here.\n",
        "- `#pragma omp critical [(name)]` : only one thread executes the block at a time; multiple critical blocks that have the same name are seen as the same block and use the same lock underneath.\n",
        "- `#pragma omp atomic` : perform a single atomic update on a shared variable.\n",
        "- `#pragma omp flush` : enforce memory consistency (ensures all threads see updated values).\n",
        "<!--- `#pragma omp ordered` : enforce ordered execution of certain loop parts marked as `ordered`.-->\n",
        "\n",
        "(\\*) Using `omp for` requires a canonical OpenMP loop, meaning that:\n",
        "- it is strictly a `for` loop;\n",
        "- it has a single integer loop induction variable;\n",
        "- the loop is countable (finite), with a linear increment or decrement (e.g. `i += 2` is ok, but not `i *= 2`);\n",
        "- the loop variable, bounds, and increment are iteration invariants;\n",
        "\n",
        "Following from the above, the number of iterations can be determined before the loop executes, even if it's not known until run time.\n",
        "In particular, note that the number of iterations doesn't need to be known statially (at compile time). It can be computed at runtime so long as it is fixed by the time the loop is reached and needs to be divided among threads.\n",
        "\n",
        "---\n",
        "\n",
        "### Cancellation\n",
        "\n",
        "OpenMP allows a thread to request that a parallel region or task region stop early, meaning the other threads should stop doing further work in that region as soon as possible.\n",
        "\n",
        "That is `#pragma omp cancel <construct> [if-clause]`, where `<construct>` is any of: `parallel`, `sections`, `for`, `taskgroup`.\n",
        "\n",
        "For this to work, cancellation points need to be present inside the construct, that upon reached make a thread check whether its current construct's cancellationflag was set: `#pragma omp cancellation point <construct>`.\n",
        "<br>\n",
        "Additionally, `barrier` and implicit barriers also act as cancellation points.\n",
        "\n",
        "Execution reuses from immediately after the cancelled construct, either serially or by the same team depending on which was the cancelled construct.\n",
        "\n",
        "*Note: for tasks, cancellation only acts on those grouped by the affected taskgroup construct. Tasks cancellation also occurs if the parallel region is canceled.*\n",
        "\n",
        "---\n",
        "\n",
        "### Scheduling\n",
        "\n",
        "When parallelizing unbalanced loops, where some iterations may take more time than others, we can balance the load between threads with a schedule:\n",
        "\n",
        "- `schedule(kind[, chunk_size])` : control iteration scheduling, `kind` can be:\n",
        "  - `static` : iterations are split into chunks of equal size, each thread is assigned its almost evenly chunks before execution begins and those never change afterwards.\n",
        "  - `dynamic` : iterations are split into chunks of equal size and placed in a queue, threads are assigned one chunk at a time from the queue. Adds a slight scheduling overhead, use only if the workload is truly unbalanced.\n",
        "  - `guided` : chunks of iterations are assigned to threads as the loop runs, but the chunk size decreases as the loop progresses. The given `chunk_size` functions as the minimum chunk size reached. Also adds a slight overhead, but less than `dynamic` due to fewer scheduling decisions as per the larger initial chunks.\n",
        "  - `runtime` : delegates the choice of schedule to the environment variable `OMP_SCHEDULE`.\n",
        "  - `auto` : the choice of schedule is delegated to either the compiler or the runtime environment.\n",
        "  - if not specified, the default schedule is implementation-dependent.\n",
        "- `collapse(n)` : collapse (flatten) `n` nested loops into a single loop (and thus single ietration space) for scheduling.\n",
        "\n",
        "Keep this in mind when parallelizing nested loops:<br>\n",
        "If execution of any associated loop changes any of the values used to compute any of the **iteration counts** (loop bounds), then the behavior is unspecified.\n",
        "\n",
        "---\n",
        "\n",
        "### Tasks and Related Pragmas\n",
        "\n",
        "Defines asynchronous units of work for fine-grained parallelism:\n",
        "\n",
        "- `#pragma omp task` : define a task for deferred execution.\n",
        "- `#pragma omp taskwait` : wait until all child tasks of the current task complete.\n",
        "- `#pragma omp taskgroup` : group tasks for collective synchronization.\n",
        "- `#pragma omp taskyield` : allow a thread to yield execution to other tasks.\n",
        "\n",
        "Common clauses for tasks:\n",
        "\n",
        "- `if(cond)` : create the task only if the condition is true; otherwise, execute it immediately.\n",
        "- `final(cond)` : mark task as “final,” disallowing creation of child tasks inside it.\n",
        "- `mergeable` : allow the task to be merged with its parent task for optimization.\n",
        "- `depend(in|out|inout : varlist)` : declare task dependencies to control execution order.\n",
        "- `untied` : allow the task to resume on a different thread than the one that started it.\n",
        "\n",
        "Whereas sections define static tasks, statically defined at compile time and whose number cannot change, that are queued up and handled in arbitrary order by threads, these true **tasks** form a graph of execution with dependencies, any task spawning more tasks, and synchronization, more like threads would in a barebones fork-join model, but with cleaner code and higher level abstractions.\n",
        "<br>\n",
        "In brief: tasks can be spawned from any point and thread inside the parallel region!\n",
        "\n",
        "A huge warning: tasks are tied to their thread by default, meaning that if they are suspended, they must resume in the same thread until they finish. This is crucial because private variables (e.g. those specified on the `parallel private(...)` that spawns the team of threads running the tasks) are **per-thread, not per-task**, so if a task relies on the private variables of it thread, and is untied, it may see those randomly changing if it ever gets rescheduled. And the same applies to other per-thread things, like IDs.\n",
        "<br>\n",
        "If you want to untie a task, make sure it only works on shared variables or its own local variables.\n",
        "\n",
        "---\n",
        "\n",
        "### Teams and Offloading (extra)\n",
        "\n",
        "The `pragma opm teams` directive was introduced mainly to support heterogeneous (accelerator/GPU) programming.\n",
        "\n",
        "It takes the place of `parallel`, but unlike it, `teams` creates multiple teams of threads, each with its own master and workers.\n",
        "Each team executes the same code region independently.\n",
        "Within each team, you can then launch further nested parallelism (using `omp parallel`) to create hierarchical parallelism.\n",
        "\n",
        "On a CPU, this may just create a single team (depending on implementation), but on GPUs, **it naturally maps to CUDA thread and blocks** seen as multiple independent teams, each with their own threads.\n",
        "\n",
        "Offloading may look like this:\n",
        "```\n",
        "#pragma omp target teams distribute parallel for device(device_num)\n",
        "for (int i = 0; i < N; ++i) {\n",
        "    A[i] = B[i] + C[i];\n",
        "}\n",
        "```\n",
        "\n",
        "Where the pragma reads as:\n",
        "- target : offload to a set device (e.g. GPU).\n",
        "- teams : create multiple teams (like CUDA thread blocks).\n",
        "- parallel for : within each team, create multiple threads to execute parts of the loop in parallel.\n",
        "\n",
        "---\n",
        "\n",
        "### Routines\n",
        "\n",
        "Functions exposed by the OpenMP API:\n",
        "\n",
        "- `int omp_get_thread_num()` : use inside a parallel region to get the unique identifier of your thread; it will be a number in [0, num_threads).\n",
        "- `int omp_get_num_threads()` : returns the number of threads created in the current parallel region.\n",
        "- `void omp_set_num_threads(int num_threads)` : sets the default number of threads to use in parallel regions.\n",
        "- `int omp_get_max_threads()` : returns the maximum number of threads to use in parallel regions.\n",
        "- `void omp_set_max_threads(int num_threads)` : sets the maximum number of threads to use in parallel regions.\n",
        "- `void omp_set_nested(int nested)` : enables or disables nested parallelism.\n",
        "- `void omp_set_dynamic(int dynamic_threads)` : enables or disables dynamic adjustment of the number of threads available for the execution of subsequent parallel regions.\n",
        "- `double omp_get_wtime()` : returns an absolute time reference, useful to time code execution.\n",
        "\n",
        "Additional routines are available inside the `teams` pragma:\n",
        "- `int omp_get_team_num()` : returns the number of created teams.\n",
        "- `int omp_get_team_num()` : returns the unique identifier of the caller thread's team; it will be a number in [0, num_teams).\n",
        "\n",
        "To access those you need to include the `omp.h` header.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "Control OpenMP runtime behavior without recompiling.\n",
        "\n",
        "- `OMP_NUM_THREADS` : default number of threads to use in parallel regions, if not specified defaults to the system's core count.\n",
        "- `OMP_SCHEDULE` : default loop scheduling policy (e.g. `\"dynamic,4\"`).\n",
        "- `OMP_DYNAMIC` : allow runtime adjustment of threads (`TRUE` or `FALSE`).\n",
        "- `OMP_PROC_BIND` : control thread-core binding (`master|close|spread`).\n",
        "- `OMP_PLACES` : specify hardware places (`threads|cores|sockets` or custom lists).\n",
        "- `OMP_MAX_ACTIVE_LEVELS` : maximum depth of nested parallel regions.\n",
        "- `OMP_WAIT_POLICY` : set thread waiting behavior (`active` or `passive`).\n",
        "- `OMP_DISPLAY_ENV` : print the current OpenMP environment at startup.\n",
        "- `OMP_STACKSIZE` : set the thread stack size.\n",
        "- `OMP_CANCELLATION` : enable or disable cancellation features in tasks or loops.\n",
        "- `OMP_NESTED` : enables nested parallel regions, usually disabled by default.\n",
        "- `OMP_DYNAMIC` : enables or disables dynamic adjustment of the number of threads available for the execution of subsequent parallel regions, usually disabled by default.\n",
        "\n",
        "---\n",
        "\n",
        "### Controlling Affinity and Thread Assignment\n",
        "\n",
        "Mechanisms to control how threads are bound to CPU cores and how their placement affects performance:\n",
        "\n",
        "- `proc_bind(master|close|spread)` : directive clause controlling how threads are distributed within the available places (as defined by `OMP_PLACES`):\n",
        "  - `master` : all threads are placed close to the master thread (usually on the same core group, e.g. socket or NUMA node).\n",
        "  - `close` : threads are packed as near as possible to each other, filling one place before moving to the next (minimize distance, maximize data locality).\n",
        "  - `spread` : threads are distributed as widely as possible across places (maximize distance, maximize resource usage).\n",
        "\n",
        "At the environment level:\n",
        "\n",
        "- `OMP_PROC_BIND` : controls whether and how threads are bound (`TRUE|FALSE|master|close|spread`).\n",
        "- `OMP_PLACES` : specifies the hardware resources threads may be placed on (e.g. `threads`, `cores`, `sockets`, or custom lists like `\"{0,1},{2,3}\"`).\n",
        "\n",
        "Typical uses:\n",
        "- use `close` for threads that frequently share many accesses the same data or work on contiguous chunks of a shared array, thus improving cache locality.\n",
        "- use `spread` for threads that are largely independent or memory-bound, hence prefer having a lot of hardware resources and may as well fill a cache line by themselves with little data accesses in common with each other.\n",
        "- with nested parallel regions, it's usual to have first a `spread` (to use different sockets or cores) and then a `close` binding (to exploit data reuse within a place), especially when inner threads see more data reuse opportunities than outer ones.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "#pragma omp parallel num_threads(4) proc_bind(spread)\n",
        "{\n",
        "  #pragma omp parallel num_threads(4) proc_bind(close)\n",
        "  {\n",
        "    // Work here\n",
        "  }\n",
        "}\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "### Settings Precedence\n",
        "\n",
        "When setting the same parameter through different means, they override each other in this order from most to least authoritative:\n",
        "\n",
        "1. Explicit clauses in pragmas (`proc_bind`, `num_threads`, ...)\n",
        "2. Explicit routine calls (`omp_set_num_threads`, ...)\n",
        "3. Environment variables (`OMP_PROC_BIND`, `OMP_PLACES`, `OMP_NUM_THREADS`, ...)\n",
        "4. Implementation defaults (compiler/runtime)\n",
        "\n",
        "---\n",
        "\n",
        "### Compiler Commands\n",
        "\n",
        "Compilers (GCC, Clang) require a flag to enable OpenMP support:\n",
        "\n",
        "```bash\n",
        "gcc/clang -fopenmp program.c -o program\n",
        "```\n",
        "\n",
        "To disable OpenMP (e.g. for debugging), you can just omit `-fopenmp`.\n",
        "<br>\n",
        "When disabled, OpenMP pragmas are ignored, and the code runs in serial mode.\n",
        "This is useful for checking correctness and debugging race conditions.\n",
        "\n",
        "---\n",
        "\n",
        "### Notes\n",
        "\n",
        "* OpenMP pragmas are hints to the compiler: if support is disabled, they are ignored and the program remains valid C/C++.\n",
        "* For portable and deterministic parallel programs, always explicitly specify data-sharing attributes and scheduling."
      ],
      "metadata": {
        "id": "BzzAQA1z5R4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "km1W492NojXB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvZp4-7XMgTp"
      },
      "source": [
        "## **Simple Examples**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1**"
      ],
      "metadata": {
        "id": "44Sjhyc_OfLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple concurrent threads printing \"Hello, World!\" (plus meaningless computation to make it run longer):"
      ],
      "metadata": {
        "id": "D9f5lhn5OeLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/hello_world_0.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main ()\n",
        "{\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    int id = omp_get_thread_num();\n",
        "    printf(\"Hello World from thread = %d\\n\", id);\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "reTXIF6UPUwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ad8abd-9c5a-4f55-c50a-1c041d7df11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/hello_world_0.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how this is different from the Pthreads implementation. Questions:\n",
        "- How many threads will be created?\n",
        "<!--implementation dependent -> usually as many as the available CPU cores-->\n",
        "- What happens if you ask for a number of threads greater than the number of cores?\n",
        "<!--oversubscription -> lower performance, scheduling overhead-->"
      ],
      "metadata": {
        "id": "LDOVUTKmPbA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile:"
      ],
      "metadata": {
        "id": "Rb_ezn2NQsyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ hello_world_0.cpp -fopenmp -o hello_world_0"
      ],
      "metadata": {
        "id": "_VBfqOmJQu6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute:"
      ],
      "metadata": {
        "id": "xPG0TV29Qt1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello_world_0"
      ],
      "metadata": {
        "id": "Jonu0ntBQ49m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07571099-a8d2-46ad-a6df-6379860857c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from thread = 1\n",
            "Hello World from thread = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2**"
      ],
      "metadata": {
        "id": "gJzYN_EF4pVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing parallelism in OpenMP can be as easy as adding pragmas, with no further modifications on the code. For example:"
      ],
      "metadata": {
        "id": "JJlO-P8vN2gO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2DBligfSAfM"
      },
      "source": [
        "%%writefile /home/OpenMP/hello_world.cpp\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void print_message(int threadIndex) {\n",
        "  printf(\"Thread number %d\\n\", threadIndex);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  #pragma omp parallel num_threads(4)\n",
        "  {\n",
        "    #pragma omp for schedule(static, 4)\n",
        "    for (int ii = 0; ii < 10; ii++) {\n",
        "      print_message(ii);\n",
        "    }\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile without the OpenMP flag:"
      ],
      "metadata": {
        "id": "wWAF2q08NVi6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vd5hp0-TiH3"
      },
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -o hello_world"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the generated LLVM IR:"
      ],
      "metadata": {
        "id": "DZp-89r8MLWQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGtfkM9_WhHP"
      },
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -S -emit-llvm\n",
        "!cat hello_world.ll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile with the OpenMP flag:"
      ],
      "metadata": {
        "id": "yq4X5DBQNL8p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP2V0HdzXKIm"
      },
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -fopenmp -o hello_world"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the generated LLVM IR:"
      ],
      "metadata": {
        "id": "84yjgm_cL8Mh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq9Os-OAX9Oy"
      },
      "source": [
        "%cd /home/OpenMP\n",
        "!clang hello_world.cpp -S -emit-llvm -fopenmp\n",
        "!cat hello_world.ll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute:"
      ],
      "metadata": {
        "id": "ewd_80SiNc2C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IM8FFhaTtdC"
      },
      "source": [
        "%cd /home/OpenMP\n",
        "!./hello_world"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculation of pi**"
      ],
      "metadata": {
        "id": "4QiOjWEyRafk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 3**"
      ],
      "metadata": {
        "id": "_nGEtHLqRlZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integral-based method to calculate pi: each thread calculates the heigth of a set of rectangles (map/SIMD pattern), the sum of all heigths is multiplied by the step size to get the area.\n",
        "<img align=\"middle\" src=\"https://drive.google.com/uc?id=17dBhvYY9F5Bl2re_pnmRWiZ717jolCPg\">\n",
        "\n",
        "Why does this work?\n",
        "<br>\n",
        "Recall that: $\\frac{d}{dx} arctan(x) = \\frac{1}{1+x^2}$ and $arctan(0) = 0°$ while $arctan(1) = 45° = \\frac{180°}{4}$, so...\n",
        "\n",
        "*More info here: https://math.stackexchange.com/questions/1085653/geometrical-interpretation-of-pi-int-01-frac41x2dx*"
      ],
      "metadata": {
        "id": "FtcFuEitRnQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic implementation with manual work-sharing using thread IDs:"
      ],
      "metadata": {
        "id": "yhY5Jlz8HsNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/integralpi.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define MAX_THREADS 4\n",
        "\n",
        "static long num_steps = 100000000;\n",
        "double step;\n",
        "\n",
        "int main() {\n",
        "\tint i, j;\n",
        "\tdouble pi, full_sum = 0.0;\n",
        "\tdouble start_time, run_time;\n",
        "\tdouble sum[MAX_THREADS];\n",
        "\n",
        "\tstep = 1.0/(double) num_steps;\n",
        "\n",
        "\t// measure scalability from 1 to MAX_THREADS threads\n",
        "\tfor (j = 1; j <= MAX_THREADS; j++) {\n",
        "\t\tomp_set_num_threads(j);\n",
        "\t\tfull_sum = 0.0;\n",
        "\t\tstart_time = omp_get_wtime();\n",
        "\n",
        "\t\t#pragma omp parallel\n",
        "\t\t{\n",
        "\t\t\tint i;\n",
        "\t\t\tint id = omp_get_thread_num();\n",
        "\t\t\tint numthreads = omp_get_num_threads();\n",
        "\t\t\tdouble x;\n",
        "\t\t\tsum[id] = 0.0;\n",
        "\t\t\tif (id == 0)\n",
        "\t\t\t\tprintf(\" num_threads = %d\", numthreads);\n",
        "\n",
        "\t\t\t// manual work allocation\n",
        "\t\t\tfor (i = id; i < num_steps; i += numthreads) {\n",
        "\t\t\t\tx = (i + 0.5)*step;\n",
        "\t\t\t\tsum[id] = sum[id] + 4.0/(1.0 + x*x);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t\tfor(full_sum = 0.0, i = 0; i < j; i++)\n",
        "\t\t\tfull_sum += sum[i];\n",
        "\n",
        "\t\tpi = step * full_sum;\n",
        "\t\trun_time = omp_get_wtime() - start_time;\n",
        "\t\tprintf(\"\\n pi is %f in %f seconds %d threads \\n\", pi, run_time, j);\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "id": "_IMvx6wvTzh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- How is work distributed among threads?\n",
        "<!--using thread IDs, each step every \"numthreads\" is assigned to a different thread-->\n",
        "- Is the result deterministic?\n",
        "<!--yes-->\n",
        "- How do you expect performance to scale with the number of threads?\n",
        "<!--ideally, linearly, as we will almost always have far more iterations to distribute than threads and there is little overhead for the creation of additional threads, aside from their creation itself; on Colab tho, anything could happen-->"
      ],
      "metadata": {
        "id": "h2ebqOckUGop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation with the `parallel for` work-sharing construct:"
      ],
      "metadata": {
        "id": "kOLx6_aAVHyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/integralpi2.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "static long num_steps = 100000000;\n",
        "double step;\n",
        "\n",
        "int main() {\n",
        "\tint i;\n",
        "\tdouble x, pi, sum = 0.0;\n",
        "\tdouble start_time, run_time;\n",
        "\n",
        "\tstep = 1.0/(double) num_steps;\n",
        "\n",
        "\tfor (i = 1; i <= 4; i++) {\n",
        "\t\tsum = 0.0;\n",
        "\t\tomp_set_num_threads(i);\n",
        "\t\tstart_time = omp_get_wtime();\n",
        "\n",
        "\t\t#pragma omp parallel for private(x) reduction(+:sum)\n",
        "\t\tfor (i = 1; i <= num_steps; i++) {\n",
        "\t\t\tx = (i-0.5)*step;\n",
        "\t\t\tsum = sum + 4.0/(1.0+x*x);\n",
        "\t\t}\n",
        "\n",
        "\t\tpi = step * sum;\n",
        "\t\trun_time = omp_get_wtime() - start_time;\n",
        "\t\tprintf(\"\\n pi is %f in %f seconds and %d threads\\n\", pi, run_time, i);\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "id": "Xd1Z5QUUVPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- How is work distributed among threads?\n",
        "<!--each thread is statically assigned some of the loop's iterations by the \"for\" pragma-->\n",
        "- Are there other ways of resolving the access to the shared variable?\n",
        "<!--yes, atomically incrementing \"sum\", even tho it's a poor idea performance-wise-->"
      ],
      "metadata": {
        "id": "Crk4g21GVUe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Variables Initialization**"
      ],
      "metadata": {
        "id": "E9y1aoBb7jWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **FirstPrivate**"
      ],
      "metadata": {
        "id": "X5rtihG370_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we need to quickly give a copy of a value to each thread, we use `firstprivate`, this saves us the time needed for each thread to go and fetch a copy of an otherwise shared variable.\n",
        "\n",
        "Say that we have an array of elements and an initial value.\n",
        "We need to find subsequences of 3 contigous values in the array that such that, if added to the initial one, overflow.\n",
        "We can dispatch the initial value to threads as a firstprivate variable."
      ],
      "metadata": {
        "id": "4K81ROlH7916"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/fistprivate.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#include <limits.h>\n",
        "\n",
        "int main(void) {\n",
        "  const int N = 12;\n",
        "  unsigned int data[N] = {10, 20, 30, 250, 5, 10, 100, 200, 50, 90, 200, 40};\n",
        "  unsigned int init_value = 1<<30;\n",
        "\n",
        "  #pragma omp parallel for firstprivate(init_value)\n",
        "  for (int i = 0; i < N - 2; i++) {\n",
        "    unsigned int a = data[i];\n",
        "    unsigned int b = data[i + 1];\n",
        "    unsigned int c = data[i + 2];\n",
        "\n",
        "    unsigned int sum = init_value;\n",
        "    int overflow = 0;\n",
        "\n",
        "    if (sum > UINT_MAX - a) overflow = 1;\n",
        "    else sum += a;\n",
        "    if (!overflow && sum > UINT_MAX - b) overflow = 1;\n",
        "    else sum += b;\n",
        "    if (!overflow && sum > UINT_MAX - c) overflow = 1;\n",
        "    else sum += c;\n",
        "\n",
        "    if (overflow)\n",
        "      printf(\"Thread %d found overflow at subsequence [%d,%d,%d]\\n\", omp_get_thread_num(), i, i+1, i+2);\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "TTZeBe0u7-FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- could we do this without `firstprivate`?\n",
        "<!--obviously yes, we could just have each thread copy the content of the then-shared \"init_value\" into its own local variable-->\n",
        "- what is the advantage of using `firstprivate`?\n",
        "<!--firstprivate guarantees that threads will not alter the global instance of the variable. It is mainly a semantical tool to ensure that when comparing the code between a parallel and serial execution the existence of threads doesn't unpredictably change the content of the thus-private variable. Ultimately, firstprivate clearly explicitate how the program intends the variable to be (safely) operated upon by threads: each thread receives its own copy of this initial value.-->"
      ],
      "metadata": {
        "id": "YNuktT9DM4pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LastPrivate**"
      ],
      "metadata": {
        "id": "6zxfZG6L7mfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With sections (or iterations of a for loop) and `lastprivate`, we can ensure that a variable is updated by the last section (or iterations) in serial program order, not whichever finishes last in real time.\n",
        "\n",
        "Arguably, lastprivate is very rarely useful..."
      ],
      "metadata": {
        "id": "5Zqy7uZG7q1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/lastprivate.cpp\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  int x = 0;\n",
        "\n",
        "  #pragma omp parallel sections lastprivate(x)\n",
        "  {\n",
        "    #pragma omp section\n",
        "    { x = 1; printf(\"Section 1: x=%d\\n\", x); }\n",
        "\n",
        "    #pragma omp section\n",
        "    { x = 2; printf(\"Section 2: x=%d\\n\", x); }\n",
        "\n",
        "    #pragma omp section\n",
        "    { x = 3; printf(\"Section 3: x=%d\\n\", x); }\n",
        "  }\n",
        "\n",
        "  printf(\"After sections, x=%d (from the *last* section)\\n\", x);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "z8S2VxRE7qOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"linked-list\"></a>\n",
        "## **Linked List Traversal**"
      ],
      "metadata": {
        "id": "Bzk_KelOV0r8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise 4**"
      ],
      "metadata": {
        "id": "QPkcvbyk_oad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallelizing the traversal of a linked list with OpenMP can be highly inefficient.\n",
        "<img align=\"middle\" src=\"https://drive.google.com/uc?id=1BrtuiwIzR2Y-xGPUt3lIX88zEfVF7e_L\">\n",
        "The *task* construct provides a better way to dynamically create concurrent work units."
      ],
      "metadata": {
        "id": "olVCRiCt_oaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/linkedlist.cpp\n",
        "#include <omp.h>\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "struct node {\n",
        "  int data;\n",
        "  int fibdata;\n",
        "  struct node* next;\n",
        "};\n",
        "\n",
        "struct node* init_list(struct node* p);\n",
        "void processwork(struct node* p);\n",
        "int fib(int n);\n",
        "\n",
        "int fib(int n) {\n",
        "  int x, y;\n",
        "  if (n < 2) {\n",
        "    return (n);\n",
        "  } else {\n",
        "    x = fib(n - 1);\n",
        "    y = fib(n - 2);\n",
        "    return (x + y);\n",
        "  }\n",
        "}\n",
        "\n",
        "void processwork(struct node* p) {\n",
        "  int n, temp;\n",
        "  n = p->data;\n",
        "  temp = fib(n);\n",
        "  p->fibdata = temp;\n",
        "}\n",
        "\n",
        "struct node* init_list(struct node* p) {\n",
        "  int i;\n",
        "  struct node* head = NULL;\n",
        "  struct node* temp = NULL;\n",
        "\n",
        "  head = malloc(sizeof(struct node));\n",
        "  p = head;\n",
        "  p->data = 38;\n",
        "  p->fibdata = 0;\n",
        "  for (i = 0; i < 5; i++) {\n",
        "    temp  = malloc(sizeof(struct node));\n",
        "    p->next = temp;\n",
        "    p = temp;\n",
        "    p->data = 38 + i + 1;\n",
        "    p->fibdata = i + 1;\n",
        "  }\n",
        "  p->next = NULL;\n",
        "  return head;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  double start, end;\n",
        "  struct node *p=NULL;\n",
        "  struct node *temp=NULL;\n",
        "  struct node *head=NULL;\n",
        "\n",
        "  printf(\"Process linked list\\n\");\n",
        "  printf(\"  Each linked list node will be processed by function 'processwork()'\\n\");\n",
        "  printf(\"  Each node will compute a subsequent Fibonacci number starting from the 38th\\n\");\n",
        "\n",
        "  p = init_list(p);\n",
        "  head = p;\n",
        "\n",
        "  start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel num_threads(4)\n",
        "  {\n",
        "    #pragma omp master\n",
        "    printf(\"Threads: %d\\n\", omp_get_num_threads());\n",
        "    #pragma omp single\n",
        "    {\n",
        "      printf(\"I am thread %d and I am creating tasks\\n\", omp_get_thread_num());\n",
        "      p = head;\n",
        "      while (p) {\n",
        "        #pragma omp task firstprivate(p) // each task gets is own copy of the pointer to the current list node\n",
        "        {\n",
        "          processwork(p);\n",
        "          printf(\"I am thread %d\\n\", omp_get_thread_num());\n",
        "        }\n",
        "        p = p->next;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  end = omp_get_wtime();\n",
        "  p = head;\n",
        "  while (p != NULL) {\n",
        "    printf(\"%d : %d\\n\",p->data, p->fibdata);\n",
        "    temp = p->next;\n",
        "    free (p);\n",
        "    p = temp;\n",
        "  }\n",
        "  free (p);\n",
        "\n",
        "  printf(\"Compute Time: %f seconds\\n\", end - start);\n",
        "}"
      ],
      "metadata": {
        "id": "LMp2ochj_oag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- How many threads create tasks?\n",
        "<!--one, it's inside the \"single\" pragma-->\n",
        "- How are tasks distributed among threads?\n",
        "<!--workpile-style, each thread fetches a new task every time it finishes the current one-->"
      ],
      "metadata": {
        "id": "L3DtkUky_oai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task Graphs**"
      ],
      "metadata": {
        "id": "UMCfutQOMK33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall:\n",
        "- $work$ : total amount of work\n",
        "- $span$ : work on the critical path\n",
        "- $parallelism = work / span$\n",
        "\n",
        "Room for optimization:\n",
        "- reducing the critical path\n",
        "- reducing overhead for anything that is not on the critical path\n",
        "\n",
        "Representation:\n",
        "- nodes: tasks with a certain amount of work to do\n",
        "- directed edges: dependencies between tasks (inbound arrows: what the current task needs to wait for)"
      ],
      "metadata": {
        "id": "D_q0ejhNQkNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For completeness, let's also recap a bit of terminology:\n",
        "- when a task creates another task, the creating task becomes the *parent task* of the new task. The new task then is called a *child task* of its parent task. - the term *sibling tasks* refers to all tasks that have the same parent.\n",
        "- a *descendant task* is a task in the ancestor chain of a parent, so either a child task or a task created by a descendant task (e.g., a child task of a child task).\n",
        "- if a new task is put into the task pool, it is said to be *deferred* while if it is executed straight away, it is *undeferred*.\n",
        "- a task is described as *completed* when it has been scheduled for execution and that execution has finished."
      ],
      "metadata": {
        "id": "X299cr7iiQPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 5**"
      ],
      "metadata": {
        "id": "bsA7Q777MRxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=17PNFB2oQAFEHfvQPflSieUmjQpmLGAmn\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "- Calculate work and parallelism. <!--span=100+250+300, work=860, parallelism=1.32-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are active during the execution of Task 5? <!--1 or 2, depending on the state of T4-->\n",
        "- Is there a better parallel implementation (considering both performance and resource usage)? I.e. do we really need to exploit all parallelism? <!--since W(T4) > W(T2) + W(T3), there is no need to run T2 and T3 in parallel (with the overhead of constructing one more thread), we could just run the sequentially T2 -> T3 and we would still be bond by the amount of work done by T4-->"
      ],
      "metadata": {
        "id": "lCVVBl9QMXv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 6**"
      ],
      "metadata": {
        "id": "_hsINTvraOMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=15gkHj2zWAZQnuLhmPMWWSjWZqOfkj3MB\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "- Calculate work and span. <!--span=670, work=1045, parallelism=1.56-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- Is this implementation faster than a sequential one? <!--yes, we have parallelism > 1 after all-->\n"
      ],
      "metadata": {
        "id": "SWEG4xwoaRuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7**"
      ],
      "metadata": {
        "id": "2jWBaggVd7z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1PXlO9Eaxu1lI27k2hYIaWD6y6tLZqDi9\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=100, W(T2)=100, W(T3)=75, W(T4)=50, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "- Calculate work and span. <!--span=400, work=800, parallelism=2-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are needed to achieve the maximum theoretical parallelism? <!--\n",
        "2 threads!\n",
        "For example, thread 1 handles T1, thread 2 handles T2; then thread 1 does T3, T4, and T5 while thread 2 handle T8, then thread 1 handles T6 and thread 2 finishes T7.\n",
        "Note: you can always get away with \"ceil(parallelism)\" threads so long as you assume that a thread can \"yield\" a task, go do some other work, and resume it later. In this case this was not needed since luckly W(T3)+W(T4)+W(T5) = W(T8).\n",
        "-->"
      ],
      "metadata": {
        "id": "g8MU2k-Vd7E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 8**"
      ],
      "metadata": {
        "id": "48oUgM9OorTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=14BBD6_ctJ-IUH1ubnaF4pMh99LNjX818\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=50, W(T2)=50, W(T3)=200, W(T4)=75, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "- Which dashed arrow prevents from implementing such a task structure with OpenMP? <!--none, there are still no cycles, though the dependency T6->T8 is redundant with T6->T7->T8-->\n",
        "- Remove the dashed arrows, calculate work and span. <!--span=600, work=850, parallelism=1.42-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph."
      ],
      "metadata": {
        "id": "2R96cl-eo8Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 9**"
      ],
      "metadata": {
        "id": "wNlob9bqqybW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following task graph:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img align=\"center\" src=\"https://drive.google.com/uc?id=1flOJlQU5-jo4kLeJmBBuou7HqYYYvQZI\">\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Assume W(T1)=50, W(T2)=50, W(T3)=50, W(T4)=75, W(T5)=75, W(T6)=100, W(T7)=100, W(T8)=200.\n",
        "\n",
        "<!--note the redundant dependency T3->T7-->\n",
        "- Calculate work and span. <!--span=500, work=700, parallelism=1.4-->\n",
        "- Write an OpenMP implementation reflecting the structure of the task graph.\n",
        "- How many threads are needed to run the program? <!--ceil(1.4)=2, while T3 and T7 run by themselves in the above path, T4 can be handled by the other thread-->\n",
        "- How many threads could be active during the execution of Task 3? How many during Task 5? <!--during T3, at most 2 (another on T4), during T5, at most 3 (another on T6 and another on T4)-->"
      ],
      "metadata": {
        "id": "HGOjC5igqxZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deadlocks 1o1**"
      ],
      "metadata": {
        "id": "tRiaOvXFMAD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very simple example of how NOT to use barriers.\n",
        "<br>\n",
        "Also not how each thread can acquire its own ID. This is written in C++ just to spice things up."
      ],
      "metadata": {
        "id": "8863nHR6MgZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/deadlock.cpp\n",
        "#include <iostream>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  #pragma omp parallel default(none) shared(std::cout) num_threads(4)\n",
        "  {\n",
        "    const int thread_num = omp_get_thread_num();\n",
        "\n",
        "    if(thread_num == 0) {\n",
        "      std::cout << \"I'm thread 0 and I caused a deadlock!\" << std::endl;\n",
        "    } else {\n",
        "      #pragma omp barrier\n",
        "    }\n",
        "\n",
        "    #pragma omp critical\n",
        "    std::cout << \"I'm thread \" << thread_num << std::endl;\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSCyMObEMGld",
        "outputId": "10904e8e-0775-41cb-b0f0-abdb7837f218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/deadlock.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run with a timeout (since we know it will deadlock):"
      ],
      "metadata": {
        "id": "1Hih3-MuMYpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ deadlock.cpp -fopenmp -o deadlock\n",
        "!timeout 4s ./deadlock && echo \"Program finished normally.\" || echo \"Program was killed by timeout.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zoPuo_NMYSZ",
        "outputId": "33002452-dd78-4fc5-8b8c-79b1ef5ce21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm thread 0 and I caused a deadlock!\n",
            "I'm thread 0\n",
            "Program was killed by timeout.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compile often helps you, however. Something this horrific will not even compile:"
      ],
      "metadata": {
        "id": "gT3_gN50NYvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/just_for_highlight.cpp\n",
        "#pragma omp single\n",
        "{\n",
        "  std::cout << \"I've caused a deadlock!\\n\";\n",
        "  #pragma omp barrier // <-- the compiler errors out on this\n",
        "}\n"
      ],
      "metadata": {
        "id": "KvyqbpGwNdFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loop Schedules**"
      ],
      "metadata": {
        "id": "7bO_AOMOXxPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the effect of different loops schedules on an unbalanced loop nest:"
      ],
      "metadata": {
        "id": "UhUmX8gFX4Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/schedules.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 5000 // outer loop iterations\n",
        "#define M 5000 // inner loop iterations\n",
        "\n",
        "#define CHUNK 1000 // chunk size\n",
        "#define THREADS 4  // number of threads\n",
        "\n",
        "int main(void) {\n",
        "  omp_set_nested(true); // enable nested parallelism\n",
        "  omp_set_num_threads(THREADS);\n",
        "\n",
        "  double start, end;\n",
        "  double total = 0.0;\n",
        "\n",
        "  printf(\"OpenMP Nested Loop Parallelization Comparison\\n\");\n",
        "  printf(\"N = %d, M = %d\\nCHUNK = %d, THREADS = %d \\n\\n\", N, M, CHUNK, THREADS);\n",
        "\n",
        "  // nested work-sharing with static schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(static)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    double local_sum = 0.0;\n",
        "    // Note: we CANNOT put another work-sharing construct here like:\n",
        "    // #pragma omp for schedule(static)\n",
        "    // OpenMP forbids two work-sharing constructs back-to-back without a barrier or parallel pragma in-between!\n",
        "    // The intended way to achieve the same result is the 'collapse' clause, see version 4.\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      // fake unbalance: the amount of work depends on i\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        local_sum += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "    #pragma omp atomic\n",
        "    total += local_sum;\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 1 (static, nested for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // nested work-sharing with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(dynamic)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    double local_sum = 0.0;\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        local_sum += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "    #pragma omp atomic\n",
        "    total += local_sum;\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 2 (dynamic, nested for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // nested parallelization with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for schedule(static) reduction(+:total)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    // Note: here we can do this, becase we create a nested parallel region\n",
        "    #pragma omp parallel for schedule(static) reduction(+:total)\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 3 (dynamic, nested parallel for): %f seconds\\n\", end - start);\n",
        "\n",
        "  // collapse clause, single parallel for with dynamic schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for collapse(2) schedule(dynamic, CHUNK)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 4 (dynamic(CHUNK), collapse(2)): %f seconds\\n\", end - start);\n",
        "\n",
        "  // collapse clause, single parallel for with guided schedule\n",
        "  total = 0.0;\n",
        "  start = omp_get_wtime();\n",
        "  #pragma omp parallel for collapse(2) schedule(guided)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < M; j++) {\n",
        "      for (int k = 0; k < (i % 50 + 1); k++) {\n",
        "        total += (i * j + k) * 1e-6;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  end = omp_get_wtime();\n",
        "  printf(\"Version 5 (guided, collapse(2)): %f seconds\\n\", end - start);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb_ukK7fX0k0",
        "outputId": "f3c8246e-8242-4fe8-e224-19bcde21da20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/schedules.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "A2Gk6i7ncx0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ schedules.cpp -fopenmp -o schedules\n",
        "!./schedules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkeViKuicyEt",
        "outputId": "cc89026a-3123-42c2-e4bf-584e2b55f836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenMP Nested Loop Parallelization Comparison\n",
            "N = 5000, M = 5000\n",
            "CHUNK = 1000, THREADS = 4 \n",
            "\n",
            "Version 1 (static, nested for): 1.903383 seconds\n",
            "Version 2 (dynamic, nested for): 1.865860 seconds\n",
            "Version 3 (dynamic, nested parallel for): 3.145333 seconds\n",
            "Version 4 (dynamic(CHUNK), collapse(2)): 4.494570 seconds\n",
            "Version 5 (guided, collapse(2)): 4.864830 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version 1**:\n",
        "<br>\n",
        "Only the outer parallel creates a team of threads.\n",
        "\n",
        "- Low overhead, only one parallel team created.\n",
        "- Static scheduling ensures predictable, reproducible thread assignment.\n",
        "- Poor load balancing.\n",
        "\n",
        "**Version 2**:\n",
        "<br>\n",
        "Still a single team of threads.\n",
        "Dynamic scheduling lets threads grab new chunks as they finish, compensates the unbalaned workload.\n",
        "\n",
        "- Load balancing.\n",
        "- More scheduling overhead than static.\n",
        "\n",
        "**Version 3**:\n",
        "<br>\n",
        "The outer parallel for creates one team of threads.\n",
        "Each outer thread that hits the inner parallel for spawns a new inner team.\n",
        "\n",
        "- Full control of both loop levels, each loop can scale and be scheduled independently.\n",
        "- Can exploit more cores if your hardware and OpenMP's runtime support nested teams efficiently.\n",
        "- High overhead: every outer iteration spawns a new parallel region.\n",
        "- Memory and scheduling overhead outweigh benefits for small and medium-sized workloads.\n",
        "- Likely to oversubscribe cores.\n",
        "\n",
        "**Version 4**:\n",
        "<br>\n",
        "The two loops are merged into a single iteration space of size N*M.\n",
        "One parallel team handles all (i, j) pairs directly.\n",
        "\n",
        "- More fine grained load balancing when combined with the dynamic schedule.\n",
        "- Scheduling overhead can be more effectively controlled with larger chunk sizes.\n",
        "- Only works if the nested loops are perfectly nested and independent.\n",
        "\n",
        "**Version 5**:\n",
        "<br>\n",
        "Same as version 4, but guided scheduling means that when there is a lot of work left to do, each thread is assigned a larger portion of it, with smaller and smaller portions being assigned as less work remains.\n",
        "\n",
        "- Very fine grained load balancing.\n",
        "- Extremely reduced scheduling overhead, as we issue more jobs only towards the end, to avoid leaving some threads at idle.\n",
        "\n",
        "Version 5 is the cleanest and often fastest option.\n",
        "<br>\n",
        "*Note: this may not show on Colab, as it gives us only 2 mere cores...*"
      ],
      "metadata": {
        "id": "2WZ6mMXbc8RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: in version 1 and 2, a reduction over `local_sum` is not needed, because the inner `#pragma omp for` is not creating a new parallel region, it is simply another work-sharing construct within the same parallel team spawned by the outer pragma.*"
      ],
      "metadata": {
        "id": "dkSRzD3MZvHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "\n",
        "If you are told that exactly one iteration every 10 needs to do 5x the work, and you have thousands of iterations, what is the best schedule and why?\n",
        "<!--\"static\", with batch multiple of 10, large enough to exploit caches, but not too large as not to cause severe unbalance in the number iterations given to each thread-->\n",
        "\n",
        "Assume now a workload consisting of roughly 2-3x as many iterations as there will be threads (e.g. 25 iterations, 10 threads), you know that each iteration has a one-in-three chance of requiring twice the amount of work as would a normal iteration. What schedule would work best and why?\n",
        "<!--\"dynamic\", chunks of 1, atmost 2, iterations, because with this little iterations the scheduling overhead is negligible and using larger chunks or \"guided\" could not effectively mitigate the unbalance due to the high likelyhood of heavier iterations-->"
      ],
      "metadata": {
        "id": "YUbYFwk8ktfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Déjà-vu**"
      ],
      "metadata": {
        "id": "XG9wQSjQou7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vector Product**"
      ],
      "metadata": {
        "id": "PHrlSXMjov0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just another way to see yet another reduce, but now with OpenMP pragmas!"
      ],
      "metadata": {
        "id": "nIylO0rkoy86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/vector_vector_prod.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  int N = 1000000;\n",
        "\n",
        "  double *A = (double*) malloc(N * sizeof(double));\n",
        "  double *B = (double*) malloc(N * sizeof(double));\n",
        "\n",
        "  // initialize vectors\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    A[i] = i * 0.001;\n",
        "    B[i] = (N - i) * 0.002;\n",
        "  }\n",
        "\n",
        "  double dot = 0.0;\n",
        "  double start_time = omp_get_wtime();\n",
        "\n",
        "  // parallel reduce\n",
        "  #pragma omp parallel for reduction(+ : dot) schedule(static)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    dot += A[i] * B[i];\n",
        "  }\n",
        "\n",
        "  double end_time = omp_get_wtime();\n",
        "\n",
        "  printf(\"Dot product = %.5f\\n\", dot);\n",
        "  printf(\"Computed in %.5f seconds using %d threads.\\n\",\n",
        "  end_time - start_time, omp_get_max_threads());\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "}\n"
      ],
      "metadata": {
        "id": "gI0oQxLCo1Qc",
        "outputId": "bfafbe06-b3f0-4132-f95f-ef8deb3da628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/vector_vector_prod.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -fopenmp /home/OpenMP/vector_vector_prod.cpp -o /home/OpenMP/vector_vector_prod\n",
        "!cd /home/OpenMP/\n",
        "!./vector_vector_prod"
      ],
      "metadata": {
        "id": "ob3-36atbyLj",
        "outputId": "f974e6c0-2c80-4abe-94f1-5f9ecc9948a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product = 333333333332.99908\n",
            "Computed in 0.00843 seconds using 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MatMul**"
      ],
      "metadata": {
        "id": "cdP1O3Oeo88A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just make each thread perform a vector-vector product!\n",
        "<br>\n",
        "However, we can try to exploit thread affinity to slightly improve cache performance by manually tiling the loop!\n",
        "<br>\n",
        "Say for example that we have a dual-socket motherboard with two 8-core CPUs, with each CPU itself divided in two NUMA nodes (e.g. each group of 4 cores has its own L2 cache and dedicated DRAM channel).\n",
        "<br>\n",
        "It's better if we split the work in 16 chunks, space far and wide 4 groups of those chunks, and then pull close the 4 chunks in each group.\n",
        "This equates to cutting the output matrix's rows and cols in 4 equi-sized groups. This results in 16 chunks. Then we give each of the 4 quadrants of the output matrix to a different NUMA node and each quarter of the quadrant to a thread.\n",
        "\n",
        "*Note: this is FAR from the best way to do a matmul on CPU! There are several improvements among which \"blocking\", a similar idea to tiling to better exploit caches!*\n",
        "<br>\n",
        "*More on this here:*\n",
        "- *BLAS: https://www.netlib.org/blas/*\n",
        "- *BLIS: https://dl.acm.org/doi/10.1145/2764454*\n",
        "- *More on BLIS: https://dl.acm.org/doi/10.1145/2925987*\n"
      ],
      "metadata": {
        "id": "qClT8PhKo_WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/matmul.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "void init_matrix(double *M, int N, double scale) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      M[i * N + j] = scale * ((i + j) % 100);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "  int N = 1024;\n",
        "  if (argc > 1) N = atoi(argv[1]);\n",
        "\n",
        "  printf(\"Matrix size: %d x %d\\n\", N, N);\n",
        "\n",
        "  double *A = (double*) malloc(N * N * sizeof(double));\n",
        "  double *B = (double*) malloc(N * N * sizeof(double));\n",
        "  double *C = (double*) calloc(N * N, sizeof(double));\n",
        "\n",
        "  init_matrix(A, N, 0.01);\n",
        "  init_matrix(B, N, 0.02);\n",
        "\n",
        "  int groups = 4;            // outer level (NUMA groups)\n",
        "  int threads_per_group = 4; // inner level threads per group\n",
        "  int tile_size = N / 4;     // 4 x 4 grid of tiles\n",
        "\n",
        "  omp_set_nested(1); // enable nested parallelism\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  // outer parallel region: spread affinity\n",
        "  #pragma omp parallel num_threads(groups) proc_bind(spread)\n",
        "  {\n",
        "    int gi = omp_get_thread_num(); // group index\n",
        "    int i_start = gi * tile_size;\n",
        "    int i_end   = (gi + 1) * tile_size;\n",
        "\n",
        "    // inner parallel region: close affinity within group\n",
        "    #pragma omp parallel num_threads(threads_per_group) proc_bind(close)\n",
        "    {\n",
        "      int gj = omp_get_thread_num(); // tile index within group\n",
        "      int j_start = gj * tile_size;\n",
        "      int j_end = (gj + 1) * tile_size;\n",
        "\n",
        "      for (int i = i_start; i < i_end; i++) {\n",
        "        for (int j = j_start; j < j_end; j++) {\n",
        "          double sum = 0.0;\n",
        "          for (int k = 0; k < N; k++) {\n",
        "            sum += A[i * N + k] * B[k * N + j];\n",
        "          }\n",
        "          C[i * N + j] = sum;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.3f s\\n\", t2 - t1);\n",
        "\n",
        "  free(A); free(B); free(C);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8QC-CbXrMuy",
        "outputId": "249a9b57-b27b-47a4-e558-c88c880a29ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/matmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "ICxNoujdDnYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ matmul.cpp -fopenmp -o matmul\n",
        "!./matmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7o7HTGADno8",
        "outputId": "6470bb51-1e06-4215-a651-1fefb58b888d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix size: 1024 x 1024\n",
            "Execution time: 14.621 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Histogram**"
      ],
      "metadata": {
        "id": "1F8IVKyzrPQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shared memory and atomics, here we go again!\n",
        "<br>\n",
        "Remember to rely on privatization!"
      ],
      "metadata": {
        "id": "wjgZFPH4rT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/histogram.cpp\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define NUM_LETTERS 26\n",
        "#define BIN_SIZE 4\n",
        "#define NUM_BINS ((NUM_LETTERS + BIN_SIZE - 1) / BIN_SIZE)\n",
        "\n",
        "void generate_random_string(char *s, size_t len) {\n",
        "  for (size_t i = 0; i < len; i++)\n",
        "    s[i] = 'a' + (rand() % 26);\n",
        "  s[len] = '\\0';\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "  size_t N = 10000000;\n",
        "\n",
        "  char *text = (char*) malloc(N + 1);\n",
        "  srand(42);\n",
        "  generate_random_string(text, N);\n",
        "\n",
        "  // global copy\n",
        "  int global_hist[NUM_BINS] = {0};\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    // private copy\n",
        "    int local_hist[NUM_BINS] = {0};\n",
        "\n",
        "    #pragma omp for\n",
        "    for (size_t i = 0; i < N; i++) {\n",
        "      char c = text[i];\n",
        "      if (c >= 'a' && c <= 'z') {\n",
        "        int bin = (c - 'a') / BIN_SIZE;\n",
        "        local_hist[bin]++;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // merge private copies atomically\n",
        "    for (int b = 0; b < NUM_BINS; b++) {\n",
        "      #pragma omp atomic\n",
        "      global_hist[b] += local_hist[b];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.4f s using %d threads\\n\", t2 - t1, omp_get_max_threads());\n",
        "\n",
        "  // print histogram\n",
        "  printf(\"\\nHistogram (4-letter bins):\\n\");\n",
        "  for (int b = 0; b < NUM_BINS; b++) {\n",
        "    char start = 'a' + b * BIN_SIZE;\n",
        "    char end   = start + BIN_SIZE - 1;\n",
        "    if (end > 'z') end = 'z';\n",
        "      printf(\"  %c-%c : %d\\n\", start, end, global_hist[b]);\n",
        "  }\n",
        "\n",
        "  free(text);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65hQ1asHrhbx",
        "outputId": "a0669080-dd1c-470d-a734-0bcbcd9f5416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/histogram.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "8IpzOOeWGhnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ histogram.cpp -fopenmp -o histogram\n",
        "!./histogram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKJxdBCwGfVx",
        "outputId": "866a4134-4b96-4c96-f02c-fe73ca60b2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 0.0392 s using 2 threads\n",
            "\n",
            "Histogram (4-letter bins):\n",
            "  a-d : 1538630\n",
            "  e-h : 1538792\n",
            "  i-l : 1538822\n",
            "  m-p : 1539186\n",
            "  q-t : 1536136\n",
            "  u-x : 1539567\n",
            "  y-z : 768867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "- what would be a good schedule for the parallel for? <!--static (at most guided) since all iterations are perfectly balanced, we just need to worry about maximizing the cache hits of each thread, so chunks should be reasonably larger while not becoming uneven-->\n",
        "- is privatization always the best option? <!--no, say that you are counting the occurrencies of 10M items, that are uniformly distributed in the input, then atomic updates on a single shared copy could be fast enough to justify saving the cost of replicating all counters-->"
      ],
      "metadata": {
        "id": "NSo7Gd7yiB7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sections and Critical Sections**"
      ],
      "metadata": {
        "id": "NnYd8YG3Bqyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two very apparent ways to optimize this code:"
      ],
      "metadata": {
        "id": "F97zRCGhCFWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/two_loops.cpp\n",
        "#include <cstdio>\n",
        "#include <omp.h>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "  const int N = 10000000;\n",
        "  const double target = std::sin(M_PI / 3.0);\n",
        "  std::vector<double> A(N, 0.0), B(N, 0.0);\n",
        "  std::vector<double> resultsA, resultsB;\n",
        "  double global_sum_A = 0.0;\n",
        "  double global_sum_B = 0.0;\n",
        "\n",
        "  double start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel default(none) shared(A, B, N, target, resultsA, resultsB, global_sum_A, global_sum_B)\n",
        "  {\n",
        "    double local_sum = 0.0;\n",
        "    std::vector<double> local_vals;\n",
        "    local_vals.reserve(N);\n",
        "\n",
        "    // first loop: compute A[i]\n",
        "    #pragma omp for schedule(static)\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      A[i] = std::sin(i * 0.001);\n",
        "      local_sum += A[i];\n",
        "      if (A[i] > target)\n",
        "        local_vals.push_back(A[i]);\n",
        "    }\n",
        "\n",
        "    // conditional accumulation on A\n",
        "    #pragma omp critical\n",
        "    {\n",
        "      global_sum_A += local_sum;\n",
        "      resultsA.insert(resultsA.end(), local_vals.begin(), local_vals.end());\n",
        "    }\n",
        "\n",
        "    // synchronize before the next loop\n",
        "    #pragma omp barrier\n",
        "    local_sum = 0.0;\n",
        "    local_vals.clear();\n",
        "\n",
        "    // second loop: compute B[i]\n",
        "    #pragma omp for schedule(static)\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      B[i] = std::cos(i * 0.001);\n",
        "      local_sum += B[i];\n",
        "      if (B[i] > target)\n",
        "        local_vals.push_back(B[i]);\n",
        "    }\n",
        "\n",
        "    // conditional accumulation on B\n",
        "    #pragma omp critical\n",
        "    {\n",
        "        global_sum_B += local_sum;\n",
        "        resultsB.insert(resultsB.end(), local_vals.begin(), local_vals.end());\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double end = omp_get_wtime();\n",
        "\n",
        "  printf(\"Global sums: A = %.3f, B = %.3f\\n\", global_sum_A, global_sum_B);\n",
        "  printf(\"Results A:\");\n",
        "  for (int r = 0; r < 10; r++) printf(\" %.3f\", resultsA[r]);\n",
        "  printf(\"\\nResults B:\");\n",
        "  for (int r = 0; r < 10; r++) printf(\" %.3f\", resultsB[r]);\n",
        "  printf(\"\\nTotal time: %.4f seconds\\n\", end - start);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JavpJFVCDwM",
        "outputId": "1589b054-3808-41cd-cefd-0cb13391eaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/two_loops.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the two loops (and subsequent critical sections) have no dependency with one-another, they could run concurrently!\n",
        "\n",
        "What we need to do is:\n",
        "- run each loop in its own parallel section\n",
        "- remove the barrier between them\n",
        "- name the two critical sections to make them independent\n",
        "- now however we shared work among sections, to also share the work of each loop's iterations, we need a nested parallel region for each loop"
      ],
      "metadata": {
        "id": "31K4qkd1Djxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/two_loops.cpp\n",
        "#include <cstdio>\n",
        "#include <omp.h>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "  omp_set_nested(true);\n",
        "\n",
        "  const int N = 10000000;\n",
        "  const double target = std::sin(M_PI / 3.0);\n",
        "  std::vector<double> A(N, 0.0), B(N, 0.0);\n",
        "  std::vector<double> resultsA, resultsB;\n",
        "  double global_sum_A = 0.0;\n",
        "  double global_sum_B = 0.0;\n",
        "\n",
        "  double start = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel num_threads(2) default(none) shared(A, B, N, target, resultsA, resultsB, global_sum_A, global_sum_B)\n",
        "  {\n",
        "    #pragma omp sections\n",
        "    {\n",
        "      // first loop: compute A[i]\n",
        "      #pragma omp section\n",
        "      {\n",
        "        double local_sum_A = 0.0;\n",
        "        std::vector<double> local_vals_A;\n",
        "        local_vals_A.reserve(N);\n",
        "\n",
        "        #pragma omp parallel for reduction(+:local_sum_A) schedule(static)\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "          A[i] = std::sin(i * 0.001);\n",
        "          local_sum_A += A[i];\n",
        "          if (A[i] > target)\n",
        "            local_vals_A.push_back(A[i]);\n",
        "        }\n",
        "\n",
        "        #pragma omp critical (acc_A)\n",
        "        {\n",
        "          global_sum_A += local_sum_A;\n",
        "          resultsA.insert(resultsA.end(), local_vals_A.begin(), local_vals_A.end());\n",
        "        }\n",
        "      }\n",
        "\n",
        "      // second loop: compute B[i]\n",
        "      #pragma omp section\n",
        "      {\n",
        "        double local_sum_B = 0.0;\n",
        "        std::vector<double> local_vals_B;\n",
        "        local_vals_B.reserve(N);\n",
        "\n",
        "        #pragma omp parallel for reduction(+:local_sum_B) schedule(static)\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "          B[i] = std::cos(i * 0.001);\n",
        "          local_sum_B += B[i];\n",
        "          if (B[i] > target)\n",
        "            local_vals_B.push_back(B[i]);\n",
        "        }\n",
        "\n",
        "        #pragma omp critical (acc_B)\n",
        "        {\n",
        "          global_sum_B += local_sum_B;\n",
        "          resultsB.insert(resultsB.end(), local_vals_B.begin(), local_vals_B.end());\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  double end = omp_get_wtime();\n",
        "\n",
        "  printf(\"Global sums: A = %.3f, B = %.3f\\n\", global_sum_A, global_sum_B);\n",
        "  printf(\"Results A:\");\n",
        "  for (int r = 0; r < 10; r++) printf(\" %.3f\", resultsA[r]);\n",
        "  printf(\"\\nResults B:\");\n",
        "  for (int r = 0; r < 10; r++) printf(\" %.3f\", resultsB[r]);\n",
        "  printf(\"\\nTotal time: %.4f seconds\\n\", end - start);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LLbr0JpEumU",
        "outputId": "9cea8a3b-f6d1-48f6-dc82-c7703765e591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/two_loops.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "2K_AXl_pF7EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ two_loops.cpp -fopenmp -o two_loops\n",
        "!./two_loops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GeweCvNF7Ut",
        "outputId": "81f3100b-92f5-4c08-fd09-282267f488c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global sums: A = 1952.308, B = -304.638\n",
            "Results A: 0.866 0.867 0.867 0.868 0.868 0.869 0.869 0.870 0.870 0.871\n",
            "Results B: 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000\n",
            "Total time: 0.5986 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tasks**"
      ],
      "metadata": {
        "id": "ZVxLNFXusHkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MergeSort**"
      ],
      "metadata": {
        "id": "4oYN1FLfsKDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much to say here, it's merge-sort, but with tasks..."
      ],
      "metadata": {
        "id": "EYmwxbGVVB8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/mergesort.cpp\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cstring>\n",
        "#include <omp.h>\n",
        "\n",
        "// merge two sorted halves (A and B) into the output array\n",
        "void merge_arrays(int* A, int nA, int* B, int nB, int* output) {\n",
        "  int i = 0, j = 0, k = 0;\n",
        "  while (i < nA && j < nB)\n",
        "    output[k++] = (A[i] < B[j]) ? A[i++] : B[j++];\n",
        "  while (i < nA) output[k++] = A[i++];\n",
        "  while (j < nB) output[k++] = B[j++];\n",
        "}\n",
        "\n",
        "void merge_sort_task(int* input, int n, int* output) {\n",
        "  if (n <= 2) {\n",
        "    // base case: sort trivially and return\n",
        "    if (n == 2 && input[0] > input[1]) {\n",
        "      output[0] = input[1];\n",
        "      output[1] = input[0];\n",
        "    } else {\n",
        "      output[0] = input[0];\n",
        "      if (n == 2) output[1] = input[1];\n",
        "    }\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  int mid = n / 2;\n",
        "\n",
        "  // allocate space for children halves\n",
        "  int* L_in  = &input[0];\n",
        "  int* R_in  = &input[mid];\n",
        "\n",
        "  int* L_out = (int*) malloc(mid * sizeof(int));\n",
        "  int* R_out = (int*) malloc((n - mid) * sizeof(int));\n",
        "\n",
        "  // spawn tasks for the left and right recursive sorts\n",
        "  #pragma omp task shared(L_in, L_out)\n",
        "  merge_sort_task(L_in, mid, L_out);\n",
        "  #pragma omp task shared(R_in, R_out)\n",
        "  merge_sort_task(R_in, n - mid, R_out);\n",
        "\n",
        "  // wait for both halves to finish\n",
        "  #pragma omp taskwait\n",
        "\n",
        "  // merge results in \"output\"\n",
        "  merge_arrays(L_out, mid, R_out, n - mid, output);\n",
        "\n",
        "  free(L_out);\n",
        "  free(R_out);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 64;\n",
        "  int* A = (int*) malloc(N * sizeof(int));\n",
        "  int* B = (int*) malloc(N * sizeof(int));\n",
        "\n",
        "  // fill with random numbers\n",
        "  for (int i = 0; i < N; i++)\n",
        "  A[i] = rand() % 100;\n",
        "\n",
        "  printf(\"Original array:\\n\");\n",
        "  for (int i = 0; i < N; i++) printf(\"%d \", A[i]);\n",
        "    printf(\"\\n\\n\");\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    #pragma omp single\n",
        "    merge_sort_task(A, N, B);\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "\n",
        "  printf(\"Sorted array:\\n\");\n",
        "  for (int i = 0; i < N; i++) printf(\"%d \", B[i]);\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  printf(\"\\nExecution time: %.6f s\\n\", t2 - t1);\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7pmDNfLVKGd",
        "outputId": "184d6092-b3d6-4de0-b26c-1a394dada32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/mergesort.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "htx8KryzVKnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ mergesort.cpp -fopenmp -o mergesort\n",
        "!./mergesort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZLyPxvoVNkH",
        "outputId": "adb0d466-b6cc-4898-cbae-25594f647a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array:\n",
            "83 86 77 15 93 35 86 92 49 21 62 27 90 59 63 26 40 26 72 36 11 68 67 29 82 30 62 23 67 35 29 2 22 58 69 67 93 56 11 42 29 73 21 19 84 37 98 24 15 70 13 26 91 80 56 73 62 70 96 81 5 25 84 27 \n",
            "\n",
            "Sorted array:\n",
            "2 5 11 11 13 15 15 19 21 21 22 23 24 25 26 26 26 27 27 29 29 29 30 35 35 36 37 40 42 49 56 56 58 59 62 62 62 63 67 67 67 68 69 70 70 72 73 73 77 80 81 82 83 84 84 86 86 90 91 92 93 93 96 98 \n",
            "\n",
            "Execution time: 0.000426 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algebraic Expression Evaluation**"
      ],
      "metadata": {
        "id": "dLnz7stMsLsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate this linar algebra expression with OpenMP tasks:\n",
        "\n",
        "$A, B, C, D, E \\in \\mathbb{R}^{4 \\times 4}$\n",
        "\n",
        "$A \\cdot (A \\cdot B - C \\cdot D) - E^2 \\cdot D^2$"
      ],
      "metadata": {
        "id": "xo-Eulc7eOlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/algebra.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 4\n",
        "\n",
        "void matmul(double A[N][N], double B[N][N], double C[N][N]) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      double s = 0.0;\n",
        "      for (int k = 0; k < N; k++) s += A[i][k] * B[k][j];\n",
        "      C[i][j] = s;\n",
        "    }\n",
        "}\n",
        "\n",
        "// alpha = +1 => add, alpha = -1 => sub\n",
        "void matadd(double A[N][N], double B[N][N], double C[N][N], double alpha) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      C[i][j] = A[i][j] + alpha * B[i][j];\n",
        "}\n",
        "\n",
        "void init(double M[N][N], double scale) {\n",
        "  for (int i = 0; i < N; i++)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      M[i][j] = scale * ((i + j) % 7);\n",
        "}\n",
        "\n",
        "void printmat(const char *name, double M[N][N]) {\n",
        "  printf(\"%s:\\n\", name);\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < N; j++)\n",
        "      printf(\"%6.2f \", M[i][j]);\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  double A[N][N], B[N][N], C[N][N], D[N][N], E[N][N];\n",
        "  double AB[N][N], CD[N][N], inner[N][N], Aterm[N][N];\n",
        "  double E2[N][N], D2[N][N], E2D2[N][N], Result[N][N];\n",
        "\n",
        "  init(A, 0.5); init(B, 0.7); init(C, 0.9);\n",
        "  init(D, 1.1); init(E, 1.3);\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  #pragma omp single\n",
        "  {\n",
        "    // AB = A*B\n",
        "    #pragma omp task depend(out:AB)\n",
        "    matmul(A, B, AB);\n",
        "\n",
        "    // CD = C*D\n",
        "    #pragma omp task depend(out:CD)\n",
        "    matmul(C, D, CD);\n",
        "\n",
        "    // inner = AB - CD\n",
        "    #pragma omp task depend(in:AB, CD) depend(out:inner)\n",
        "    matadd(AB, CD, inner, -1.0);\n",
        "\n",
        "    // Aterm = A * inner\n",
        "    #pragma omp task depend(in:A, inner) depend(out:Aterm)\n",
        "    matmul(A, inner, Aterm);\n",
        "\n",
        "    // E2 = E*E\n",
        "    #pragma omp task depend(out:E2)\n",
        "    matmul(E, E, E2);\n",
        "\n",
        "    // D2 = D*D\n",
        "    #pragma omp task depend(out:D2)\n",
        "    matmul(D, D, D2);\n",
        "\n",
        "    // E2D2 = E2 * D2\n",
        "    #pragma omp task depend(in:E2, D2) depend(out:E2D2)\n",
        "    matmul(E2, D2, E2D2);\n",
        "\n",
        "    // Result = Aterm - E2D2\n",
        "    #pragma omp task depend(in:Aterm, E2D2) depend(out:Result)\n",
        "    matadd(Aterm, E2D2, Result, -1.0);\n",
        "\n",
        "    #pragma omp taskwait\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "  printf(\"Execution time: %.3f ms\\n\", 1e3 * (t2 - t1));\n",
        "\n",
        "  printmat(\"Result\", Result);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "3Bl4ZHYOejeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "7q2xJu8zejsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ algebra.cpp -fopenmp -o algebra\n",
        "!./algebra"
      ],
      "metadata": {
        "id": "IYGgIxHjej6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sum of Products**"
      ],
      "metadata": {
        "id": "UhkFvLvdf1cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume you are given strings repersenting algebraic expressions in the form of a sums of products with only 2 possible variable, \"a\" and \"b\", like:\n",
        "<br>\n",
        "\"a+a\\*b\\*b+b\\*b\\*a\\*b\\*a+b\\*b\\*b\"\n",
        "\n",
        "The following program parses the expression and dynamically spawns an OpenMP task to solve each product, then reducing sequentially (an alternative implementation could have each task di an atomic add to the global sum).\n",
        "<br>\n",
        "This scheme comes in handy when the input is very long and you would rather not have to iterate over it more than once, but you can still identify suitable independente (and thus parallelizable) units of work as you go."
      ],
      "metadata": {
        "id": "jss7qwm0f4M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/sop.cpp\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N_TERMS 64\n",
        "\n",
        "// evaluate a product term like \"a*b*b*a\"\n",
        "float eval_product(const char *term, float a, float b) {\n",
        "  float res = 1.0f;\n",
        "  for (const char *p = term; *p; p++) {\n",
        "    if (*p == 'a') res *= a;\n",
        "    else if (*p == 'b') res *= b;\n",
        "  }\n",
        "  return res;\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "  if (argc < 2) {\n",
        "      std::cerr << \"Usage: \" << argv[0] << \" <expression>\\n\";\n",
        "      return 1;\n",
        "  }\n",
        "\n",
        "  const char* expr = argv[1]; // e.g. \"a*b*b*a+a*a+b*b\"\n",
        "  float a = 2.0f, b = 3.0f;\n",
        "\n",
        "  printf(\"Expression: %s\\n\", expr);\n",
        "  printf(\"a = %.2f, b = %.2f\\n\", a, b);\n",
        "\n",
        "  char *expr_copy = strdup(expr); // duplicate the string\n",
        "  char *terms[N_TERMS]; // array of pointers to product terms\n",
        "  int n_terms = 0;\n",
        "  // split the expression at every '+', isolating product terms\n",
        "  for (char *tok = strtok(expr_copy, \"+\"); tok; tok = strtok(NULL, \"+\"))\n",
        "    terms[n_terms++] = tok;\n",
        "\n",
        "  float partials[N_TERMS] = {0.0f};\n",
        "  float result = 0.0f;\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  #pragma omp parallel\n",
        "  #pragma omp single\n",
        "  {\n",
        "    // spawn a parsing task that spawns product tasks\n",
        "    #pragma omp task depend(out:partials)\n",
        "    {\n",
        "      for (int i = 0; i < n_terms; i++) {\n",
        "        int idx = i;\n",
        "        const char *term = terms[i];\n",
        "        #pragma omp task firstprivate(idx, term) depend(out:partials[idx])\n",
        "        {\n",
        "          partials[idx] = eval_product(term, a, b);\n",
        "          //printf(\"Term %d (%s) = %.2f\\n\", idx, term, partials[idx]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // reduction task that depends on all partials\n",
        "    #pragma omp task depend(in:partials) depend(out:result)\n",
        "    {\n",
        "      float sum = 0.0f;\n",
        "      for (int i = 0; i < n_terms; i++) sum += partials[i];\n",
        "      result = sum;\n",
        "    }\n",
        "\n",
        "    #pragma omp taskwait\n",
        "  }\n",
        "\n",
        "  double t2 = omp_get_wtime();\n",
        "\n",
        "  printf(\"Result = %.2f\\n\", result);\n",
        "  printf(\"Execution time: %.3f ms\\n\", 1e3 * (t2 - t1));\n",
        "\n",
        "  free(expr_copy);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "A3BEHehGgr0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b74cb7f-d89f-4dcb-8cbf-9727aaf2d5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/sop.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "BMtqdvwCgsCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ sop.cpp -fopenmp -o sop\n",
        "!./sop a*b*b*a+a*a+b*b+a*b*b*a*a*a+b*b*b*b*b+a*b*a*a*b*a+a*a+b*b*a*a*a"
      ],
      "metadata": {
        "id": "SgGxzZhPgscB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4524cd-b459-4e5d-f3e7-a5e268b8ec9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression: a*b*b*a+a*a+b*b+a*b*b*a*a*a+b*b*b*b*b+a*b*a*a*b*a+a*a+b*b*a*a*a\n",
            "a = 2.00, b = 3.00\n",
            "Result = 0.00\n",
            "Execution time: 0.146 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MatMul with Tasks**"
      ],
      "metadata": {
        "id": "CWCs-sLdjTx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see a slightly new pragma, `taskloop`: splits the iteration space of a loop into OpenMP tasks.\n",
        "While this looks similar to the `for` worksharing construct, the behavior is fundamentally different.\n",
        "When using the `for` construct, all threads in the parallel region have to encounter the construct so that they can split up the work, whereas the taskloop construct needs only be executed by a single thread (e.g. the master thread).\n",
        "\n",
        "*Note: the taskloop construct is defined in a way that is similar to the definition of a regular task.\n",
        "This means that if N threads encounter the construct, each of the threads will start executing the same loop, so the loop will be executed N times rather than having a single incarnation of the loop split between them.*\n",
        "\n",
        "The `grainsize` clause defines how many iterations should be executed per task.\n",
        "Funnily enough, the OpenMP standard allows this to be an interval, which in this example will be eight to sixteen iterations, so an implementation has some flexibility in choosing the exact number of iterations (and, therefore, tasks).\n",
        "The construct also supports the `num_tasks` clause, which specifies exactly how many tasks should be created, and then adjusts the chunk size accordingly."
      ],
      "metadata": {
        "id": "j6WkolDPjaIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/tmatmul.cpp\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <omp.h>\n",
        "\n",
        "void matmul_taskloop(float *C, const float *A, const float *B, size_t n) {\n",
        "  #pragma omp parallel firstprivate(n)\n",
        "  {\n",
        "    #pragma omp master\n",
        "    {\n",
        "      // we could also add \"collapse(3)\" if we had >n HW threads\n",
        "      #pragma omp taskloop firstprivate(n) grainsize(8)\n",
        "      for (int i = 0; i < n; ++i)\n",
        "        for (int k = 0; k < n; ++k)\n",
        "          for (int j = 0; j < n; ++j)\n",
        "            C[i * n + j] += A[i * n + k] * B[k * n + j];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void init_matrix(float *M, int n, float scale) {\n",
        "  for (int i = 0; i < n; i++)\n",
        "    for (int j = 0; j < n; j++)\n",
        "      M[i*n + j] = scale * float((i + j) % 13);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int n = 512;\n",
        "\n",
        "  printf(\"Matrix size: %d x %d\\n\", n, n);\n",
        "\n",
        "  float *A = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "  float *B = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "  float *C = (float*) aligned_alloc(64, n * n * sizeof(float));\n",
        "\n",
        "  init_matrix(A, n, 0.01f);\n",
        "  init_matrix(B, n, 0.02f);\n",
        "  init_matrix(C, n, 0.0f);\n",
        "\n",
        "  double t1 = omp_get_wtime();\n",
        "  matmul_taskloop(C, A, B, n);\n",
        "  double t2 = omp_get_wtime();\n",
        "\n",
        "  printf(\"Time: %.3f sec\\n\", t2 - t1);\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "  free(C);\n",
        "}\n"
      ],
      "metadata": {
        "id": "w6zEnj11jZqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa07bbdd-fa75-4932-9f63-b35092491632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/tmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "_pxw3kqU_toA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ tmatmul.cpp -fopenmp -o tmatmul\n",
        "!./tmatmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uLocZpO_tIS",
        "outputId": "dda6a8d3-eb19-4e25-d3e2-e89579cb108e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix size: 512 x 512\n",
            "Time: 0.707 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cancellation**"
      ],
      "metadata": {
        "id": "FWXcfjqOZbId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very useful for programs that \"work in parallel towards an objective\". And upon the achievement of the objective by on threads, others end their purpose too.\n",
        "\n",
        "In this simple example, we search for a specific value in an array. Upon a thread finding it, cancellation allows the whole parallel region to end cleanly."
      ],
      "metadata": {
        "id": "KvF9433jZdsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/cancellation.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 1000000\n",
        "\n",
        "int main() {\n",
        "  const int target = 777;\n",
        "  int found_index = -1;\n",
        "\n",
        "  int data[N];\n",
        "  for (int i = 0; i < N; ++i)\n",
        "    data[i] = 1;\n",
        "\n",
        "  data[543210] = target; // hide the target somewhere\n",
        "\n",
        "  // parallel search with cancellation\n",
        "  #pragma omp parallel shared(found_index)\n",
        "  {\n",
        "    #pragma omp for\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "\n",
        "      if (data[i] == target) {\n",
        "        found_index = i;\n",
        "        // stop everyone\n",
        "        #pragma omp cancel for\n",
        "      }\n",
        "\n",
        "      #pragma omp cancellation point for\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"Found target at index: %d\\n\", found_index);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKvCy69AZe0O",
        "outputId": "e44684bd-73e8-46e5-efdf-ee820fc00f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/cancellation.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "axrJWa7dZeJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ cancellation.cpp -fopenmp -o cancellation\n",
        "!./cancellation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e4aisDyZfXm",
        "outputId": "45b479d1-a907-40f7-a27a-de63679e2634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found target at index: 543210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"loop-dependencies\"></a>\n",
        "## **Loop Dependencies**"
      ],
      "metadata": {
        "id": "NsUY71pncAZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loops are always the easiest, and most beneficial, target for optimization!\n",
        "That is all well and good so long as each iteration is independent of the others. However, start considering inter-iteration (loop-carried) data dependencies, and there may be an hard limit on the parallelism that a loopnest can exploit.\n",
        "\n",
        "These exercises focus on identifying such data dependecies between iterations and thus inferring the highest degree of concurrency existing in a loopnest.\n",
        "\n",
        "How to:\n",
        "- identify the indices accessed by each iterations\n",
        "- identify the memory locations read and written by each iteration\n",
        "- see if subsequent iterations cause a pattern of reads occurring where there previously was a write (RAW), writing something that was read previously (WAR), written something twice but with different values (WAW)\n",
        "- measure the distance between the each of the above reads and writes\n",
        "- how those patterns would unfold sequentially is your ground truth\n",
        "- you can go parallel only so long as the relative order of operations in each dependency is preserved\n",
        "\n",
        "*Another way to view loops with inter-iteraiton dependencies: try to find a window of (often contigous, but not necessarily) iterations that can run in parallel. Such a window is valid iff no data dependencie outgoing from its iterations lands back in the window itself.*"
      ],
      "metadata": {
        "id": "YDD4sG33cEPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 10**"
      ],
      "metadata": {
        "id": "mmbsYBuFeMYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "for (j = 10; j < 100; j++)\n",
        "    for (i = 10; i < 100; i++)\n",
        "        a[i][j] += f(a[i][j - 10]);"
      ],
      "metadata": {
        "id": "j9F9SA1aeO4n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How and under what conditions can the following code be parallelized? Is there a way to write the code for better parallelization?\n",
        "<!--\n",
        "As of now, the \"i\" loop can run fully in parallel for every iteration of \"j\".\n",
        "Every batch of 10 consecutive iterations on \"j\" can run concurrently, as there is a loop-carried dependency W(j)(a)->R(j + 10)(a) [write now, read 10 js after].\n",
        "\n",
        "Yes, we can split the \"j\" loop to explicitate the dependency:\n",
        "for (j_d = 1; j_d < 10; j_d++)\n",
        "    for (j_p = 0; j_p < 10; j_p++) // can run fully in parallel now\n",
        "        int j = j_d*10 + j_p;\n",
        "        for (i = 10; i < 100; i++)\n",
        "            a[i][j] += f(a[i][j - 10]);\n",
        "-->"
      ],
      "metadata": {
        "id": "H9Ac3rHrebtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 11**"
      ],
      "metadata": {
        "id": "i26MCz01t76K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "for (i = 4; i < 100; i++) {\n",
        "    for (j = 1; j < 100; j++)\n",
        "        a[i][0] *= f(a[i][j]);\n",
        "    for (j = 0; j < 100; j++)\n",
        "        b[i - 4][j] += f(b[i][j]);\n",
        "}"
      ],
      "metadata": {
        "id": "09M3Bc7st8Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How and under what conditions can the following code be parallelized? Is there a way to write the code for better parallelization? Assume that `a` and `b` store integers.\n",
        "<!--\n",
        "Note that \"a\" only depends on \"a\" and \"b\" only on \"b\". Therefore an independent \"i\" loop could be built for each of them.\n",
        "\n",
        "Then, the \"j\" loop on \"a\" is essentially performing a product-based reduction of all elements of \"a\" for a given \"i\". So that can be easily parallelized, e.g. by following a binary reduction tree. While the \"i\" loop on \"a\" can run fully in parallel.\n",
        "\n",
        "For \"b\", instead, each \"i\" iteration depends on the \"i-4\" one, therefore we can at best run 4 iterations in parallel by splitting the loops into two.\n",
        "Finally, \"b\"'s \"j\" loop can run fully in parallel for each \"i\", as there are no dependencies between iterations.\n",
        "\n",
        "Final version:\n",
        "for (i = 4; i < 100; i++)\n",
        "    // e.g. omp for reduce(*:a[i][0])\n",
        "    for (j = 1; j < 100; j++)\n",
        "        a[i][0] *= f(a[i][j]);\n",
        "\n",
        "for (i_d = 1; i_d < 25; i_d++)\n",
        "    for (i_p = 0; i_p < 4; i_p++)\n",
        "        int i = i_d*4 + i_p;\n",
        "        for (j = 0; j < 100; j++)\n",
        "            b[i-4][j] += f(b[i][j]);\n",
        "-->"
      ],
      "metadata": {
        "id": "efBULsomt8Wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 12**"
      ],
      "metadata": {
        "id": "KmJNzxMmzgb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "for (j = 0; j < 999; j++)\n",
        "    for (i = 2; i < 999; i++)\n",
        "        a[i][j] = f(a[i - 2][j]);"
      ],
      "metadata": {
        "id": "YEhmJ-Dfzj4d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the cache-friendliness of this code and consider its relationship to parallelizability. How does the memory layout of the array affect performance, and what changes could improve cache utilization vs parallelizability? Justify your answer. Assume that `a` stores integers.\n",
        "<!--\n",
        "Assuming \"a\" to be stored in row-major, we have that elements with consecutive \"j\"s are contigous in memory, while between consecutive \"i\"s there is a huge gap. Therefore, one innermost iteration, that is on \"i\", accesses two elements quite far apart, the next one jumps to two further different locations, and only the next one again has \"i-2\" landing close to where the first iteration wrote, and so on. This repeats for all \"j\"s.\n",
        "In terms of performance, at regime, this would cause one every two memory accesses to be a cache miss.\n",
        "Solution:\n",
        "Either store \"a\" in col-major or iterate on \"i\" first.\n",
        "\n",
        "For parallelizability, two iterations of \"i\" could run in parallel, therefore we could break the \"i\" loop into two.\n",
        "The \"j\" loop is instead fully parallelizable.\n",
        "\n",
        "\n",
        "One possible final version:\n",
        "for (i_d = 1; i_d < 499; i_d++)\n",
        "    for (i_p = 0; i_p < 2; i_p++)\n",
        "        int i = i_d*2 + i_p;\n",
        "        for (j = 0; j < 999; j++)\n",
        "            a[i][j] = f(a[i - 2][j]);\n",
        "-->"
      ],
      "metadata": {
        "id": "kp_BXfJtzkQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 13**"
      ],
      "metadata": {
        "id": "Ma4x8L2LQRuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "for (i = 0; i < 100; i++)\n",
        "  for (j = 5; j < 200; j++)\n",
        "    a[i][j] = f(a[i][j - 5], b[i + 1][j]);"
      ],
      "metadata": {
        "id": "wfL1zRPFQR8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under what conditions can the code be parallelized? Is there a way to restructure the loops to expose more parallelism?\n",
        "\n",
        "There is a loop-carried dependency with distance 5 on \"j\": W(j)(a)->R(j + 5)(a).\n",
        "Therefore only 5 iterations on \"j\" can run in parallel at once.\n",
        "All iterations on \"i\" can instead run in parallel.\n",
        "As for \"b\", it carries no dependencies, as it is never written.\n",
        "\n",
        "We can rewrite the loopnest as follows to explose more parallelism:\n",
        "for (i = 0; i < 100; i++) // fully parallel\n",
        "  for (j_d = 1; j_d < 40; j_d++)\n",
        "    for (j_p = 0; j_p < 5; j_p++) // now those can run in parallel\n",
        "      int j = j_d * 5 + j_p;\n",
        "      a[i][j] = g(a[i][j - 5], b[i + 1][j]);"
      ],
      "metadata": {
        "id": "_Vw6ssSCQSKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 14**"
      ],
      "metadata": {
        "id": "WpfLfSzdSz4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "// assume \"a\" already initialized to some useful data\n",
        "for (i = 0; i < 100; i++)\n",
        "  for (j = 1; j < 100; j++)\n",
        "    a[i][j] = f(a[i][j - 1], a[i + 1][j]);"
      ],
      "metadata": {
        "id": "M4YPAPSWS0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine true data dependencies. Is either loop safe to parallelize? Is there any transformation that could expose more parallelism?\n",
        "\n",
        "Dependencies:\n",
        "W(i)(j)(a) -> R(i)(j + 1)(a) // RAW leftward\n",
        "R(i)(j)(a) -> W(i + 1)(j)(a) // WAR downward\n",
        "\n",
        "In words: \"to compute my element, I need the original, untouched, one below me, and the already computed on on my left\".\n",
        "\n",
        "This forms a tight \"dependencies diamond\", for which loop parallelization (over \"i\" or \"j\") is impossible. And even if the offsets were slightly more than \"1\"s, very little could be achieved.\n",
        "\n",
        "However, we CAN parallelize over each diagonal! Since all elements on each diagonal are fully independent!\n",
        "We can iterate over a diagonal at a time, top-left towards the bottom-right, and inside each diagonal all computations can run in parallel because:\n",
        "- each diagonal only sees each row once, top to bottom, therefore the WAR is satisfied\n",
        "- each diagonal only sees each column once, left to right, therefore the RAW is satisfied\n",
        "\n",
        "Possible rewrite:\n",
        "for (int d = 1; d <= 198; d++) // sequential: one diagonal at a time\n",
        "  for (int i = 0; i <= d; i++) // can be fully parallel: inside the diagonal\n",
        "    int j = d - i;\n",
        "    if (i >= 0 && i < 100 && j >= 1 && j < 100 && i + 1 < 100)\n",
        "      a[i][j] = f(a[i][j - 1], a[i + 1][j]);\n",
        "\n",
        "Only drawback: the number of the elements on each diagonal varies! Therefore each outer iteration has a different number of possibly parallel inner iterations..."
      ],
      "metadata": {
        "id": "1hN8mqbkS0hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SoA vs AoS**"
      ],
      "metadata": {
        "id": "4Z6q5N1tL_T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good momento to pause and reflect on access patterns and cache locality...\n",
        "\n",
        "<br>\n",
        "\n",
        "Example of when **AoS (Array of Structures)** wins:\n",
        "\n",
        "You repeatedly iterate over all fields of one object at a time, e.g., computing the Euclidean length of each 3D vector.\n",
        "<br>\n",
        "Good spatial locality is achieved if all elements of each vector are close together."
      ],
      "metadata": {
        "id": "ejPubvekMCoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/highlight_only.cpp\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "\n",
        "struct Vec3 {\n",
        "  float x, y, z;\n",
        "};\n",
        "\n",
        "// this computation uses all three components per element -> keep them close to each other to localize accesses\n",
        "void compute_lengths(const std::vector<Vec3> &points, std::vector<float> &out) {\n",
        "  size_t N = points.size();\n",
        "  for (size_t i = 0; i < N; ++i) {\n",
        "    const Vec3 &p = points[i];\n",
        "    out[i] = std::sqrt(p.x*p.x + p.y*p.y + p.z*p.z);\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // AoS\n",
        "  std::vector<Vec3> pts(1'000'000, {1,2,3});\n",
        "  std::vector<float> lengths(pts.size());\n",
        "  compute_lengths(pts, lengths);\n",
        "  std::cout << \"First length = \" << lengths[0] << \"\\n\";\n",
        "}\n"
      ],
      "metadata": {
        "id": "CaLd7z_QMC3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why this is optimal (AoS):\n",
        "- we repeatedly load x, y, and z of the same object\n",
        "- with AoS, these are near each other, leading to fewer cache misses\n",
        "- SIMD compilers can load {x, y, z} as a single vector load\n",
        "- prefetching becomes trivial\n",
        "\n",
        "Using SoA instead would cause:\n",
        "- three independent large arrays to fetch\n",
        "- more translation lookaside buffer (TLB) pressure\n",
        "- more cache bandwidth needed (streams from 3 arrays)"
      ],
      "metadata": {
        "id": "hvPOHaH8XQRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of when **SoA (Structure of Arrays)** wins:\n",
        "\n",
        "You repeatedly iterate over one field across all objects, e.g. updating only the x coordinates of many points.\n",
        "Good spatial locality is achieved if elements accessed in quick succession are nearby."
      ],
      "metadata": {
        "id": "Rwkx8AlgMDHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/highlight_only.cpp\n",
        "#include <vector>\n",
        "#include <iostream>\n",
        "\n",
        "// SoA\n",
        "struct PointsSoA {\n",
        "  std::vector<float> x, y, z;\n",
        "  PointsSoA(size_t N) : x(N), y(N), z(N) {}\n",
        "};\n",
        "\n",
        "// this computation uses only one field -> keep is successive values close for locality\n",
        "void update_x(PointsSoA &pts) {\n",
        "  size_t N = pts.x.size();\n",
        "  for (size_t i = 0; i < N; ++i) {\n",
        "    pts.x[i] += 1.0f;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  PointsSoA pts(1'000'000);\n",
        "  update_x(pts);\n",
        "  std::cout << \"First x = \" << pts.x[0] << \"\\n\";\n",
        "}\n"
      ],
      "metadata": {
        "id": "2n0VtEl_MDUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why this is optimal (SoA):\n",
        "- the loop reads/modifies only the x array\n",
        "- memory access is perfectly sequential\n",
        "- compilers generate SIMD code easily for sequential accesses\n",
        "- zero waste: no need to touch y or z\n",
        "\n",
        "AoS here would cause:\n",
        "- hardware reads fill cache lines with {x, y, z} even though you only need x\n",
        "- you waste 2/3 of bandwidth\n",
        "- lower memory throughput\n",
        "- harder for the compiler to auto-vectorize (strided loads instead of contiguous)"
      ],
      "metadata": {
        "id": "hciGcfOnYIE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parallel Patterns**"
      ],
      "metadata": {
        "id": "E5Ds6XFzW9Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of these examples were originally from this [notebook](https://colab.research.google.com/drive/1guQZSGxxDmSizKv4cHd-H0WpgpA4aw9I?usp=sharing).\n",
        "<br>\n",
        "And since that notebook was written in C++, we are gonna stick with C++ for a bit here as well.\n",
        "\n",
        "Write the following file before compiling any of the examples below.\n",
        "<br>\n",
        "This is just a basic helper to time the various patterns."
      ],
      "metadata": {
        "id": "x2KUva5OpWM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/timer.hpp\n",
        "#ifndef TIMER_HDR\n",
        "#define TIMER_HDR\n",
        "\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "\n",
        "template <class clock_type = std::chrono::high_resolution_clock> class timer {\n",
        "  using time_point = std::chrono::time_point<clock_type>;\n",
        "  time_point _start;\n",
        "  std::string _name;\n",
        "\n",
        "public:\n",
        "  timer(std::string name = \"\")\n",
        "      : _start(clock_type::now()), _name(std::move(name)) {}\n",
        "  ~timer() {\n",
        "    const auto diff = std::chrono::duration_cast<std::chrono::milliseconds>(\n",
        "        clock_type::now() - _start);\n",
        "    std::cerr << \"[ function \" << _name << \" took \" << diff.count() << \"ms ]\"\n",
        "              << std::endl;\n",
        "  }\n",
        "};\n",
        "\n",
        "template <class callable_type, class... arguments_type>\n",
        "inline auto timer_print(std::string name, callable_type &&function,\n",
        "                        arguments_type &&...args) {\n",
        "  timer t(std::move(name));\n",
        "  return function(args...);\n",
        "}\n",
        "\n",
        "#define time(name, ...) timer_print(#name, name, __VA_ARGS__)\n",
        "\n",
        "#endif // TIMER_HDR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGcQnEI8pPwA",
        "outputId": "1633756e-8c9f-4ff7-cd3f-1cf545654349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/timer.hpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General rule of thum:\n",
        "- MAP: each element updates independently of all others\n",
        "- GATHER: each element compute the location where to read (in other words, threads sit on the sender side)\n",
        "- SCATTER: each element compute the location where to write (in other words, threads sit on the receiver side)"
      ],
      "metadata": {
        "id": "O1wxBNVOQeR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Map**"
      ],
      "metadata": {
        "id": "BI3NPZLOXD_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This application implement the *SAXPY* : Single Precision A times X plus Y, which is the computation `z[i] = a*x[i] + y[i]` with `x, y, z` three vectors of equal length.\n",
        "\n",
        "Clearly, this can be seen as a MAP pattern from each element of `x` and `y` to one of `z`, and parallelized as such.\n",
        "\n",
        "After recognizing the pattern, a MAP is trivial to parallelize in OpenMP, almost always merely requiring a `parallel for`."
      ],
      "metadata": {
        "id": "teK3rbkCn7UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/map.cpp\n",
        "#include <algorithm>\n",
        "#include <array>\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "// define the data structures\n",
        "static constexpr auto vector_size = std::size_t{10000};\n",
        "std::array<float, vector_size> x, y, z;\n",
        "\n",
        "// define a random generator\n",
        "float random_float() {\n",
        "  static std::mt19937 random_generator{2023};\n",
        "  std::uniform_real_distribution<float> interval(float{0.0}, float{1.0});\n",
        "  return interval(random_generator);\n",
        "}\n",
        "\n",
        "// define the computation [mapped] function\n",
        "void saxpy(std::array<float, vector_size> &z, const float a, const std::array<float, vector_size> &x, const std::array<float, vector_size> &y) {\n",
        "  #pragma omp parallel for\n",
        "  for (std::size_t i = 0; i < vector_size; ++i) {\n",
        "    z[i] = a * x[i] + y[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // initialize the input data\n",
        "  const float a = 4.87;\n",
        "  std::generate(std::begin(x), std::end(x), random_float);\n",
        "  std::generate(std::begin(y), std::end(y), random_float);\n",
        "  std::fill(std::begin(z), std::end(z), float{0});\n",
        "\n",
        "  // perform the computation\n",
        "  time(saxpy, z, a, x, y);\n",
        "\n",
        "  // print the output\n",
        "  std::cout << \"z = {\";\n",
        "  if (vector_size > std::size_t{0}) {\n",
        "    std::cout << z[0];\n",
        "  }\n",
        "  for (std::size_t i = 1; i < std::min(std::size_t{10}, vector_size); ++i) {\n",
        "    std::cout << \", \" << z[i];\n",
        "  }\n",
        "  if (vector_size > std::size_t{10}) {\n",
        "    std::cout << \", ...\";\n",
        "  }\n",
        "  std::cout << \"}\" << std::endl;\n",
        "\n",
        "  // if we reach this point, everything is fine\n",
        "  return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xwn3RXin8BA",
        "outputId": "14def0ff-cb9d-43bc-ede3-144ab3f320d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/map.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "LwavV1abn8P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ map.cpp -fopenmp -o map\n",
        "!./map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLyJ88rxn-DR",
        "outputId": "a1386905-24a1-4ba8-9339-2fb0792c8497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ function saxpy took 0ms ]\n",
            "z = {2.48043, 3.70224, 4.61079, 3.09507, 3.03533, 4.25946, 1.59451, 0.476707, 1.19255, 4.47768, ...}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Map Reduce**"
      ],
      "metadata": {
        "id": "km7PBnWkoGyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we need to compute the [Shannon Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of a large-ish dataset.\n",
        "\n",
        "Given an array of length $N$, containing integers in the interval $\\{0, \\dots K - 1\\}$, such entropy is defined as:\n",
        "$$\n",
        "H = -\\sum_{i=0}^{K-1} p_i \\cdot log_2(p_i) \\text{ and } p_i = \\frac{count_i}{N}.\n",
        "$$\n",
        "Where $count_i$ is the number of occurencies of integer $i$ in the above interval, and thus $p_i$ is the estimated probability with which $i$ occurs in the array.\n",
        "\n",
        "For this example, assume $K$ is small, such as $K < 16$.\n",
        "<br>\n",
        "We can compute all $count_i$-s as an histogram (SCATTER-REDUCE) and then compute the Shannon entropy (MAP-REDUCE).\n",
        "<br>\n",
        "Specifically, using privatization for the histogram, threads map (scatter) each array value to its bin, then all private copies are reduced over into the global one.\n",
        "Then, a map-reduce can be performed to compute $H$'s sum, mapping each histogram counter to $p_i \\cdot log_2(p_i)$, with each thread accumulating a local partial sum, then all threads reducing to the final global sum for $H$.\n",
        "\n",
        "*Note: do not confuse the binning, with incremented counters, of the histogram, with the BIN pattern. The BIN patter starts by building an histogram as well, but then proceeds with a PACK by bucket.*"
      ],
      "metadata": {
        "id": "3tuL1rYfoKL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/map_reduce.cpp\n",
        "#include <algorithm>\n",
        "#include <cmath>\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "static constexpr std::size_t N = 50000;\n",
        "static constexpr std::size_t K = 12;\n",
        "\n",
        "int *data;\n",
        "std::size_t hist[K];\n",
        "\n",
        "// random integer generator in [0, K)\n",
        "int random_int() {\n",
        "  static std::mt19937 rng{2025};\n",
        "  std::uniform_int_distribution<int> dist(0, K - 1);\n",
        "  return dist(rng);\n",
        "}\n",
        "\n",
        "// HISTOGRAM with PRIVATIZATION and SCATTER-REDUCE\n",
        "void histogram_map_reduce(std::size_t hist[K], const int *data) {\n",
        "  for (std::size_t i = 0; i < K; ++i) hist[i] = 0;\n",
        "\n",
        "  #pragma omp parallel for reduction(+: hist[:K])\n",
        "  for (std::size_t i = 0; i < N; ++i) {\n",
        "    hist[data[i]]++;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ENTROPY with MAP-REDUCE\n",
        "double entropy_map_reduce(const std::size_t hist[K]) {\n",
        "  double Nf = double(N);\n",
        "  double H = 0.0;\n",
        "\n",
        "  #pragma omp parallel for reduction(+:H)\n",
        "  for (std::size_t k = 0; k < K; ++k) {\n",
        "    if (hist[k] == 0) continue;\n",
        "    double p = hist[k] / Nf;\n",
        "    H += -p * std::log2(p);\n",
        "  }\n",
        "\n",
        "  return H;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  data = new int[N];\n",
        "  std::generate(data, data + N, random_int);\n",
        "\n",
        "  // PHASE 1: HISTOGRAM\n",
        "  time(histogram_map_reduce, hist, data);\n",
        "\n",
        "  // PHASE 2: ENTROPY\n",
        "  double H = time(entropy_map_reduce, hist);\n",
        "\n",
        "  std::cout << \"Histogram: \";\n",
        "  for (std::size_t k = 0; k < K; ++k)\n",
        "    std::cout << hist[k] << \" \";\n",
        "  std::cout << \"\\nEntropy H = \" << H << \" bits\\n\";\n",
        "\n",
        "  delete[] data;\n",
        "  return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x70IDbYmoIzH",
        "outputId": "d42d039f-29ca-423e-965a-b6474533de65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/map_reduce.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "Botw_p3boKeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ map_reduce.cpp -fopenmp -o map_reduce\n",
        "!./map_reduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvQ_8SwroJkP",
        "outputId": "707b74db-605a-4b8c-ba56-ff121f37f7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ function histogram_map_reduce took 0ms ]\n",
            "[ function entropy_map_reduce took 0ms ]\n",
            "Histogram: 4011 4183 4138 4095 4180 4289 4147 4225 4174 4175 4242 4141 \n",
            "Entropy H = 3.58477 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Workpile**"
      ],
      "metadata": {
        "id": "lq5LODvF9bE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we generalize the map pattern to a scenario where the total number of tasks (applications of the function) is not known a priori.\n",
        "In particular, we consider a tree search:\n",
        "\n",
        "<img src=\"https://i.imgur.com/WHBRp2C.png\">\n",
        "\n",
        "This code first constructs a balanced binary tree with the values [1, 200].\n",
        "Then it searches a value in the tree.\n",
        "<br>\n",
        "The tree search is carried out by a finite team of threads via tasks.\n",
        "First, one thread searches the root.\n",
        "Then, as a thread does not find the value and needs to prosecute down multiple branches, its spawns a new task per-branch.\n",
        "This way tasks, representing yet-to-visit branches, build up a WORKPILE that threads progressively work through.\n",
        "\n",
        "Possible improvements:\n",
        "- as soon as a task finds the value, set a flag and dispose of all other tasks immediately.\n",
        "- build a binary search tree (BST) instead"
      ],
      "metadata": {
        "id": "cNV0O4yG9iJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/workpile.cpp\n",
        "#include <memory>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "\n",
        "struct node {\n",
        "  inline node(const int v) : value(v) {}\n",
        "  int value;\n",
        "  std::unique_ptr<node> left = nullptr;\n",
        "  std::unique_ptr<node> right = nullptr;\n",
        "};\n",
        "\n",
        "static void print_internal(std::unique_ptr<node> &root, std::ostream &out) {\n",
        "  if (root) {  // we have a valid tree\n",
        "    if (root->left) {\n",
        "      out << '\\t' << root->value << \" -> \" << root->left->value << std::endl;\n",
        "    }\n",
        "    if (root->right) {\n",
        "      out << '\\t' << root->value << \" -> \" << root->right->value << std::endl;\n",
        "    }\n",
        "    if (root->left || root->left) {  // we have some child\n",
        "      print_internal(root->left, out);\n",
        "      print_internal(root->right, out);\n",
        "    } else {  // we don't have any child\n",
        "      out << '\\t' << root->value << std::endl;\n",
        "    }\n",
        "  } else {  // this is an empty leaf\n",
        "  }\n",
        "}\n",
        "\n",
        "void print(std::unique_ptr<node> &root, std::ostream &out) {\n",
        "  out << \"digraph {\" << std::endl;\n",
        "  print_internal(root, out);\n",
        "  out << \"}\" << std::endl;\n",
        "}\n",
        "\n",
        "// functions to manipulate the tree\n",
        "\n",
        "static void insert(std::unique_ptr<node> &root, const int value) {\n",
        "  if (root) {  // we have a valid tree (continue to change it)\n",
        "    //if (value < root->value) { // this would yield a BST\n",
        "    if (std::rand() & 1) {\n",
        "      if (root->left) {\n",
        "        insert(root->left, value);\n",
        "      } else {\n",
        "        root->left = std::make_unique<node>(value);\n",
        "      }\n",
        "    } else if (value > root->value) {\n",
        "      if (root->right) {\n",
        "        insert(root->right, value);\n",
        "      } else {\n",
        "        root->right = std::make_unique<node>(value);\n",
        "      }\n",
        "    } else {\n",
        "      // we have a duplicate value, no need to do anything\n",
        "    }\n",
        "  } else {  // we don't have a valid tree as input (a new one)\n",
        "    root = std::make_unique<node>(value);\n",
        "  }\n",
        "}\n",
        "\n",
        "void populate(std::unique_ptr<node> &root, const int min_value, const int max_value) {\n",
        "  if (min_value < max_value) {  // we are building the tree branches\n",
        "    const int range = min_value + max_value;\n",
        "    const int middle_value = range / 2;\n",
        "    insert(root, middle_value);\n",
        "    populate(root, min_value, middle_value - 1);\n",
        "    populate(root, middle_value + 1, max_value);\n",
        "  } else if (min_value == max_value - 1) {  // leaf with two values\n",
        "    insert(root, min_value);\n",
        "    insert(root, max_value);\n",
        "  } else if (min_value == max_value) {  // leaf with just one value\n",
        "    insert(root, min_value);\n",
        "  }\n",
        "}\n",
        "\n",
        "// function applied over the tree\n",
        "\n",
        "std::string path_builder(const node* root, const int value, const std::string partial_result) {\n",
        "  if (root != nullptr) {\n",
        "    if (value == root->value) {  // we found the actual value\n",
        "      return partial_result + std::to_string(root->value);\n",
        "    } else {  // we need to look on both ways\n",
        "      std::string left, right;\n",
        "      if (root->left) {\n",
        "        #pragma omp task shared(left)\n",
        "        left = path_builder(root->left.get(), value, partial_result + std::to_string(root->value) + \" -> \");\n",
        "      }\n",
        "      if (root->right) {\n",
        "        #pragma omp task shared(right)\n",
        "        right = path_builder(root->right.get(), value, partial_result + std::to_string(root->value) + \" -> \");\n",
        "      }\n",
        "      #pragma omp taskwait\n",
        "      return !left.empty() ? left : right;\n",
        "    }\n",
        "  } else {\n",
        "    return \"\";\n",
        "  }\n",
        "}\n",
        "\n",
        "std::string pathfinder(const node* root, const int value) {\n",
        "  std::string result;\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    #pragma omp single\n",
        "    {\n",
        "      result = path_builder(root, value, std::string{});\n",
        "    }\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // initialize the tree\n",
        "  std::unique_ptr<node> tree;\n",
        "  populate(tree, 1, 200);\n",
        "  print(tree, std::cerr);\n",
        "\n",
        "  // find an element in the tree\n",
        "  const int target_value = 187;\n",
        "  std::cout << \"Path to \" << target_value << \": \" << pathfinder(tree.get(), target_value) << std::endl;\n",
        "\n",
        "  return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F8xf2wM9ie8",
        "outputId": "b61a538f-950c-452e-c9f5-bf2d7c538cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/workpile.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "QcYoN1x-9iu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ workpile.cpp -fopenmp -o workpile\n",
        "!./workpile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hixZe2fj9j1k",
        "outputId": "3b079c49-3839-4310-cd21-2d05e83bbc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "digraph {\n",
            "\t100 -> 50\n",
            "\t100 -> 125\n",
            "\t50 -> 12\n",
            "\t50 -> 62\n",
            "\t12 -> 13\n",
            "\t12 -> 45\n",
            "\t13 -> 20\n",
            "\t13 -> 21\n",
            "\t20 -> 60\n",
            "\t20 -> 61\n",
            "\t60\n",
            "\t61 -> 111\n",
            "\t61 -> 73\n",
            "\t111 -> 178\n",
            "\t111 -> 159\n",
            "\t178\n",
            "\t159\n",
            "\t73 -> 144\n",
            "\t73 -> 165\n",
            "\t144 -> 151\n",
            "\t144 -> 175\n",
            "\t151\n",
            "\t175\n",
            "\t165 -> 176\n",
            "\t165\n",
            "\t21 -> 34\n",
            "\t21 -> 66\n",
            "\t34 -> 123\n",
            "\t34 -> 35\n",
            "\t123 -> 171\n",
            "\t171\n",
            "\t35 -> 58\n",
            "\t35 -> 55\n",
            "\t58\n",
            "\t55\n",
            "\t66 -> 79\n",
            "\t66 -> 86\n",
            "\t79 -> 91\n",
            "\t79\n",
            "\t86\n",
            "\t45 -> 68\n",
            "\t45 -> 52\n",
            "\t68 -> 120\n",
            "\t68 -> 126\n",
            "\t120 -> 162\n",
            "\t162 -> 182\n",
            "\t162\n",
            "\t126\n",
            "\t52 -> 63\n",
            "\t52 -> 150\n",
            "\t63 -> 82\n",
            "\t63 -> 95\n",
            "\t82 -> 108\n",
            "\t82 -> 152\n",
            "\t108 -> 174\n",
            "\t174\n",
            "\t152\n",
            "\t95 -> 118\n",
            "\t95\n",
            "\t150 -> 141\n",
            "\t150 -> 157\n",
            "\t141 -> 177\n",
            "\t177\n",
            "\t157\n",
            "\t62 -> 71\n",
            "\t62 -> 85\n",
            "\t71 -> 77\n",
            "\t71 -> 74\n",
            "\t77 -> 84\n",
            "\t77 -> 80\n",
            "\t84 -> 135\n",
            "\t84 -> 112\n",
            "\t135\n",
            "\t112 -> 196\n",
            "\t196\n",
            "\t80 -> 153\n",
            "\t80 -> 192\n",
            "\t153 -> 164\n",
            "\t164\n",
            "\t192 -> 193\n",
            "\t192\n",
            "\t74 -> 92\n",
            "\t74 -> 110\n",
            "\t92 -> 170\n",
            "\t92 -> 185\n",
            "\t170\n",
            "\t185\n",
            "\t110 -> 131\n",
            "\t110\n",
            "\t85 -> 158\n",
            "\t85 -> 109\n",
            "\t158 -> 167\n",
            "\t167 -> 188\n",
            "\t188\n",
            "\t109 -> 121\n",
            "\t109 -> 124\n",
            "\t121 -> 154\n",
            "\t121 -> 140\n",
            "\t154\n",
            "\t140 -> 163\n",
            "\t163 -> 187\n",
            "\t163\n",
            "\t124 -> 143\n",
            "\t124\n",
            "\t125 -> 106\n",
            "\t125 -> 137\n",
            "\t106 -> 116\n",
            "\t106 -> 117\n",
            "\t116 -> 130\n",
            "\t116 -> 122\n",
            "\t130 -> 169\n",
            "\t130 -> 132\n",
            "\t169 -> 190\n",
            "\t190 -> 197\n",
            "\t197\n",
            "\t132\n",
            "\t122 -> 146\n",
            "\t122 -> 136\n",
            "\t146 -> 186\n",
            "\t186 -> 199\n",
            "\t186\n",
            "\t136 -> 195\n",
            "\t195\n",
            "\t117 -> 156\n",
            "\t117 -> 133\n",
            "\t156 -> 194\n",
            "\t194\n",
            "\t133 -> 173\n",
            "\t173 -> 198\n",
            "\t173\n",
            "\t137 -> 127\n",
            "\t137 -> 138\n",
            "\t127 -> 172\n",
            "\t127 -> 134\n",
            "\t172 -> 180\n",
            "\t172 -> 200\n",
            "\t180 -> 191\n",
            "\t180\n",
            "\t200\n",
            "\t134 -> 139\n",
            "\t134 -> 142\n",
            "\t139 -> 145\n",
            "\t139 -> 168\n",
            "\t145\n",
            "\t168\n",
            "\t142 -> 155\n",
            "\t155\n",
            "\t138 -> 148\n",
            "\t138 -> 149\n",
            "\t148 -> 184\n",
            "\t184 -> 183\n",
            "\t183\n",
            "\t149 -> 181\n",
            "\t149 -> 160\n",
            "\t181\n",
            "\t160 -> 161\n",
            "\t160 -> 166\n",
            "\t161 -> 179\n",
            "\t161\n",
            "\t166\n",
            "}\n",
            "Path to 187: 100 -> 50 -> 62 -> 85 -> 109 -> 121 -> 140 -> 163 -> 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scatter and Gather**"
      ],
      "metadata": {
        "id": "OLFlk37vB5ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a GATHER if not a MAP pattern whose function is a memory **read**.\n",
        "<br>\n",
        "What is a SCATTER if not a MAP pattern whose function is a memory **write** (with a side-dish of race conditions)."
      ],
      "metadata": {
        "id": "M-cEIb1uCwdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An obvious example: the histogram pattern with privatization! (yes, again XD)"
      ],
      "metadata": {
        "id": "5VScoUESEsdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/gather_scatter.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "  const int N = 10000;\n",
        "  const int K = 16;\n",
        "\n",
        "  std::vector<int> data(N);\n",
        "  std::vector<int> global_hist(K, 0);\n",
        "\n",
        "  // init. input\n",
        "  std::mt19937 gen(2024);\n",
        "  std::uniform_int_distribution<int> dist(0, K - 1);\n",
        "  for (int &v : data) v = dist(gen);\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    // thread-private histogram\n",
        "    std::vector<int> local_hist(K, 0);\n",
        "\n",
        "    // GATHER: each thread reads its part of the input\n",
        "    #pragma omp for\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      int v = data[i];\n",
        "      local_hist[v]++;\n",
        "    }\n",
        "\n",
        "    // SCATTER REDUCTION: merge private hists into the global one\n",
        "    for (int k = 0; k < K; ++k) {\n",
        "      #pragma opm atomic\n",
        "      global_hist[k] += local_hist[k];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  std::cout << \"Histogram counts:\\n\";\n",
        "  for (int k = 0; k < K; ++k)\n",
        "  std::cout << \"bin \" << k << \" : \" << global_hist[k] << \"\\n\";\n",
        "\n",
        "  return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD2nTuimCxKc",
        "outputId": "06dcb7d1-3afc-4b11-8487-fa4eedabb37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/gather_scatter.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "MlqLaZYxCwuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ gather_scatter.cpp -fopenmp -o gather_scatter\n",
        "!./gather_scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyW7y-b2CyeX",
        "outputId": "4be766e7-5ad6-4c8d-b1d7-01d40748d72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Histogram counts:\n",
            "bin 0 : 622\n",
            "bin 1 : 596\n",
            "bin 2 : 658\n",
            "bin 3 : 619\n",
            "bin 4 : 647\n",
            "bin 5 : 601\n",
            "bin 6 : 645\n",
            "bin 7 : 640\n",
            "bin 8 : 604\n",
            "bin 9 : 635\n",
            "bin 10 : 620\n",
            "bin 11 : 621\n",
            "bin 12 : 600\n",
            "bin 13 : 619\n",
            "bin 14 : 623\n",
            "bin 15 : 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Permute**"
      ],
      "metadata": {
        "id": "SELVL1FtttFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is constructing an array's permutation in parallel if not a SCATTER!\n",
        "<br>\n",
        "And not any SCATTER, if, by construction, there never is a collision, there are no data races to worry about.\n",
        "\n",
        "For an example of this, let's \"decrypt\" a message with a simple encoding:\n",
        "<br>\n",
        "Split the message into even-indexed and odd-indexed characters\n",
        "```\n",
        "even_part = message[0], message[2], message[4], …\n",
        "odd_part = message[1], message[3], message[5], …\n",
        "```\n",
        "then concatenate them\n",
        "```\n",
        "shuffled = even_part + odd_part\n",
        "```\n",
        "finally perform a cyclic left shift by K positions\n",
        "```\n",
        "scrambled[i] = shuffled[(i + k) mod N]\n",
        "```\n",
        "The results is a \"not very much readable\" a permutation of the original message.\n",
        "\n",
        "Decoding such messages naturally maps onto the PERMUTE (scatter) pattern!\n",
        "\n",
        "*Possible improvement: could do the un-rotate and un-concatenate in one single permute!*"
      ],
      "metadata": {
        "id": "3lwD3mdVtwm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/permute.cpp\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "// -----------------------------------------------------------\n",
        "// ENCODING (done offline):\n",
        "//   1) split message into even-index chars and odd-index chars\n",
        "//   2) concatenate evens first, then odd ones ==> interleave shuffle\n",
        "//   3) left-rotate the resulting string by K positions\n",
        "//\n",
        "// DECODING (done here):\n",
        "//   1) right-rotate the input string by K\n",
        "//   2) SCATTER each character to its original index\n",
        "// -----------------------------------------------------------\n",
        "\n",
        "std::string rotate_right(const std::string& s, int k) {\n",
        "  int n = s.size();\n",
        "  k = ((k % n) + n) % n;\n",
        "  return s.substr(n - k) + s.substr(0, n - k);\n",
        "}\n",
        "\n",
        "std::string unscramble_interleave(const std::string& enc) {\n",
        "  int n = enc.size();\n",
        "  std::string out(n, '?');\n",
        "\n",
        "  int num_evens = (n + 1) / 2; // ceil(n/2)\n",
        "\n",
        "  // PERMUTE (SCATTER)\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    if (i < num_evens) { // even pos\n",
        "      int orig = 2 * i;\n",
        "      out[orig] = enc[i];\n",
        "    } else { // odd pos\n",
        "      int j = i - num_evens;\n",
        "      int orig = 2 * j + 1;\n",
        "      out[orig] = enc[i];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  return out;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  std::string original = \"OPENMP MAKES ME FEEL LIKE I HAVE CLONES\";\n",
        "\n",
        "  int k = 7;\n",
        "\n",
        "  // encoding (done sequentially)\n",
        "  std::string enc1;\n",
        "  std::string ev, od;\n",
        "  for (int i = 0; i < (int)original.size(); ++i)\n",
        "      (i % 2 == 0 ? ev : od).push_back(original[i]);\n",
        "  enc1 = ev + od;\n",
        "  std::string scrambled = enc1.substr(k) + enc1.substr(0, k);\n",
        "\n",
        "  std::cout << \"Scrambled message:\\n\" << scrambled << \"\\n\\n\";\n",
        "\n",
        "  // decode\n",
        "  std::string rotated_back = rotate_right(scrambled, k);\n",
        "  std::string recovered = time(unscramble_interleave, rotated_back);\n",
        "\n",
        "  std::cout << \"Recovered message:\\n\" << recovered << \"\\n\";\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSjEm9_ctw3f",
        "outputId": "24214818-492a-4440-b4c4-f382f61b2bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/permute.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "dmQXVNeRtxSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ permute.cpp -fopenmp -o permute\n",
        "!./permute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KKfFUjVtxnI",
        "outputId": "7a573783-622d-4931-fe21-203ba61643fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scrambled message:\n",
            "EFE IEIHV LNSPNPMKSM ELLK  AECOEOEM AE \n",
            "\n",
            "[ function unscramble_interleave took 0ms ]\n",
            "Recovered message:\n",
            "OPENMP MAKES ME FEEL LIKE I HAVE CLONES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gather and Scatter - a practical use case**"
      ],
      "metadata": {
        "id": "CdXuPYMmhlL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a classic, slightly simplified, computer-vision pipeline for edge-detection consisting of two phases:\n",
        "\n",
        "1) Gradient magnitude computation.\n",
        "Each pixel reads its neighbors to compute local intensity changes. <br>GATHER: a single output location gathers multiple input values.\n",
        "It is fully parallel and conflict-free.\n",
        "Could also be seen as a 4-point STENCIL.\n",
        "\n",
        "2) [Hough transform](https://en.wikipedia.org/wiki/Hough_transform) line voting.\n",
        "Each detected edge pixel casts multiple \"votes\" into an accumulator array,\n",
        "marking all lines that could pass through it. <br>SCATTER: one input produces many outputs, and many inputs write to the same accumulator cells.\n",
        "Requires atomic updates or privatization.\n",
        "\n",
        "Variations of this pipeline come up often in computer vision, doing something like:\n",
        "<br>\n",
        "input image -> edge map -> line accumulator -> line detection"
      ],
      "metadata": {
        "id": "j8ewkTfmhvQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/edge_detection.cpp\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "// gradient magnitude (GATHER): each output pixel reads neighbors to estimate an edge strength\n",
        "void compute_gradient(const std::vector<float> &img, int H, int W, std::vector<float> &grad) {\n",
        "  #pragma omp parallel for collapse(2)\n",
        "  for (int i = 1; i < H - 1; i++) {\n",
        "    for (int j = 1; j < W - 1; j++) {\n",
        "      // 4-point stencil\n",
        "      float gx = img[i * W + (j + 1)] - img[i * W + (j - 1)];\n",
        "      float gy = img[(i + 1) * W + j] - img[(i - 1) * W + j];\n",
        "      grad[i * W + j] = std::sqrt(gx * gx + gy * gy);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// Hough voting (SCATTER): each strong edge pixel scatters votes into a global accumulator\n",
        "void hough_transform(const std::vector<float> &grad, int H, int W, std::vector<int> &acc, int num_rho, int max_rho, int num_theta) {\n",
        "  float dtheta = M_PI / num_theta;\n",
        "\n",
        "  #pragma omp parallel for collapse(2)\n",
        "  for (int i = 0; i < H; i++) {\n",
        "    for (int j = 0; j < W; j++) {\n",
        "\n",
        "      if (grad[i * W + j] < 50.0f)\n",
        "        continue; // ignore weak pixels\n",
        "\n",
        "      // scatter votes\n",
        "      for (int t = 0; t < num_theta; t++) {\n",
        "        float theta = t * dtheta;\n",
        "        float rho = j * std::cos(theta) + i * std::sin(theta);\n",
        "        int r = int(rho + max_rho);\n",
        "\n",
        "        #pragma omp atomic\n",
        "        acc[t * num_rho + r]++;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int H = 128, W = 128;\n",
        "\n",
        "  std::vector<float> img(H * W, 0.0f);\n",
        "  for (int i = 0; i < H; i++)\n",
        "    img[i * W + i] = 255.0f; // bright diagonal\n",
        "\n",
        "  std::cout << \"Image:\\n\";\n",
        "  for (int i = 0; i < H && i < 20; i++) {\n",
        "    for (int j = 0; j < W && j < 20; j++)\n",
        "      std::cout << std::setw(5) << std::fixed << std::setprecision(2) << std::right << img[i*W + j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "\n",
        "  std::vector<float> grad(H * W, 0.0f);\n",
        "  time(compute_gradient, img, H, W, grad);\n",
        "\n",
        "  float max_rho = std::sqrt(H * H + W * W);\n",
        "  int num_rho = 2 * int(max_rho);\n",
        "  const int num_theta = 180;\n",
        "  std::vector<int> acc;\n",
        "  acc.assign(num_theta * num_rho, 0);\n",
        "  time(hough_transform, grad, H, W, acc, num_rho, max_rho, num_theta);\n",
        "\n",
        "  std::cout << \"Votes:\\n\";\n",
        "  for (int i = 0; i < num_rho && i < 20; i++) {\n",
        "    for (int j = 0; j < num_theta && j < 20; j++)\n",
        "      std::cout << std::setw(5) << std::fixed << std::setprecision(2) << std::right << acc[i*num_theta + j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaO9vFlFhvj-",
        "outputId": "a2528b96-d16b-44c2-ae13-8f9ef77c6d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/edge_detection.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "MVeY0e-yhvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ edge_detection.cpp -fopenmp -o edge_detection\n",
        "!./edge_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EMfwcRchwjF",
        "outputId": "2d09330d-bfb7-4a63-a32e-2192630e60f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image:\n",
            "255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 255.00 \n",
            "[ function compute_gradient took 0ms ]\n",
            "[ function hough_transform took 1ms ]\n",
            "Votes:\n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     1     2     2     2     2     2     2     2     2     2     2     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     1     2     2     2     2     2     2     2     2     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     1     2     2     2     2     2     2     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     1     2     2     2     2     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     1     2     2     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     1     2     2     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     1     2     2     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     1     2     2     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     1     2 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What does this transform actually do?*\n",
        "\n",
        "The goal of the Hough transform is to detect straight lines in an image.\n",
        "A line can be represented in \"normal form\":\n",
        "$$\n",
        "\\rho = x \\cdot cos\\theta + y \\cdot sin\\theta\n",
        "$$\n",
        "Where $\\theta$ is the angle of the line's normal (0 to $\\pi$), $\\rho$ is the perpendicular distance from the origin to the line, and $x, y$ the pixel coordinates.\n",
        "\n",
        "From this, each line in the image corresponds to one point in the ($\\theta$, $\\rho$) space.\n",
        "\n",
        "In practice, the code, for each strong (high-gradient) edge pixel at ($x$, $y$),loops over all possible $\\theta$s, e.g., 0°, 1°, 2°, ... 179°.\n",
        "<br>\n",
        "For each it computes $\\rho = x \\cdot cos\\theta + y \\cdot sin\\theta$ and goes to increment the ($\\theta$, $\\rho$) accumulator.\n",
        "\n",
        "\n",
        "*How does this detect lines?*\n",
        "\n",
        "Consider all edge pixels on a real line L in the image.\n",
        "For the $\\theta$ corresponding to L's orientation:\n",
        "- their computed $\\rho$ values will be the same\n",
        "- they all vote into the same accumulator\n",
        "\n",
        "Hence, that accumulator becomes larger than the rest, with such a peak revealing the presence (and parameters) of the line.\n",
        "\n",
        "<br>\n",
        "\n",
        "For us:\n",
        "- GATHER for the gradients;\n",
        "- SCATTER from image-space ($x$, $y$) to ($\\theta$, $\\rho$) space;"
      ],
      "metadata": {
        "id": "-wCyLFPPmDs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Zip and Permute**"
      ],
      "metadata": {
        "id": "g3H6zk5BMJW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given three same-length vectors of integers (SoA form) we want to convert it into a single contigous array with one tripled of values after the other, each tripled having a value per same index position in the original array.\n",
        "Then, XOR each value with a given key and internally sort each triplet.\n",
        "\n",
        "We start with the three arrays of equal length, our job is to look at each group of three elements in the same position in the three arrays as a single tuple.\n",
        "Therefore, we ZIP (GATHER) the arrays into one, alternatingly picking elements from the first, second, and third array.\n",
        "<br>\n",
        "Subsequently, after each element is XORed with the fixed integer (MAP), we need to ensure that each tuple of three elements that we zipped together is internally sorted (lowest element comes first). This is done by a PERMUTE (SCATTER).\n",
        "\n",
        "In short:\n",
        "<br>\n",
        "ZIP (into tuples) -> MAP (xor) -> PERMUTE (sort each tuple)"
      ],
      "metadata": {
        "id": "lkoC1GGQM-Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/zip_permute.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "int main() {\n",
        "  const int N = 20;\n",
        "  const int XOR_VALUE = 0x5A5A;\n",
        "\n",
        "  // init. input arrays\n",
        "  std::vector<int> A(N), B(N), C(N);\n",
        "\n",
        "  std::mt19937 gen(42);\n",
        "  std::uniform_int_distribution<int> dist(1, 100);\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    A[i] = dist(gen);\n",
        "    B[i] = dist(gen);\n",
        "    C[i] = dist(gen);\n",
        "  }\n",
        "\n",
        "  // zipped array of tuples, Z[3*i] = A[i], Z[3*i + 1] = B[i], Z[3*i + 2] = C[i]\n",
        "  std::vector<int> Zipped(3 * N);\n",
        "\n",
        "  // ZIP (GATHER)\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    Zipped[3*i] = A[i];\n",
        "    Zipped[3*i + 1] = B[i];\n",
        "    Zipped[3*i + 2] = C[i];\n",
        "  }\n",
        "\n",
        "  // MAP: XOR each element in the zipped array\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < 3*N; ++i) {\n",
        "    Zipped[i] ^= XOR_VALUE;\n",
        "  }\n",
        "\n",
        "  // PERMUTE (SCATTER) each tuple\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    int a = Zipped[3*i];\n",
        "    int b = Zipped[3*i + 1];\n",
        "    int c = Zipped[3*i + 2];\n",
        "\n",
        "    // scatter permutation into correct locations (manual binary comparisons tree)\n",
        "    if (b < a) {\n",
        "        if (c < b) {\n",
        "            Zipped[3*i] = c; Zipped[3*i + 1] = b; Zipped[3*i + 2] = a;\n",
        "        } else if (c < a) {\n",
        "            Zipped[3*i] = b; Zipped[3*i + 1] = c; Zipped[3*i + 2] = a;\n",
        "        } else {\n",
        "            Zipped[3*i] = b; Zipped[3*i + 1] = a; Zipped[3*i + 2] = c;\n",
        "        }\n",
        "    } else {\n",
        "        if (c < a) {\n",
        "            Zipped[3*i] = c; Zipped[3*i + 1] = a; Zipped[3*i + 2] = b;\n",
        "        } else if (c < b) {\n",
        "            Zipped[3*i] = a; Zipped[3*i + 1] = c; Zipped[3*i + 2] = b;\n",
        "        } else {\n",
        "            Zipped[3*i] = a; Zipped[3*i + 1] = b; Zipped[3*i + 2] = c;\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  std::cout << \"Final permuted tuples:\\n\";\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    std::cout << \"(\" << Zipped[3*i] << \", \" << Zipped[3*i+1] << \", \" << Zipped[3*i+2] << \") \";\n",
        "  }\n",
        "  std::cout << \"\\n\";\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3SKkAsBM-a7",
        "outputId": "531ecbdd-2e9f-4280-db77-ed4ab03103c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/zip_permute.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "b29MnPrGM-mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ zip_permute.cpp -fopenmp -o zip_permute\n",
        "!./zip_permute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N6v7p9bM-zo",
        "outputId": "b7df8baa-fedb-46ca-bcd0-59ad86da251a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final permuted tuples:\n",
            "(23050, 23098, 23164) (23056, 23060, 23113) (23114, 23142, 23142) (23114, 23120, 23159) (23053, 23132, 23156) (23125, 23143, 23160) (23064, 23069, 23129) (23059, 23099, 23132) (23044, 23054, 23116) (23102, 23113, 23131) (23109, 23113, 23140) (23131, 23140, 23151) (23108, 23129, 23158) (23140, 23151, 23154) (23108, 23124, 23135) (23096, 23106, 23167) (23061, 23120, 23156) (23118, 23140, 23165) (23097, 23142, 23150) (23052, 23135, 23157) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scan**"
      ],
      "metadata": {
        "id": "zHsMPZZ2SJsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the classic prefix sums or SCAN.\n",
        "<br>\n",
        "Implemented here in the classical Blelloch fashion and adapted to OpenMP with explicit tiles."
      ],
      "metadata": {
        "id": "0gutRmxOTdrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/scan.cpp\n",
        "#include <omp.h>\n",
        "#include <algorithm>\n",
        "#include <array>\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <vector>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "// data structures\n",
        "static constexpr auto vector_size = std::size_t{100000};\n",
        "std::array<int, vector_size> x, cumulative;\n",
        "\n",
        "// random generator for initialization\n",
        "int random_int() {\n",
        "  static std::mt19937 random_generator{2023};\n",
        "  std::uniform_int_distribution<int> interval(0, 100);\n",
        "  return interval(random_generator);\n",
        "}\n",
        "\n",
        "// computation function -> scan (inclusive, out-of-place)\n",
        "void kernel(std::array<int, vector_size> &cumulative, const std::array<int, vector_size> &x) {\n",
        "  static_assert(vector_size > std::size_t{0});\n",
        "  cumulative[0] = x[0];\n",
        "\n",
        "  // temporary array to store intermediate results\n",
        "  std::vector<int> intermediate_results;\n",
        "\n",
        "  #pragma omp parallel\n",
        "    {\n",
        "    // identify the tile size (elements per thread)\n",
        "    const std::size_t num_tiles = static_cast<std::size_t>(::omp_get_num_threads());\n",
        "    const std::size_t tile_size = (vector_size - std::size_t{1}) / num_tiles;\n",
        "\n",
        "    // -> allocate the intermeadiate results array\n",
        "    // intermediate_results[i] = prefix sum up to, and exclusing, tile \"i\"\n",
        "    #pragma omp single\n",
        "    {\n",
        "      intermediate_results.resize(num_tiles, int{0});\n",
        "      intermediate_results[0] = x[0]; // hard-coded\n",
        "    }\n",
        "\n",
        "    // -> reduction phase of the algorithm\n",
        "    // reduce sequentially inside each tile, and in parallel among them\n",
        "    #pragma omp for\n",
        "    for (std::size_t tile_index = 1; tile_index < num_tiles; ++tile_index) {\n",
        "      const std::size_t start_index = (tile_index - std::size_t{1}) * tile_size + std::size_t{1};\n",
        "      const std::size_t stop_index = start_index + tile_size;\n",
        "      for (std::size_t i = start_index; i < stop_index; ++i) {\n",
        "        intermediate_results[tile_index] += x[i];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // -> sequential scan among tiles reduction results\n",
        "    #pragma omp single\n",
        "    {\n",
        "      for (std::size_t i = std::size_t{1}; i < num_tiles; ++i) {\n",
        "        intermediate_results[i] += intermediate_results[i - std::size_t{1}];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // -> perform the \"missing operations\" phase of the algorithm\n",
        "    // prefix sum sequentially inside each tile, and in parallel among them\n",
        "    // add the reduced sum from the tile before yours at the start\n",
        "    #pragma omp for\n",
        "    for (std::size_t tile_index = 0; tile_index < num_tiles; ++tile_index) {\n",
        "      const std::size_t start_index = (tile_index * tile_size) + std::size_t{1};\n",
        "      const std::size_t stop_index = start_index + tile_size;\n",
        "      cumulative[start_index] = intermediate_results[tile_index] + x[start_index];\n",
        "      for (std::size_t i = start_index + std::size_t{1}; i < stop_index; ++i) {\n",
        "        cumulative[i] = cumulative[i - std::size_t{1}] + x[i];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // take care of the leftovers\n",
        "    const std::size_t start_index = num_tiles * tile_size + std::size_t{1};\n",
        "    #pragma omp single\n",
        "    for (std::size_t i = start_index; i < vector_size; ++i) {\n",
        "      cumulative[i] = cumulative[i - std::size_t{1}] + x[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // initialize the input data\n",
        "  std::generate(std::begin(x), std::end(x), random_int);\n",
        "  std::fill(std::begin(cumulative), std::end(cumulative), int{0});\n",
        "\n",
        "  // perform the computation\n",
        "  kernel(cumulative, x);\n",
        "\n",
        "  // print the last ten values\n",
        "  std::cout << \"cumulative = {\";\n",
        "  if (vector_size > std::size_t{11}) {\n",
        "    std::cout << \"...\";\n",
        "    for (std::size_t i = vector_size - 11; i < vector_size; ++i) {\n",
        "      std::cout << \", \" << cumulative[i];\n",
        "    }\n",
        "  } else {\n",
        "    std::cout << cumulative[0];\n",
        "    for (std::size_t i = 1; i < vector_size; ++i) {\n",
        "      std::cout << \", \" << cumulative[i];\n",
        "    }\n",
        "  }\n",
        "  std::cout << \"}\" << std::endl;\n",
        "\n",
        "  return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BkUCt3ATd4P",
        "outputId": "c6794127-29f8-4b92-b3f6-27caa96a7cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/scan.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "JD69JNlXTeFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ scan.cpp -fopenmp -o scan\n",
        "!./scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFXa3XljTec5",
        "outputId": "d98e8b88-f62a-46e8-a54a-8b15c6d8d9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cumulative = {..., 5007296, 5007342, 5007380, 5007462, 5007491, 5007564, 5007596, 5007679, 5007718, 5007742, 5007759}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Counting Sort**"
      ],
      "metadata": {
        "id": "BOJ_-nXTP_eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another use of our good old friend the histogram.\n",
        "\n",
        "Non-comparative sorting algorithms don't have the \"n*logn\" limit, but need to leverage some assumption on the input. In this case, we need to sort an array containing only numbers in $\\{0, \\dots 99\\}$.\n",
        "\n",
        "GATHER for the histogram -> SCAN over bins to get output offsets -> SCATTER to permute the input into the output\n",
        "\n",
        "This sequence of GATHER->SCAN->SCATTER is the **BIN** parallel pattern!\n",
        "<br>\n",
        "The BIN parallel pattern is a two-phase counting-and-placement operation over a collection of data elements, where:\n",
        "- first, each element is mapped to a discrete category (bin)\n",
        "- then, all elements belonging to the same category are grouped together in the output\n",
        "\n",
        "*Note: SCATTER race conditions prevented by keeping an atomically incremented counter per BIN indicating where to insert its next element.*"
      ],
      "metadata": {
        "id": "VeEvkG9-QCyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/counting_sort.cpp\n",
        "#include <vector>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <algorithm>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "static constexpr int N = 40; // input length\n",
        "static constexpr int K = 100; // input integers are in [0, K - 1]\n",
        "\n",
        "void counting_sort(const std::vector<int> &input, std::vector<int> &output) {\n",
        "  const size_t N = input.size();\n",
        "  output.resize(N);\n",
        "\n",
        "  // global histogram\n",
        "  std::vector<int> global_bins(K, 0);\n",
        "\n",
        "  // SCATTER\n",
        "  // count the occurrencies of each value\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    // thread-private histogram\n",
        "    std::vector<int> local_bins(K, 0);\n",
        "\n",
        "    #pragma omp for\n",
        "    for (size_t i = 0; i < N; ++i) {\n",
        "      int v = input[i];\n",
        "      ++local_bins[v];\n",
        "    }\n",
        "\n",
        "    // a tiny SCATTER to finish the historam\n",
        "    for (int k = 0; k < K; ++k) {\n",
        "      #pragma omp atomic\n",
        "      global_bins[k] += local_bins[k];\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // SCAN (prefix sum)\n",
        "  // compute the starting idx of each distinct value\n",
        "  // note: done serially here, to keep the code clean,\n",
        "  //       there is parallel example above anyway\n",
        "  std::vector<int> offsets(K);\n",
        "  int running = 0;\n",
        "  for (int k = 0; k < K; ++k) {\n",
        "    offsets[k] = running;\n",
        "    running += global_bins[k];\n",
        "  }\n",
        "\n",
        "  std::vector<int> global_pos = offsets;\n",
        "\n",
        "  // SCATTER\n",
        "  // place each value in its final position\n",
        "  #pragma omp parallel for\n",
        "  for (size_t i = 0; i < N; ++i) {\n",
        "    int v = input[i];\n",
        "    // same as atomic \"int pos = global_pos[v]++;\"\n",
        "    // practically: reserve the next position in the bin\n",
        "    int pos = __atomic_fetch_add(&global_pos[v], 1, __ATOMIC_RELAXED);\n",
        "    output[pos] = v;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  std::mt19937 rng(2025);\n",
        "  std::uniform_int_distribution<int> dist(0, 99);\n",
        "\n",
        "  std::vector<int> input(N);\n",
        "  for (auto &x : input)\n",
        "  x = dist(rng);\n",
        "\n",
        "  std::vector<int> sorted;\n",
        "  time(counting_sort, input, sorted);\n",
        "\n",
        "  std::cout << \"Input: \";\n",
        "  for (int v : input) std::cout << v << \" \";\n",
        "  std::cout << \"\\n\";\n",
        "\n",
        "  std::cout << \"Sorted: \";\n",
        "  for (int v : sorted) std::cout << v << \" \";\n",
        "  std::cout << \"\\n\";\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxFM4uazQDFB",
        "outputId": "4d910eb1-e8f9-487b-dfc3-25642c9ffcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /home/OpenMP/counting_sort.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "27nZFm0dQDRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ counting_sort.cpp -fopenmp -o counting_sort\n",
        "!./counting_sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctOVhW0AQDfR",
        "outputId": "9991acd5-9f02-4cbf-b583-f086bc60b525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ function counting_sort took 9ms ]\n",
            "Input: 13 49 88 34 93 10 44 64 38 92 25 28 65 78 49 12 96 88 80 38 45 15 80 37 4 52 76 36 0 8 29 33 61 73 91 7 30 7 24 19 \n",
            "Sorted: 0 4 7 7 8 10 12 13 15 19 24 25 28 29 30 33 34 36 37 38 38 44 45 49 49 52 61 64 65 73 76 78 80 80 88 88 91 92 93 96 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Radix Sort**"
      ],
      "metadata": {
        "id": "SCUk7-QeEXiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take counting sort up a notch and remove its fatal flaw of depending on the number of distinct entries in the array by determining ourself what those distinct entries are.\n",
        "\n",
        "The idea behind radix sort is to only look at the lowest R of bits of each value, sort w.r.t. them, by using them as bins for counting sort, and then repeat the process for the next R lowest bit. If the sorting algorithm used underneath is **stable** (preserves relative order of equivalent elements), iterating leads to a final sorted array. All while dealing with exactly $2^{\\text{R}}$ distinct elements and no more at each round.\n",
        "\n",
        "For each round, the pattern is the same as counting sort, that is, the BIN:\n",
        "<br>\n",
        "GATHER for the radix histogram -> SCAN for offsets -> SCATTER to sort -> repeat after shifting the bitmask\n",
        "<br>\n",
        "From the above, we require this BIN pattern to be stable.\n",
        "\n",
        "*Note: if R is 1, thus we sort by just one bit at a time, the BIN collapses into the SPLIT pattern!*"
      ],
      "metadata": {
        "id": "hEP7d5xSEbB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/radix_sort.cpp\n",
        "#include <vector>\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <algorithm>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "static constexpr int N = 40; // input length\n",
        "\n",
        "constexpr int RADIX_BITS = 4; // setting to 1 collapses the BIN pattern in to a SPLIT\n",
        "constexpr int RADIX = 1 << RADIX_BITS; // count of possible distinct within the radix\n",
        "constexpr unsigned int MASK = RADIX - 1; // 0x00..FF\n",
        "\n",
        "void radix_sort(std::vector<unsigned>& a) {\n",
        "  const int n = a.size();\n",
        "  // ping-pong copy of the array\n",
        "  std::vector<unsigned> tmp(n);\n",
        "\n",
        "  int num_threads = omp_get_max_threads();\n",
        "  const int passes = (32 + RADIX_BITS - 1) / RADIX_BITS;\n",
        "\n",
        "  // global histogram\n",
        "  std::vector<int> global_bins(RADIX, 0);\n",
        "  // bins[thread][bucket] -> local histogram copies per thread, one bin per different radix\n",
        "  std::vector<std::vector<int>> bins(num_threads, std::vector<int>(RADIX));\n",
        "  // thread_offsets[thread][bucket]\n",
        "  std::vector<std::vector<int>> thread_offsets(num_threads, std::vector<int>(RADIX));\n",
        "  // offsets[b] -> starting index where to store the content of bucket b\n",
        "  std::vector<int> offsets(RADIX, 0);\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    int tid = omp_get_thread_num();\n",
        "    for (int pass = 0; pass < passes; pass++) {\n",
        "      unsigned shift = pass * RADIX_BITS;\n",
        "\n",
        "      std::fill(global_bins.begin(), global_bins.end(), 0);\n",
        "      std::fill(bins[tid].begin(), bins[tid].end(), 0);\n",
        "      std::fill(offsets.begin(), offsets.end(), 0);\n",
        "\n",
        "      // classic parallel histogram: each threed sees a part of the array\n",
        "      #pragma omp for schedule(static)\n",
        "      for (int i = 0; i < n; i++) {\n",
        "        unsigned digit = (a[i] >> shift) & MASK;\n",
        "        bins[tid][digit]++;\n",
        "      }\n",
        "\n",
        "      // one thread merges the local copies and builds global offsets\n",
        "      #pragma omp single\n",
        "      {\n",
        "        // merge private histograms\n",
        "        // NOTE: as alwasy, could be done in parallel with REDUCE or ATOMICS\n",
        "        for (int t = 0; t < num_threads; t++)\n",
        "          for (int b = 0; b < RADIX; b++)\n",
        "            global_bins[b] += bins[t][b];\n",
        "\n",
        "        // compute where each bin can be stored from its size\n",
        "        // NOTE: could be done with a parallel SCAN\n",
        "        for (int b = 1; b < RADIX; b++)\n",
        "          offsets[b] = offsets[b - 1] + global_bins[b - 1];\n",
        "\n",
        "        // assign to each thread the offset for its bins w.r.t. the global offset of each bin\n",
        "        for (int b = 0; b < RADIX; b++) {\n",
        "          int offset = offsets[b];\n",
        "          for (int t = 0; t < num_threads; t++) {\n",
        "            thread_offsets[t][b] = offset;\n",
        "            offset += bins[t][b];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "\n",
        "      // SCATTER from each thread's portion of the array to their computed offsets\n",
        "      #pragma omp for schedule(static)\n",
        "      for (int i = 0; i < n; i++) {\n",
        "        unsigned digit = (a[i] >> shift) & MASK;\n",
        "        int pos = thread_offsets[tid][digit]++;\n",
        "        tmp[pos] = a[i];\n",
        "      }\n",
        "\n",
        "      // copy back the written buffer over the original one\n",
        "      #pragma omp for schedule(static)\n",
        "      for (int i = 0; i < n; i++)\n",
        "        a[i] = tmp[i];\n",
        "\n",
        "      #pragma omp barrier\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  std::mt19937 rng(2025);\n",
        "  std::uniform_int_distribution<unsigned> dist(0, 1 << 24);\n",
        "\n",
        "  std::vector<unsigned> input(N);\n",
        "  for (auto &x : input)\n",
        "  x = dist(rng);\n",
        "\n",
        "  time(radix_sort, input);\n",
        "\n",
        "  std::cout << \"Input: \";\n",
        "  for (unsigned v : input) std::cout << v << \" \";\n",
        "  std::cout << \"\\n\";\n",
        "\n",
        "  std::cout << \"Sorted: \";\n",
        "  for (unsigned v : input) std::cout << v << \" \";\n",
        "  std::cout << \"\\n\";\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1MJLx55EbSU",
        "outputId": "6218a04e-1137-4f75-80d7-d974d3782df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/radix_sort.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "Sd0Ax_LeEb1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ radix_sort.cpp -fopenmp -o radix_sort\n",
        "!./radix_sort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkA6KStyEcJW",
        "outputId": "19a04531-6a6d-45b4-d80f-7c4a3a10ed0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ function radix_sort took 0ms ]\n",
            "Input: 53202 699911 1267256 1312747 1444825 1770878 2120783 2273114 2590756 3319207 4170793 4321751 4815406 4912526 5035094 5669727 5727787 6093619 6348579 6379525 6513511 7475393 7637077 8264741 8384383 8866734 10249442 10864917 11028798 12300798 12909361 13132676 13438290 13439525 14798413 14895680 15318058 15573585 15646527 16177237 \n",
            "Sorted: 53202 699911 1267256 1312747 1444825 1770878 2120783 2273114 2590756 3319207 4170793 4321751 4815406 4912526 5035094 5669727 5727787 6093619 6348579 6379525 6513511 7475393 7637077 8264741 8384383 8866734 10249442 10864917 11028798 12300798 12909361 13132676 13438290 13439525 14798413 14895680 15318058 15573585 15646527 16177237 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filter and Pack**"
      ],
      "metadata": {
        "id": "8KLXCFxzpub9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given an array of integers.\n",
        "<br>\n",
        "You need to construct an array containing only those integers that are divisible by 2 in the original.\n",
        "Crucially, the relative order of array elements must be preserved.\n",
        "\n",
        "The slow way to do this: go in parallel over the array and append survivors to a thread-private list. Then, in increasing order of thread-id, concatenate the private lists into the final one.\n",
        "\n",
        "The fast way to do this: parallel MAP to filter the array, counting how many \"to keep\" ones each thread has seen, and replacing others with -1. Parallel SCAN to provide each thread with the number of elements that were kept before his own (essentially, the offset where to write its results). Parallel PACK to put together all values.\n",
        "\n",
        "*Note: it is often the case for PACK to rely on SCAN (exclusive) to compute offsets and coordinate writes!*"
      ],
      "metadata": {
        "id": "NWd8rhrup3dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/filter_pack.cpp\n",
        "#include <vector>\n",
        "#include <iostream>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 1000000\n",
        "\n",
        "int main() {\n",
        "  std::vector<int> in(N);\n",
        "  for (int i = 0; i < N; i++)\n",
        "    in[i] = i;\n",
        "\n",
        "  std::vector<int> out;\n",
        "\n",
        "  // per-thread counts and offsets (built from counts after the SCAN)\n",
        "  std::vector<int> counts;\n",
        "  std::vector<int> offsets;\n",
        "\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    int tid = omp_get_thread_num();\n",
        "    int nth = omp_get_num_threads();\n",
        "\n",
        "    #pragma omp single\n",
        "    {\n",
        "      counts.resize(nth);\n",
        "      offsets.resize(nth);\n",
        "    }\n",
        "\n",
        "    // manual work allocation between threads\n",
        "    int chunk = (int)((N + nth - 1) / nth);\n",
        "    int begin = tid * chunk;\n",
        "    int end = std::min(N, begin + chunk);\n",
        "\n",
        "    // MAP\n",
        "    int local_keep = 0;\n",
        "    for (int i = begin; i < end; i++) {\n",
        "      if (in[i] % 2 == 0)\n",
        "        local_keep++;\n",
        "      else\n",
        "        in[i] = -1;\n",
        "    }\n",
        "    counts[tid] = local_keep;\n",
        "\n",
        "    #pragma omp barrier\n",
        "\n",
        "    // INCLUSIVE SCAN (PREFIX-SUM)\n",
        "    // NOTE: to keep the code brief, this is a work-inefficient (n^2) version with a ton of barriers!\n",
        "    for (int step = 1; step < nth; step *= 2) {\n",
        "      int add = 0;\n",
        "      if (tid >= step)\n",
        "        add = counts[tid - step];\n",
        "\n",
        "      #pragma omp barrier\n",
        "\n",
        "      offsets[tid] = counts[tid] + add;\n",
        "\n",
        "      #pragma omp barrier\n",
        "\n",
        "      counts[tid] = offsets[tid];\n",
        "    }\n",
        "\n",
        "    #pragma omp barrier\n",
        "\n",
        "    // make the SCAN EXCLUSIVE\n",
        "    offsets[tid] -= local_keep;\n",
        "\n",
        "    // let the last thread resize the output\n",
        "    if (tid == nth - 1)\n",
        "      out.resize(offsets[tid] + local_keep);\n",
        "\n",
        "    #pragma omp barrier\n",
        "\n",
        "    // PACK\n",
        "    int outpos = offsets[tid];\n",
        "    for (int i = begin; i < end; i++) {\n",
        "      if (in[i] != -1) {\n",
        "        out[outpos++] = in[i];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  std::cout << \"Filtered values: \";\n",
        "  for (int i = 0; i < N && i < 20; i++)\n",
        "    std::cout << out[i] << \" \";\n",
        "  std::cout << \"\\n\";\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNoiUwUDp3om",
        "outputId": "2d30d157-a408-480f-dbff-c076056428f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/filter_pack.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "kmuqJCGFp3zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ filter_pack.cpp -fopenmp -o filter_pack\n",
        "!./filter_pack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_slQCnAmp4DP",
        "outputId": "4de62cc4-5268-4869-b0cd-f4b5a39e2901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered values: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sparse Matrix-Vector-Multiply**"
      ],
      "metadata": {
        "id": "XAhajkbP4kIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a sparse matrix, most entries are zero. So, instead of storing all $N \\times N$ entries, we store only the non-zero ones.\n",
        "<br>\n",
        "The most common format to do this is [**CSR (Compressed Sparse Row)**](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)):\n",
        "\n",
        "- `k`: `0..(# non-zero elements)-1` index of a non-zero element\n",
        "- `values[k]`: array of the non-zero elements, stored contigously row after row\n",
        "- `columnidx[k]`: index of the column to which the `k`th non-zero element belongs\n",
        "- `i`: `0..N-1` row index\n",
        "- `rowptr[i]`: index of `values` (`k`) where the `i`th row begins\n",
        "\n",
        "*A typical access pattern: from `rowptr` you get the initial `k` for the `i`-th row, then, for each `value` in `k`, `k + 1`, ... you check which column it belongs to `columnidx`.*\n",
        "\n",
        "---\n",
        "\n",
        "Here we want to perform a sparse matrix-vector product, where we assume the vector to be dense and the matrix sparse.\n",
        "$$\n",
        "res_i = \\sum_{j = 0}^{N - 1} M_{i j} \\cdot vec_j\n",
        "$$\n",
        "\n",
        "The idea is for every row (= output element) to GATHER data from arbitrary indices in the input vector `vec` according to the matrix's existing entries.\n",
        "That is, for each position in a row of the matrix not holding a zero, we gather the corresponding `vec` element and carry out the multiply and accumulate.\n",
        "\n",
        "In fact, working on row `i` of a sparse matrix requires this loop:\n",
        "```c\n",
        "for (int k = rowptr[i]; k < rowptr[i+1]; k++) {\n",
        "  int col = columnidx[k];\n",
        "  float val = values[k]; // M[i][col]\n",
        "  float input = vec[col];\n",
        "}\n",
        "```\n",
        "Where we clearly \"gather\" `vec[col]` at arbitrary column positions for each non-zero."
      ],
      "metadata": {
        "id": "OZbT0qm44lbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/spmv.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "struct CSR {\n",
        "  int n;\n",
        "  std::vector<double> values;\n",
        "  std::vector<int> columnidx;\n",
        "  std::vector<int> rowptr;\n",
        "};\n",
        "\n",
        "// build a random sparse matrix in CSR format\n",
        "// - n : number of rows/columns\n",
        "// - nnz_per_row : number of non-zeros per row\n",
        "CSR random_csr(int n, int nnz_per_row) {\n",
        "  CSR M;\n",
        "  M.n = n;\n",
        "  M.rowptr.resize(n + 1);\n",
        "\n",
        "  std::mt19937 rng(12345);\n",
        "  std::uniform_int_distribution<int> columndist(0, n - 1);\n",
        "  std::uniform_real_distribution<double> values_dense(-1.0, 1.0);\n",
        "\n",
        "  M.values.reserve(n * nnz_per_row);\n",
        "  M.columnidx.reserve(n * nnz_per_row);\n",
        "\n",
        "  int k = 0;\n",
        "  for (int i = 0; i < n; i++) {\n",
        "    M.rowptr[i] = k;\n",
        "    for (int t = 0; t < nnz_per_row; t++) {\n",
        "      int col = columndist(rng);\n",
        "      M.columnidx.push_back(col);\n",
        "      M.values.push_back(values_dense(rng));\n",
        "      k++;\n",
        "    }\n",
        "  }\n",
        "  M.rowptr[n] = k;\n",
        "\n",
        "  return M;\n",
        "}\n",
        "\n",
        "// parallel SpMV\n",
        "void spmv(const CSR& M, const std::vector<double>& vec, std::vector<double>& res) {\n",
        "  int n = M.n;\n",
        "  res.assign(n, 0.0);\n",
        "\n",
        "  // parallel over rows\n",
        "  #pragma omp parallel for schedule(static)\n",
        "  for (int i = 0; i < n; i++) {\n",
        "\n",
        "    double sum = 0.0;\n",
        "    int start = M.rowptr[i];\n",
        "    int end = M.rowptr[i + 1];\n",
        "\n",
        "    // GATHER vector entries for the row's columns\n",
        "    #pragma omp parallel for schedule(static) reduction(+:sum)\n",
        "    for (int k = start; k < end; k++) {\n",
        "      int col = M.columnidx[k]; // index from which to gather\n",
        "      sum += M.values[k] * vec[col];\n",
        "    }\n",
        "\n",
        "    res[i] = sum;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int n = 2000;\n",
        "  int nnz_per_row = 10;\n",
        "\n",
        "  CSR M = random_csr(n, nnz_per_row);\n",
        "\n",
        "  std::vector<double> vec(n), res;\n",
        "\n",
        "  for (int i = 0; i < n; i++)\n",
        "    vec[i] = (double)i * 0.001;\n",
        "\n",
        "  double t0 = omp_get_wtime();\n",
        "  time(spmv, M, vec, res);\n",
        "  double t1 = omp_get_wtime();\n",
        "\n",
        "  std::cout << \"y[0] = \" << res[0] << \"\\n\";\n",
        "  std::cout << \"y[last] = \" << res.back() << \"\\n\";\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAh6QQQr4lqI",
        "outputId": "328c2bcd-1340-4290-eada-896afa934a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/spmv.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and ruin - ah, ehm... no - run:"
      ],
      "metadata": {
        "id": "bvVGj5k74l6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ spmv.cpp -fopenmp -o spmv\n",
        "!./spmv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxf4aq-E4mIA",
        "outputId": "c094852b-db91-47a0-b670-8a948d22d6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ function spmv took 1ms ]\n",
            "y[0] = -0.334976\n",
            "y[last] = 1.00368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image Upsampling - Gather vs Scatter**"
      ],
      "metadata": {
        "id": "yK94no68BRls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the task of upsampling a B&W image, e.g. doubling its width and height while retaining its content (visually speaking).\n",
        "<br>\n",
        "In practice, this means computing each pixel in the new image from pixels of the original. For us, each original pixel will be copied over to its transposed coordinates and then blur a 1-pixed radius (3x3 area) around himself by averaging its value with that of nearby pixels doing the same thing.\n",
        "\n",
        "There are two patterns we could think of to parallelize this:\n",
        "- each input pixel goes to write himself its 3x3 target output area: SCATTER\n",
        "- each output pixel goes fetch the input pixels to compute its value: GATHER\n",
        "\n",
        "Which approach do you think is better?\n",
        "<!--answer: see comments after the code-->\n",
        "For now, let's implement both!\n",
        "\n",
        "<br>\n",
        "\n",
        "*Another way to see the two:*\n",
        "- *GATHER: parallelize along output dimensions, each output pixel reads its input(s)*\n",
        "- *SCATTER: parallelize along input dimensions, each input writes its output(s)*"
      ],
      "metadata": {
        "id": "gij0_zApBWeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/upsample.cpp\n",
        "#include <cmath>\n",
        "#include <vector>\n",
        "#include <iomanip>\n",
        "#include <iostream>\n",
        "#include <omp.h>\n",
        "\n",
        "#include \"timer.hpp\"\n",
        "\n",
        "// SIMPLE GATHER: each output pixel reads from its nearest neighbor input pixels\n",
        "// NOTE: this is not used here, but it's just to show how simple a gether can get in this case\n",
        "void gather_upsample_easy(const std::vector<float>& input, int H, int W, std::vector<float>& output, int H2, int W2) {\n",
        "  #pragma omp parallel for collapse(2)\n",
        "  for (int i = 0; i < H2; i++) {\n",
        "    for (int j = 0; j < W2; j++) {\n",
        "      int src_i = i / 2;\n",
        "      int src_j = j / 2;\n",
        "      output[i * W2 + j] = input[src_i * W + src_j];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// GATHER: each output pixel reads from input pixels and averages its 1, 2, or 4 nearest neighbors\n",
        "void gather_upsample(const std::vector<float>& input, int H, int W, std::vector<float>& output, int H2, int W2) {\n",
        "  #pragma omp parallel for collapse(2)\n",
        "  for (int oi = 0; oi < H2; oi++) {\n",
        "    for (int oj = 0; oj < W2; oj++) {\n",
        "      float sum = 0.0f;\n",
        "      // visit a 3x3 output area\n",
        "      for (int di = -1; di <= 1; di++) {\n",
        "        for (int dj = -1; dj <= 1; dj++) {\n",
        "          // keep only output elements corresponding to a new input\n",
        "          if ((oi - di) % 2 != 0) continue;\n",
        "          if ((oj - dj) % 2 != 0) continue;\n",
        "          int i = (oi - di) / 2;\n",
        "          int j = (oj - dj) / 2;\n",
        "          if (i < 0 || i >= H || j < 0 || j >= W) continue;\n",
        "          float v = input[i * W + j];\n",
        "          // average of 1, 2, or 4 inputs, depending on the\n",
        "          // positions in the 3x3 area where they are found\n",
        "          sum += v / std::pow(2.0f, float(di*di + dj*dj));\n",
        "        }\n",
        "      }\n",
        "\n",
        "      output[oi * W2 + oj] = sum;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// SCATTER: each input pixel spreads into a 3x3 region of output\n",
        "void scatter_upsample(const std::vector<float>& input, int H, int W, std::vector<float>& output, int H2, int W2) {\n",
        "  std::fill(output.begin(), output.end(), 0.0f);\n",
        "\n",
        "  #pragma omp parallel for collapse(2)\n",
        "  for (int i = 0; i < H; i++) {\n",
        "    for (int j = 0; j < W; j++) {\n",
        "      float v = input[i * W + j];\n",
        "      int oi = 2 * i;\n",
        "      int oj = 2 * j;\n",
        "      // visit a 3x3 output area\n",
        "      for (int di = -1; di <= 1; di++) {\n",
        "        for (int dj = -1; dj <= 1; dj++) {\n",
        "          int ii = oi + di;\n",
        "          int jj = oj + dj;\n",
        "          if (ii >= 0 && ii < H2 && jj >= 0 && jj < W2) {\n",
        "            #pragma omp atomic\n",
        "            output[ii * W2 + jj] += v / std::pow(2.0f, dj*dj + di*di);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int H = 600, W = 800;\n",
        "  int H2 = H * 2, W2 = W * 2;\n",
        "\n",
        "  std::vector<float> input(H * W);\n",
        "  std::vector<float> output(H2 * W2);\n",
        "\n",
        "  for (int i = 0; i < H; i++)\n",
        "    for (int j = 0; j < W; j++)\n",
        "      input[i * W + j] = i * 1.4f + j;\n",
        "\n",
        "  std::cout << \"Input:\\n\";\n",
        "  for (int i = 0; i < H && i < 5; i++) {\n",
        "    for (int j = 0; j < W && j < 5; j++)\n",
        "      std::cout << std::setw(6) << std::fixed << std::setprecision(2) << std::right << input[i * W + j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "\n",
        "  std::cout << \"Gather upsampling...\\n\";\n",
        "  time(gather_upsample, input, H, W, output, H2, W2);\n",
        "\n",
        "  std::cout << \"Output:\\n\";\n",
        "  for (int i = 0; i < H2 && i < 10; i++) {\n",
        "    for (int j = 0; j < W2 && j < 10; j++)\n",
        "      std::cout << std::setw(6) << std::fixed << std::setprecision(2) << std::right << output[i * W2 + j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "\n",
        "  std::cout << \"Scatter upsampling...\\n\";\n",
        "  time(scatter_upsample, input, H, W, output, H2, W2);\n",
        "\n",
        "  std::cout << \"Output:\\n\";\n",
        "  for (int i = 0; i < H2 && i < 10; i++) {\n",
        "    for (int j = 0; j < W2 && j < 10; j++)\n",
        "      std::cout << std::setw(6) << std::fixed << std::setprecision(2) << std::right << output[i * W2 + j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAyoJMXGBWxO",
        "outputId": "54e8c640-5243-4c2e-eef0-68eac61fcfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/upsample.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just by glancing at the code, the GATHET can be implemented very cleanly, while the SCATTER is far more troublesome!\n",
        "\n",
        "That is because:\n",
        "- in the GATHER, each output pixel determines which input pixel(s) it needs, thus each thread writes to its own output location: safe, race-free.\n",
        "\n",
        "- in the SCATTER, each input pixel distributes its value into multiple output pixels, multiple threads may end up writing to the same output location: we must use atomics."
      ],
      "metadata": {
        "id": "XoUvJlNBFr7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "lCW5RSiCBW-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ upsample.cpp -fopenmp -o upsample\n",
        "!./upsample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSiSLyK6BXO5",
        "outputId": "487243de-06f3-48e5-d485-b38d0752db91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "  0.00   1.00   2.00   3.00   4.00 \n",
            "  1.40   2.40   3.40   4.40   5.40 \n",
            "  2.80   3.80   4.80   5.80   6.80 \n",
            "  4.20   5.20   6.20   7.20   8.20 \n",
            "  5.60   6.60   7.60   8.60   9.60 \n",
            "Gather upsampling...\n",
            "[ function gather_upsample took 130ms ]\n",
            "Output:\n",
            "  0.00   0.50   1.00   1.50   2.00   2.50   3.00   3.50   4.00   4.50 \n",
            "  0.70   1.20   1.70   2.20   2.70   3.20   3.70   4.20   4.70   5.20 \n",
            "  1.40   1.90   2.40   2.90   3.40   3.90   4.40   4.90   5.40   5.90 \n",
            "  2.10   2.60   3.10   3.60   4.10   4.60   5.10   5.60   6.10   6.60 \n",
            "  2.80   3.30   3.80   4.30   4.80   5.30   5.80   6.30   6.80   7.30 \n",
            "  3.50   4.00   4.50   5.00   5.50   6.00   6.50   7.00   7.50   8.00 \n",
            "  4.20   4.70   5.20   5.70   6.20   6.70   7.20   7.70   8.20   8.70 \n",
            "  4.90   5.40   5.90   6.40   6.90   7.40   7.90   8.40   8.90   9.40 \n",
            "  5.60   6.10   6.60   7.10   7.60   8.10   8.60   9.10   9.60  10.10 \n",
            "  6.30   6.80   7.30   7.80   8.30   8.80   9.30   9.80  10.30  10.80 \n",
            "Scatter upsampling...\n",
            "[ function scatter_upsample took 143ms ]\n",
            "Output:\n",
            "  0.00   0.50   1.00   1.50   2.00   2.50   3.00   3.50   4.00   4.50 \n",
            "  0.70   1.20   1.70   2.20   2.70   3.20   3.70   4.20   4.70   5.20 \n",
            "  1.40   1.90   2.40   2.90   3.40   3.90   4.40   4.90   5.40   5.90 \n",
            "  2.10   2.60   3.10   3.60   4.10   4.60   5.10   5.60   6.10   6.60 \n",
            "  2.80   3.30   3.80   4.30   4.80   5.30   5.80   6.30   6.80   7.30 \n",
            "  3.50   4.00   4.50   5.00   5.50   6.00   6.50   7.00   7.50   8.00 \n",
            "  4.20   4.70   5.20   5.70   6.20   6.70   7.20   7.70   8.20   8.70 \n",
            "  4.90   5.40   5.90   6.40   6.90   7.40   7.90   8.40   8.90   9.40 \n",
            "  5.60   6.10   6.60   7.10   7.60   8.10   8.60   9.10   9.60  10.10 \n",
            "  6.30   6.80   7.30   7.80   8.30   8.80   9.30   9.80  10.30  10.80 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Expand**"
      ],
      "metadata": {
        "id": "gg5jAluslAFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say you are developing a text editor. Yes, why not!\n",
        "<br>\n",
        "You need to deal with a very large amount of text, and worst of all, this text could undergo updates, insertions, and deletions, at any arbitrary point.\n",
        "<br>\n",
        "To deal with this, a very elementary solution would be to subdivide the text into a double-linked list of constant-sized arrays.\n",
        "If you can make it so that each array has a bit of extra free space at the end, you can then immediately accomodate small arbitrary updates, and only then dynamically alter the linked list structure to enable larger changes.\n",
        "<br>\n",
        "<br>\n",
        "Then, just as with any coding project for \"personal use\", you had an evil genius \"I hate myself so much for doing this\" moment.\n",
        "Spaces are costly, and tabs are annoying, let's therefore store any space as two consecutive bytes, one for the space character itself, the other for the number of spaces it truly represents.\n",
        "<br>\n",
        "<br>\n",
        "Now you have a problem tho, your file's content, that should be a continous string, is now fragmented into several arrays, each with arbitrary amounts of padding at the end, and with a notation for spaces not compatible with any other \"normal\" text editor.\n",
        "<br>\n",
        "When it is time to save the file, we need to concatenate all the valid parts of the arrays into a single one and expand spaces.\n",
        "<br>\n",
        "This task turns out to be quite easily parallelizable!\n",
        "\n",
        "We can use the EXPAND pattern, the fusion of MAP and PACK, where the mapped function can return 0, 1, or more elements and the pack concatenates all such returned values.\n",
        "\n",
        "In our case:\n",
        "- mapped function: iterate each array and prepare its final string;\n",
        "- pack: SCAN of the number of elements returned by each array, then SCATTER them into the final array;\n",
        "\n",
        "*Note: as already showed in [Linked List Traversal](#linked-list), going over linked lists in parallel isn't that efficient, but in this case it could be worth it to accelerate the ensuing EXPAND pattern.*"
      ],
      "metadata": {
        "id": "pThPwlh2lBnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/text_expand.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <cstring>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <omp.h>\n",
        "\n",
        "#define BLOCK_SIZE 64 // maximum size of each block's array\n",
        "\n",
        "// one of the arrays in the linked list\n",
        "struct Block {\n",
        "  char data[BLOCK_SIZE];\n",
        "  int size; // number of valid characters\n",
        "  Block* next;\n",
        "  Block* prev;\n",
        "  Block() : size(0), next(nullptr), prev(nullptr) {}\n",
        "};\n",
        "\n",
        "// initialize the linked list from a custom multi-line string\n",
        "Block* build_list_from_text() {\n",
        "  std::string input =\n",
        "R\"(\n",
        "int main() {\n",
        "  int N = 1000000;\n",
        "\n",
        "  double *A = (double*) malloc(N * sizeof(double));\n",
        "  double *B = (double*) malloc(N * sizeof(double));\n",
        "\n",
        "  // initialize vectors\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    A[i] = i * 0.001;\n",
        "    B[i] = (N - i) * 0.002;\n",
        "  }\n",
        "                       //HOW MUCH I LOVE SPACES\n",
        "  double dot = 0.0;\n",
        "  double start_time = omp_get_wtime();\n",
        "\n",
        "  // parallel reduce\n",
        "  #pragma omp parallel for reduction(+ : dot) schedule(static)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    dot += A[i] * B[i];\n",
        "  }\n",
        "                       //THIS SPACES ARE SO EFFICIENT NOW\n",
        "  double end_time = omp_get_wtime();\n",
        "\n",
        "  printf(\"Dot product = %.5f\\n\", dot);\n",
        "  printf(\"Computed in %.5f seconds using %d threads.\\n\",\n",
        "  end_time - start_time, omp_get_max_threads());\n",
        "                       //SPAAAAAAACE! <robot-like voice>\n",
        "  free(A);\n",
        "  free(B);\n",
        "}\n",
        ")\";\n",
        "\n",
        "  if (input.empty()) return nullptr;\n",
        "\n",
        "  // helper: append one byte to the current block, allocating a new one if needed\n",
        "  auto append_byte = [](Block*& current, char byte) {\n",
        "    if (current->size == BLOCK_SIZE) {\n",
        "      Block* nxt = new Block();\n",
        "      current->next = nxt;\n",
        "      nxt->prev = current;\n",
        "      current = nxt;\n",
        "    }\n",
        "    current->data[current->size++] = byte;\n",
        "  };\n",
        "\n",
        "  Block* head = new Block();\n",
        "  Block* current = head;\n",
        "\n",
        "  // encode spaces: a run of k spaces => ' ' + (unsigned char)k\n",
        "  for (size_t i = 0; i < input.size(); ++i) {\n",
        "    char c = input[i];\n",
        "    if (c == ' ') {\n",
        "      size_t j = i;\n",
        "      while (j < input.size() && input[j] == ' ')\n",
        "        ++j;\n",
        "      size_t run_len = j - i;\n",
        "      while (run_len > 0) {\n",
        "        unsigned char chunk = static_cast<unsigned char>(\n",
        "            std::min(run_len, static_cast<size_t>(255)));\n",
        "        append_byte(current, ' ');\n",
        "        append_byte(current, static_cast<char>(chunk));\n",
        "        run_len -= chunk;\n",
        "      }\n",
        "      i = j - 1;\n",
        "    } else {\n",
        "      append_byte(current, c);\n",
        "    }\n",
        "  }\n",
        "  return head;\n",
        "}\n",
        "\n",
        "\n",
        "// mapped function: pre-process a block\n",
        "// => isolate the string and convert \"space + count\" into repeated spaces\n",
        "std::string expand_block(const Block* blk) {\n",
        "  std::string out;\n",
        "  out.reserve(blk->size * 2); // rough guess\n",
        "\n",
        "  for (int i = 0; i < blk->size; ++i) {\n",
        "    char c = blk->data[i];\n",
        "    if (c == ' ' && i + 1 < blk->size) {\n",
        "      unsigned count = static_cast<unsigned char>(blk->data[i + 1]);\n",
        "      out.append(count, ' ');\n",
        "      i++; // skip the count byte\n",
        "    } else {\n",
        "      out.push_back(c);\n",
        "    }\n",
        "  }\n",
        "  return out;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  Block* head = build_list_from_text();\n",
        "\n",
        "  // linked lists are a pain:\n",
        "  // collect block pointers in a vector for quicker indexing\n",
        "  std::vector<Block*> blocks;\n",
        "  for (Block* p = head; p != nullptr; p = p->next)\n",
        "    blocks.push_back(p);\n",
        "\n",
        "  int n = blocks.size();\n",
        "  if (n == 0) {\n",
        "    std::cout << \"\" << std::endl;\n",
        "    return 0;\n",
        "  }\n",
        "\n",
        "  // MAP: expand each block independently\n",
        "  std::vector<std::string> expanded(n);\n",
        "  std::vector<size_t> sizes(n);\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    expanded[i] = expand_block(blocks[i]);\n",
        "    sizes[i] = expanded[i].size();\n",
        "  }\n",
        "\n",
        "  // INCLUSIVE SCAN: prefix sum of sizes\n",
        "  // NOTE: to keep the code brief, this is done sequentially, after all we have seen the scan plenty of times already!\n",
        "  std::vector<size_t> prefix(n + 1, 0);\n",
        "  for (int i = 0; i < n; ++i)\n",
        "    prefix[i + 1] = prefix[i] + sizes[i];\n",
        "  size_t total = prefix[n];\n",
        "\n",
        "  // SCATTER: write into the final buffer\n",
        "  std::string final_text(total, '\\0');\n",
        "  #pragma omp parallel for\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    size_t start = prefix[i];\n",
        "    std::memcpy(final_text.data() + start, expanded[i].data(), sizes[i]);\n",
        "  }\n",
        "\n",
        "  std::cout << final_text << std::endl;\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKonBwFvk_bS",
        "outputId": "c1a9513e-89c7-4767-cc7b-ff5f1f32cea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/text_expand.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "KETVtZIslAqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ text_expand.cpp -fopenmp -o text_expand\n",
        "!./text_expand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQa2ETmpCDfu",
        "outputId": "9a2f5f1e-ec97-4aa4-93a6-6565223668d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "int main() {\n",
            "  int N = 1000000;\n",
            "\n",
            "  double *A = (double*) malloc(N * sizeof(double));\n",
            "  double *B = (double*) malloc(N * sizeof(double));\n",
            "\n",
            "  // initialize vectors\n",
            "  for (int i = 0; i < N; i++) {\n",
            "    A[i] = i * 0.001;\n",
            "    B[i] = (N - i) * 0.002;\n",
            "  }\n",
            "                       //HOW MUCH I LOVE SPACES\n",
            "  double dot = 0.0;\n",
            "  double start_time = omp_get_wtime();\n",
            "\n",
            "  // parallel reduce\n",
            "  #pragma omp parallel for reduction(+ : dot) schedule(static)\n",
            "  for (int i = 0; i < N; i++) {\n",
            "    dot += A[i] * B[i];\n",
            "  }\n",
            "                       //THIS SPACES ARE SO EFFICIENT NOW\n",
            "  double end_time = omp_get_wtime();\n",
            "\n",
            "  printf(\"Dot product = %.5f\\n\", dot);\n",
            "  printf(\"Computed in %.5f seconds using %d threads.\\n\",\n",
            "  end_time - start_time, omp_get_max_threads());\n",
            "                       //SPAAAAAAACE! <robot-like voice>\n",
            "  free(A);\n",
            "  free(B);\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parallelizing Loops**"
      ],
      "metadata": {
        "id": "iyQQDcDr14D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you have analyzed in the [Loop Dependencies](#loop-dependencies) section, when we have inter-iteration data dependencies there is often still a bit of parallelism to squeeze out of loopnests. Let's now see that in practice.\n",
        "\n",
        "<img src=\"https://i.imgur.com/2euHumY.png\" align=\"right\" width=\"450px\" border=\"2\"> In general: a **recurrence pattern** is a type of loop where the calculation for one iteration depends on the resuls of a previous iteration.\n",
        "\n",
        "To identify a viable parallelization direction, the trick is to find a plane that cuts through the grid of intermediate results so that all references to previously computed values are on one side of the plane.\n",
        "Such a plane is called a *separating hyperplane*.\n",
        "You can then sweep (iterate) through the data in a direction perpendicular to this plane and perform all the operations on the plane at each iteration in parallel.\n",
        "<br>\n",
        "*Note: you can visualize it as moving the plan along its normal direction at each global iteration, then going in parallel \"inside\" the plane.*\n",
        "\n",
        "It can be proven that an hyperplane can always be found if the dependencies are constant offsets.\n",
        "See \"[The parallel execution of DO loops](https://dl.acm.org/doi/10.1145/360827.360844)\", Laslie Lamport."
      ],
      "metadata": {
        "id": "hn9ERJ1H18od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 15 - Spot the Hyperplane**"
      ],
      "metadata": {
        "id": "UOlChChDv8e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following loop-nests, identify their data dependencies and identify the slope of the separating hyperplane along which the computation can be parallelized.\n",
        "\n",
        "For convenience, let use say that $i$ spans the rows and $j$ the columns.\n",
        "\n",
        "<!--\n",
        "Loopnest 1)\n",
        "Imagining the computation to go from the top-left towards the bottom-right of a grid, data dependencies come in from the left and top of every cell, at distance 1.\n",
        "The hyperplane (a line, here) has angular coefficient m = 1.\n",
        "\n",
        "Loopnest 2)\n",
        "Imagining the computation to go from the top-left towards the bottom-right of a grid, data dependencies come in from the left and top-right of every cell, at distance 1.\n",
        "The hyperplane (a line, here) has angular coefficient m = 1/2.\n",
        "You can read this as \"for every cell I move downward, I need to go 2 to the left to stay within dependencies\".\n",
        "\n",
        "Loopnest 3)\n",
        "Imagining the computation to go from the top-left towards the bottom-right of a grid, data dependencies come in from the left of every cell at distance 1, and top of every cell at distance 2.\n",
        "The hyperplane (a line, here) has angular coefficient m = 2.\n",
        "\n",
        "Loopnest 4)\n",
        "Imagining the computation to go from the top-left-back towards the bottom-right-front of a grid, data dependencies come in from the left, top, and back of every cell, at distance 1.\n",
        "The hyperplane (a 2D plane, here) has both angular equal 1.\n",
        "\n",
        "\n",
        "In general, higher angular coefficients mean that more rows can run in parallel, while lower ones imply that more columns can run in parallel.\n",
        "-->"
      ],
      "metadata": {
        "id": "TWVKL7fHwCfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_me.cpp\n",
        "\n",
        "// Loopnest 1)\n",
        "for (j = 0; j < 100; j++)\n",
        "    for (i = 0; i < 100; i++)\n",
        "        a[i][j] = f(a[i][j - 1], a[i - 1][j]);\n",
        "\n",
        "// Loopnest 2)\n",
        "for (j = 0; j < 100; j++)\n",
        "    for (i = 0; i < 100; i++)\n",
        "        a[i][j] = f(a[i][j - 1], a[i - 1][j + 1]);\n",
        "\n",
        "// Loopnest 3)\n",
        "for (j = 0; j < 100; j++)\n",
        "    for (i = 0; i < 100; i++)\n",
        "        a[i][j] = f(a[i][j - 1], a[i - 2][j]);\n",
        "\n",
        "// Loopnest 4)\n",
        "for (j = 0; j < 100; j++)\n",
        "    for (i = 0; i < 100; i++)\n",
        "        for (k = 0; k < 20; k++)\n",
        "            a[i][j][k] = f(\n",
        "              a[i][j - 1][k], a[i - 1][j + 1][k],\n",
        "              a[i][j][k - 1], a[i][j][k]\n",
        "            );"
      ],
      "metadata": {
        "id": "GHSCaXP_wWbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recurrence Pattern**"
      ],
      "metadata": {
        "id": "LpDwyYK62-D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a 5-points STENCIL pattern.\n",
        "It can be seen as a MAP where the output depends on multiple, neighboring, inputs!\n",
        "\n",
        "It is typically used in simulations (e.g. automata) where it updates an entire grid in discrete time steps. To do this sequentially, we can use a single grid instance, but to go parallel often two are used in a \"ping-pong\" fashion, a copy is read-only and serves as the previous state from which to write the next state in the other copy, then the two are swapped and the process repeats for the next time step.\n",
        "\n",
        "But what if the grid is massive, or we simply can't afford to have multiple copies? We need **in-place** updates.\n",
        "And because of this, must we go serial now? Well, the loop often looks like this:\n",
        "```\n",
        "for i in [1, N-1):\n",
        "  for j in [1, M-1):\n",
        "    g[i][j] = f(g[i][j], g[i-1][j], g[i][j-1], g[i+1][j], g[i][j+1])\n",
        "```\n",
        "<img src=\"https://www.eecg.utoronto.ca/~tsa/jasmine/blocksched1.gif\" align=\"right\" width=\"200px\"> Well, taking a lesson from dynamic programming, we can use what is called **wavefront**: sweep the computation diagonally!\n",
        "<br>\n",
        "In particular, for any inter-iteration, fixed-offset, dependency, there is always a skew for which a wavefront can propagate along diagonals, running in parallel inside each diagonal.\n",
        "\n",
        "A good source on the subject: https://www.eecg.utoronto.ca/~tsa/jasmine/wave.html\n",
        "\n",
        "An important distinction to be made while look at the code as if it ran sequentially:\n",
        "- `g[i][j], g[i+1][j], g[i][j+1]` are anti-dependencies, old values are read when the code runs serially, they don't constitute a problem unless we are \"too much ahead\";\n",
        "- `g[i-1][j], g[i][j-1]` are true dependencies, signifying that the present computation requires, new, up-to-date values from previous ones;\n",
        "\n",
        "---\n",
        "\n",
        "In this example we step forward a 7-ponts STENCIL computation in-place. It is a 5-points stencil, but we also go 2-right and 2-left of the current cell.\n",
        "\n",
        "For parallelization, we have a first sequential loop over diagonals, and then a parallel loop along each diagonal.\n",
        "\n",
        "Be wary: the diagonal varies a log in span throughout the computation, and so does the amount of work we can give to processes. Load balancing must be kept in mind here!"
      ],
      "metadata": {
        "id": "CMfCPhUz3DMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/wavefront.cpp\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <algorithm>\n",
        "#include <vector>\n",
        "#include <omp.h>\n",
        "\n",
        "constexpr int N = 512;\n",
        "\n",
        "int main() {\n",
        "  std::vector<std::vector<double>> g(N, std::vector<double>(N, 0.0));\n",
        "\n",
        "  // initial state (every fifth row at \"i\")\n",
        "  for (int i = 0; i < N; i += 5)\n",
        "    for (int j = 0; j < N; j++)\n",
        "      g[i][j] = (float)(i);\n",
        "\n",
        "  std::cout << \"Initial:\\n\";\n",
        "  for (int i = 0; i < N && i < 20; i++) {\n",
        "    for (int j = 0; j < N && j < 20; j++)\n",
        "      std::cout << std::setw(5) << std::fixed << std::setprecision(2) << std::right << g[i][j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "\n",
        "  // wavefront along diagonals (from top-left to bottom-right)\n",
        "  for (int d = 2; d <= 2*(N - 3); d++) {\n",
        "\n",
        "    #pragma omp parallel for\n",
        "    for (int j = 2; j < N-2; j++) {\n",
        "      int i = d - j;\n",
        "      if (i < 2 || i >= N-2) continue;\n",
        "      double newv = g[i - 1][j] + g[i + 1][j] + g[i][j - 1] + g[i][j + 1] + g[i][j - 2] + g[i][j + 2] + g[i][j];\n",
        "      g[i][j] = newv / 7.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  std::cout << \"Final:\\n\";\n",
        "  for (int i = 0; i < N && i < 20; i++) {\n",
        "    for (int j = 0; j < N && j < 20; j++)\n",
        "      std::cout << std::setw(5) << std::fixed << std::setprecision(2) << std::right << g[i][j] << \" \";\n",
        "    std::cout << \"\\n\";\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZvDXN2I3CQX",
        "outputId": "d9f29cfc-8522-4c29-fafe-f64eff8d565e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/wavefront.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "wmZ_oAfE3Dlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ wavefront.cpp -fopenmp -o wavefront\n",
        "!./wavefront"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A35JqA83DyR",
        "outputId": "46855ab2-3aaf-4940-a5b9-246e344a1741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial:\n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            "10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            "15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 15.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            "Final:\n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
            " 0.00  0.00  0.71  0.82  0.93  0.96  0.99  0.99  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 \n",
            " 5.00  5.00  3.67  3.50  3.30  3.25  3.22  3.21  3.20  3.20  3.20  3.20  3.20  3.20  3.20  3.20  3.20  3.20  3.20  3.20 \n",
            " 0.00  0.00  0.52  0.57  0.63  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64  0.64 \n",
            " 0.00  0.00  0.07  0.09  0.11  0.12  0.12  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13  0.13 \n",
            " 0.00  0.00  0.01  0.01  0.02  0.02  0.02  0.02  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03 \n",
            " 0.00  0.00  1.43  1.63  1.87  1.93  1.98  1.99  2.00  2.00  2.00  2.00  2.00  2.00  2.01  2.01  2.01  2.01  2.01  2.01 \n",
            "10.00 10.00  7.35  7.00  6.60  6.50  6.44  6.42  6.41  6.40  6.40  6.40  6.40  6.40  6.40  6.40  6.40  6.40  6.40  6.40 \n",
            " 0.00  0.00  1.05  1.15  1.26  1.27  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28  1.28 \n",
            " 0.00  0.00  0.15  0.19  0.23  0.24  0.25  0.25  0.25  0.26  0.26  0.26  0.26  0.26  0.26  0.26  0.26  0.26  0.26  0.26 \n",
            " 0.00  0.00  0.02  0.03  0.04  0.04  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05 \n",
            " 0.00  0.00  2.15  2.45  2.81  2.90  2.96  2.99  3.00  3.01  3.01  3.01  3.01  3.01  3.01  3.01  3.01  3.01  3.01  3.01 \n",
            "15.00 15.00 11.02 10.50  9.90  9.76  9.66  9.63  9.61  9.61  9.60  9.60  9.60  9.60  9.60  9.60  9.60  9.60  9.60  9.60 \n",
            " 0.00  0.00  1.57  1.72  1.89  1.91  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92  1.92 \n",
            " 0.00  0.00  0.22  0.28  0.34  0.36  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38  0.38 \n",
            " 0.00  0.00  0.03  0.04  0.06  0.07  0.07  0.07  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08 \n",
            " 0.00  0.00  2.86  3.27  3.74  3.87  3.95  3.99  4.00  4.01  4.01  4.01  4.01  4.02  4.02  4.02  4.02  4.02  4.02  4.02 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Successive Over-Relaxations**"
      ],
      "metadata": {
        "id": "CEJhxYo0BTEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successive Over-Relaxations (SOR) is an iterative method for numerically solving grid-based Partial Differential Equations (PDEs) (e.g. finite-difference discretizations of Laplace/Poisson).\n",
        "<br>\n",
        "It is essentially the Gauss-Seidel iterative method, but each update is multiplied by a relaxation parameter $\\omega \\in (0, 2)$:\n",
        "$$\n",
        "u^{(new)} = (1 - \\omega) \\cdot u^{(old)} + \\omega \\cdot u^{(GS)}\n",
        "$$\n",
        "where $u^{(GS)}$ is the value you would compute with standard Gauss-Seidel method.\n",
        "\n",
        "Advantage:\n",
        "<br>\n",
        "Our input isn't made of only old values, SOR uses updated values immediately as they become available.\n",
        "Thus the grid can be updated **in-place**, and we can thus get away with a single copy of it!\n",
        "De facto, the update uses the latest information in a fixed order (sweep).\n",
        "\n",
        "For everyone unfamiliar: everything up to now comes down to a 4-points average STENCIL (5-points minus the center).\n",
        "However, we can leverage the properties of the problem to know that we can update, in a staggered fashion, one cell every two at each time step.\n",
        "And an acceptable solution will still be reached.\n",
        "In other words, the 4-points stencil makes the grid-wise dependency graph bipartite.\n",
        "This removes our data dependencies and gives us one fully parallel iteration every two steps.\n",
        "\n",
        "<img src=\"https://wallpapers.com/images/hd/red-black-background-pomifr0hf37fyulf.jpg\" border=\"2\" align=\"right\" width=\"200px\">\n",
        "\n",
        "SOR works when:\n",
        "- the update stencil is monotone (e.g. average)\n",
        "- the iteration order is consistent (e.g. left-to-right, top-to-bottom)\n",
        "- there are no data races (this we need to enforce with our code!)\n",
        "\n",
        "How to parallelize: red-black coloring!\n",
        "<br>\n",
        "Red-black SOR divides the grid like a chessboard, with diagonally adjacent cells all of the same color, either red or black (Note: the specific scheme depends on the stencil's shape).\n",
        "First, only red cells are updated in parallel (all black neighbors are thus safe to read).\n",
        "Then, we update black cells in parallel (all red neighbors are thus safe).\n",
        "And repeat.\n",
        "Ultimately, the stencil never reads a value being written in the same step."
      ],
      "metadata": {
        "id": "nm2tKqeYBWfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/sor.cpp\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 256\n",
        "#define MAX_ITERS 5000\n",
        "#define TOL 1e-4\n",
        "\n",
        "int main() {\n",
        "  static double u[N][N] = {0};\n",
        "  const double omega = 1.8;\n",
        "\n",
        "  // example: boundary condition on the top row\n",
        "  for (int j = 0; j < N; j++)\n",
        "    u[0][j] = 1.0;\n",
        "\n",
        "  for (int iter = 0; iter < MAX_ITERS; iter++) {\n",
        "    double maxdiff = 0.0;\n",
        "\n",
        "    // RED step\n",
        "    #pragma omp parallel for reduction(max:maxdiff)\n",
        "    for (int i = 1; i < N-1; i++)\n",
        "      for (int j = 1 + (i & 1); j < N-1; j += 2) {\n",
        "        double old = u[i][j];\n",
        "        double avg = 0.25 * (u[i - 1][j] + u[i + 1][j] + u[i][j - 1] + u[i][j + 1]);\n",
        "        double newv = (1.0 - omega)*old + omega*avg;\n",
        "        u[i][j] = newv;\n",
        "        maxdiff = fmax(maxdiff, fabs(newv - old));\n",
        "      }\n",
        "\n",
        "    // BLACK step\n",
        "    #pragma omp parallel for reduction(max:maxdiff)\n",
        "    for (int i = 1; i < N-1; i++)\n",
        "      for (int j = 1 + ((i+1) & 1); j < N-1; j += 2) {\n",
        "        double old = u[i][j];\n",
        "        double avg = 0.25 * (u[i - 1][j] + u[i + 1][j] + u[i][j - 1] + u[i][j + 1]);\n",
        "        double newv = (1.0 - omega)*old + omega*avg;\n",
        "        u[i][j] = newv;\n",
        "        maxdiff = fmax(maxdiff, fabs(newv - old));\n",
        "      }\n",
        "\n",
        "    if (maxdiff < TOL) {\n",
        "      printf(\"Converged after %d iterations (diff=%.6f)\\n\", iter, maxdiff);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"u[64][64] = %f\\n\", u[64][64]);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sTdv38qBWGM",
        "outputId": "0686986b-606e-495b-8afe-2f7e074ce84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/sor.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "pSmUnGJPBW1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ sor.cpp -fopenmp -o sor\n",
        "!./sor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9deAz_rCBXBu",
        "outputId": "7f60ca56-4385-456b-c5ea-46f3cc7511e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 1279 iterations (diff=0.000100)\n",
            "u[64][64] = 0.393263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inspect The Hardware**"
      ],
      "metadata": {
        "id": "kGBPzM7QgNMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what the machine we are using (here on Colab) is capable of:"
      ],
      "metadata": {
        "id": "aSsfp_hzgQW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /home/OpenMP/inspect_hw.cpp\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "#include <omp.h>\n",
        "#include <unistd.h>\n",
        "#include <thread>\n",
        "\n",
        "// helper to read cache info from Linux sysfs\n",
        "long read_cache_size(int level) {\n",
        "    std::string path = \"/sys/devices/system/cpu/cpu0/cache/index\" + std::to_string(level) + \"/size\";\n",
        "    std::ifstream file(path);\n",
        "    if (!file.is_open()) return -1;\n",
        "    std::string value;\n",
        "    file >> value;\n",
        "    long size = std::stol(value);\n",
        "    if (value.find('K') != std::string::npos) size *= 1024;\n",
        "    if (value.find('M') != std::string::npos) size *= 1024 * 1024;\n",
        "    return size;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // CPU and threading info\n",
        "  int omp_procs = omp_get_num_procs();\n",
        "  int omp_max_threads = omp_get_max_threads();\n",
        "  unsigned int hw_threads = std::thread::hardware_concurrency();\n",
        "\n",
        "  std::cout << \"=== System Info (OpenMP + Hardware) ===\\n\";\n",
        "  std::cout << \"Logical processors available (OpenMP): \" << omp_procs << \"\\n\";\n",
        "  std::cout << \"Max OpenMP threads: \" << omp_max_threads << \"\\n\";\n",
        "  std::cout << \"Hardware concurrency (std::thread): \" << hw_threads << \"\\n\";\n",
        "\n",
        "  // hyperthreading available if hardware threads > physical cores\n",
        "  if (hw_threads > omp_procs / 2)\n",
        "    std::cout << \"Hyperthreading likely enabled.\\n\";\n",
        "  else\n",
        "    std::cout << \"No hyperthreading detected (or not applicable).\\n\";\n",
        "\n",
        "  // cache info\n",
        "  for (int i = 0; i < 3; ++i) {\n",
        "    long size = read_cache_size(i);\n",
        "    if (size > 0)\n",
        "      std::cout << \"L\" << (i + 1) << \" cache size: \" << size / 1024 << \" KB\\n\";\n",
        "  }\n",
        "\n",
        "  // RAM info\n",
        "  long pages = sysconf(_SC_PHYS_PAGES);\n",
        "  long page_size = sysconf(_SC_PAGE_SIZE);\n",
        "  double total_ram_gb = (double)pages * page_size / (1024.0 * 1024.0 * 1024.0);\n",
        "  std::cout << \"Total physical memory: \" << total_ram_gb << \" GB\\n\";\n",
        "\n",
        "  // OpenMP runtime confirmation\n",
        "  #pragma omp parallel\n",
        "  {\n",
        "    #pragma omp single\n",
        "    std::cout << \"Actual threads used by default by OpenMP: \" << omp_get_num_threads() << \"\\n\";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVxC1SOogTon",
        "outputId": "0160982c-bf04-4579-cdb2-d4ea3faa8591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /home/OpenMP/inspect_hw.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "-lwy9qAHhBfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ inspect_hw.cpp -fopenmp -o inspect_hw\n",
        "!./inspect_hw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKCbPzBphCzq",
        "outputId": "a38b9422-f38a-489a-8719-7825238b97a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== System Info (OpenMP + Hardware) ===\n",
            "Logical processors available (OpenMP): 2\n",
            "Max OpenMP threads: 2\n",
            "Hardware concurrency (std::thread): 2\n",
            "Hyperthreading likely enabled.\n",
            "L1 cache size: 32 KB\n",
            "L2 cache size: 32 KB\n",
            "L3 cache size: 256 KB\n",
            "Total physical memory: 12.6714 GB\n",
            "Actual threads used by default by OpenMP: 2\n"
          ]
        }
      ]
    }
  ]
}