{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CI3PKgDGNeLJ",
        "1uFjY9mrbzGQ",
        "uhCMbl630JM2",
        "s2ZLrun5GbZ1",
        "XOpnAE1XnMy3",
        "INe2-F1gtBAC",
        "Vg61LYb-DVxE",
        "QE5eNfgYpPkj",
        "cVRuiHZVwKKu",
        "lOersQlA8xQw",
        "oU-S1BxF-PEY",
        "srRrhR6G_CE7",
        "MDPuD2Qx_jJI",
        "Zf9ZaB_e_-YZ",
        "jPRUKt--7zUT",
        "OZaUNa3GP5ie",
        "meWx-rTRQS2W",
        "jIr0L3IXQ34l",
        "1hF7UbsJNYNd",
        "omnfU6F246oE",
        "bWo71Y9d5p40",
        "Kr0MRAX5eKAE",
        "GUFcfWgThYT9",
        "Gv-T1xlJeRQu",
        "y8Tw87vzeZX9",
        "fJPEJshGec_8",
        "QLSAwTAYgLV4",
        "W7gpJuR0aLRd",
        "M6Fh_k6cn3LP",
        "XEM4xdZHaP5e",
        "B01Tt39IzLBB",
        "zOBN5HqsVx_8",
        "NIIjR9Mm-9br",
        "rX37hvYHvNav"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thomas-Fabbris/parallel-computing-polimi/blob/main/CUDA_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA**\n",
        "\n",
        "Exploiting NVIDIA GPUs for general-purpose computing.\n"
      ],
      "metadata": {
        "id": "8XpMiVQXJXYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "CI3PKgDGNeLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "JGdTnbSyvSnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ],
      "metadata": {
        "id": "Mghv9aT0fzx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18eaf18b-6e92-4736-91e2-8fb8e470ae8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Glossary**"
      ],
      "metadata": {
        "id": "UM0BtfnO2ip2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thread**: one running instance of the kernel, with its own execution context.<br>\n",
        "**Warp**: group of threads that are scheduled together on a streaming multiprocessor (SM) and get SIMD (or, as Nvidia calls it, SIMT) execution when their program counters align.<br>\n",
        "**Block**: group of threads that get assigned to the same streaming multiprocessor, sharing shared memory and synchronization. A block internally consists of one or more warps.<br>\n",
        "**Grid**: group of blocks that defines the full launch configuration of a kernel.\n",
        "\n",
        "*Note: Warps are decided by the hardware, blocks and grid are user defined.*\n",
        "\n",
        "---\n",
        "\n",
        "Full math for a thread's ID inside a block:\n",
        "```\n",
        "thread_id = threadIdx.x +\n",
        "            threadIdx.y * blockDim.x +\n",
        "            threadIdx.z * blockDim.x * blockDim.y\n",
        "```\n",
        "\n",
        "ID of the block the current thread is part of:\n",
        "```\n",
        "block_id = blockIdx.x +\n",
        "           gridDim.x * blockIdx.y +\n",
        "           gridDim.x * gridDim.y * blockIdx.z\n",
        "```\n",
        "\n",
        "Absolute (grid-level) thread ID and position:\n",
        "```\n",
        "global_thread_id = block_id * (blockDim.x * blockDim.y * blockDim.z) + thread_id\n",
        "\n",
        "global_x_pos = blockIdx.x * blockDim.x + threadIdx.x\n",
        "global_y_pos = blockIdx.y * blockDim.y + threadIdx.y\n",
        "global_z_pos = blockIdx.z * blockDim.z + threadIdx.z\n",
        "```\n",
        "\n",
        "Thread idx inside a warp and warp ID in a block:\n",
        "```\n",
        "idx_in_warp = thread_id % warpSize\n",
        "\n",
        "warp_id = thread_id / warpSize\n",
        "```\n",
        "Dimensions are projected into a linearized row-major (x before y before z) order before partitioning into warps.\n",
        "\n",
        "*Note: for a block whose size is not a multiple of warpSize, the last warp will be padded with extra threads to fill up the warpSize-thread positions. For example, with warpSize = 32, if a block has 48 threads, it will be partitioned into two warps, and its second warp will be padded with 16 extra thread indices.*\n",
        "\n",
        "*Note: a nice name for this is **hierarchical indexing**.*\n",
        "\n",
        "*Note: warps aside, you are not constrained to scanning each block and then the grid row-wise like it's done here, but this is enough general-purpose that you rarely need another way to linearize thread indices.*\n",
        "\n",
        "---\n",
        "\n",
        "Available variables to each thread:\n",
        "- `blockIdx(.x|.y|.z)`: idx of the thread's block\n",
        "- `blockDim(.x|.y|.z)`: size of one block (threads per dimension)\n",
        "- `threadIdx(.x|.y|.z)`: idx of the thread in its block\n",
        "- `gridDim(.x|.y|.z)`: size of the grid of blocks (blocks per dimension)\n",
        "- `warpSize`: number of threads per warp (implementation detail)\n",
        "\n",
        "*Note: he type `dim3` is a 3D uint vector with default value 1 for unspecified dimensions.*\n",
        "\n",
        "*Note: grid size = number of blocks, block size = number of threads per block*\n",
        "\n",
        "---\n",
        "\n",
        "Launching a kernel (2D example):\n",
        "```\n",
        "// you choose threads_per_row, threads_per_col\n",
        "dim3 blockDim(threads_per_row, threads_per_col);\n",
        "// given in_rows, in_cols for the data you need to process, infer the required grid\n",
        "dim3 gridDim((in_rows + blockDim.x - 1)/blockDim.x, (in_cols + blockDim.y - 1)/blockDim.y);\n",
        "// e.g.: one shared value per thread in the block\n",
        "unsigned int sharedMemory = blockDim.x*blockDim.y*sizeof(...);\n",
        "kernelFunction<<<numBlocks, blockSize, sharedMemory>>>(...);\n",
        "cudaDeviceSynchronize();\n",
        "```\n",
        "All those \"weird\" sums before divisions are a fast way to compute the ceiling with integers:\n",
        "\n",
        "Let $n, m \\in \\mathbb{N}$, then $\\text{ceil}(n/m) = \\text{floor}((n + m - 1)/m)$.\n",
        "\n",
        "We can also write it as $\\lceil \\frac{n}{m} \\rceil = \\lfloor \\frac{n + m - 1}{m} \\rfloor$.\n",
        "\n",
        "Then, recall that 'floor' is automatically applied by the division between unsigned integers in C/C++.\n",
        "In truth, what happens is 'truncation', so for negative numbers it equates to applying 'ceil'.\n",
        "But for index arithmetics this matters not, as it's almost always positive.\n",
        "Therefore, by using the right side of the above equalities, we don't need to compute anything beside an integer sum and division, no ceil/floor required.\n",
        "\n",
        "---\n",
        "\n",
        "Available attributes:\n",
        "- `__global__`: marks a function (kernel) callable from the host with the <<<...>>> syntax\n",
        "- `__shared__`: marks memory that is shared by all threads of a block (vital for inter-thread **data re-use**)\n",
        "  - `extern __shared__` the shared memory size is decided at kernel launch (last entry in <<<...>>>)\n",
        "- `__device__`: marks a function or variable that lives on the device\n",
        "- `__constant__`: marks constant memory, i.e., cached, read-only memory on the device\n",
        "- `__host__`: marks a function as a host function (runs on the CPU), implied by default\n",
        "- `__host__ __device__`: marks a function collable from both device and host (one version is compiled for each)\n",
        "- `__managed__`: declares a variable in unified memory, accessible from both host and device, with automatic migration\n",
        "- `__restrict__`: tells the compiler that a pointer does not alias with others, allowing better optimization (this is C++, not CUDA-specific)\n",
        "\n",
        "---\n",
        "\n",
        "The CUDA memory model:\n",
        "\n",
        "<!--<img src=\"https://polimi365-my.sharepoint.com/:i:/r/personal/10669641_polimi_it/Documents/Images/CUDA_memory_model.PNG\" alt=\"CUDA memory hierarchy overview.\" width=\"500\" border=\"2\">-->\n",
        "<img src=\"https://i.imgur.com/yoEW1tU.png\" alt=\"CUDA memory hierarchy overview.\" width=\"500\" border=\"2\">\n",
        "\n",
        "*Credit: \"Programming Massiveli Parallel Processors\" by W. Hwu, D. Kirk, I. Hajj*\n",
        "\n",
        "---\n",
        "\n",
        "CUDA gives you small vector data types:\n",
        "- `double2`\n",
        "- `double3`\n",
        "- `double4`\n",
        "- `float2`\n",
        "- `float3`\n",
        "- `float4`\n",
        "- `half`: 16bit float\n",
        "- `half2`\n",
        "- `bfloat16`: 8bit exponent (same as float), 7+1bit mantissa (low precision)\n",
        "- `bfloat162`\n",
        "\n",
        "---\n",
        "\n",
        "Additional useful stuff:\n",
        "- `__syncthreads()`: used inside a kernel to create a barrier (typical case: load data in shared memory -> sync -> compute on data)\n",
        "- `int __syncthreads_count(predicate)`: sync and count how many instances of \"predicate\" are true among all threads\n",
        "- `int __syncthreads_and(predicate)`, `int __syncthreads_or(predicate)`: same as above, but return the AND or OR of predicates\n",
        "- `int __all(predicate)`: same as `__syncthreads_and`, but among threads in the same warp (used to manually speedup branches)\n",
        "- `int __any(predicate)`: same as `__syncthreads_or`, but among threads in the same warp\n",
        "- atomic operations: `atomicAdd`, `atomicSub`, `atomicMin`, `atomicMax`, `atomicInc` (increment), `atomicDec` (decrement), `atomicExch` (exchange), `atomicCAS` (compare-and-swap), `atomicAnd` (bitwise), `atomicOr`, `atomicXor`\n",
        "- ` __threadfence_block()`: wait until all global and shared memory writes are visible to all threads in the same block\n",
        "- ` __threadfence()`:  wait until all shared memory writes are visible to all threads in block and global memory writes are visible to all threads\n",
        "\n",
        "Note: the CAS is very powerful to create global atomic locks!\n",
        "```\n",
        "// global variable: 0 unlocked, 1 locked\n",
        "__device__ int lock = 0;\n",
        "__global__ void kernel(...) {\n",
        "  ...\n",
        "  if (threadIdx.x == 0) {\n",
        "    // acquire lock\n",
        "    do {} while(atomicCAS(&lock, 0, 1));\n",
        "    ...\n",
        "     __threadfence();\n",
        "    // release lock\n",
        "    lock = 0;\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Warp shuffles:\n",
        "- `T __shfl_up_sync(unsigned mask, T var, unsigned int delta)`, retrieves the value of `var` from another thread in the same warp that has warp_id `delta` lower than the caller\n",
        "  - `mask` controls which threads are involved, usually set to -1 or 0xffffffff (every thread in the warp)\n",
        "  - `var` is the local register variable from which to take the values to shuffle around\n",
        "  - `delta` is the offset within the warp where to go fetch the value, if the appropriate thread does not exist (i.e. it's off the end of the warp) then the value is taken from the current thread\n",
        "  - return value retrieved by the current thread\n",
        "- `T __shfl_down_sync(unsigned mask, T var, unsigned int delta)`, same as above,but the `delta` is added to the caller's warp_id instead of subtracted\n",
        "- `T __shfl_xor_sync(unsigned mask, T var, unsigned int lane_mask)` a XOR operation is performed between `lane_mask` and the calling thread's warp_id (or \"lane\") to determine the thread from which to copy the value (`lane_mask` controls which bits of the caller's id are \"flipped\")\n",
        "- `__shfl_sync(unsigned mask, T var, unsigned int src_lane)`, simply go and fetch `var` from the thread with warp_id `src_lane`\n",
        "\n",
        "---\n",
        "\n",
        "Rules of thumb:\n",
        "- the CUDA mentality is to launch exactly as many thread blocks as will fill\n",
        "the GPU or match the input/output problem shape\n",
        "- first fix the blockDim (# threads per block) then select gridDim (# of blocks) to fit the input (or output - depends on the algorithm's degree of parallelism)\n",
        "- one can determine if a control construct can result in thread divergence by inspecting its decision condition. If the decision condition is based on threadIdx values, the control statement can potentially cause thread divergence\n",
        "- unless you can prove no edge cases ever occur, every memory access needs to have a corresponding check that ensures that the indices used in the access are within the bounds of the array being accessed\n",
        "- preload data in shared memory when more than one thread in a block accesses (reuses) the same element (ideally many times)\n",
        "- imagine the mapping of global thread indices to data indices as a multivalued function, if the collective image of the threads in one block overlaps with the image of those in another, you will have duplicate elements in shared memory (this is fine, but you need more memory)\n",
        "- you don't need to work with the idea of sub-cores and warp scheduling in mind, that's out of your control, but you need to keep in mind which threads end up in the same warp, and possibly which blocks (likely) in the same SM\n",
        "\n",
        "---\n",
        "\n",
        "Some general steps to compute control divergence:\n",
        "- consider a single block, analyze how it is divided in warps\n",
        "- identify blocks that have divergence\n",
        "  - if the divergence is caused by a boundary condition, those are the blocks on the boundary\n",
        "  - otherwise, you must find the underlying pattern causing the divergence\n",
        "- for each of these blocks, identify the warps that have divergence\n",
        "  - do not evaluate every block from scratch, bundle together those seeing the same divergence pattern\n",
        "- you can now compute the fraction of diverged warps\n",
        "\n",
        "<!--\n",
        "Continuing:\n",
        "- for each diverged warp, identify and count the distinct operations collectively required by its threads\n",
        "- for non-diverged warp, follow once every possible independent path (control flow) they could follow, count its instructions\n",
        "- for each diverged and not diverged control-flow you observe, count the warps that follow it\n",
        "- the instructions overhead of divergece is given by the sum of the instructions needed by each warp, diverged and not, divided by the total instructions of non-diverged threads plus the average number of instructions between all paths times the number of diverged threads\n",
        "-->"
      ],
      "metadata": {
        "id": "Szu0EHCS2mwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: this notebook sometimes voluntarily changes coding conventions, like using 'i' for IDs instead of 'id'. This is intended to train the reader to understand someone else's code at a glance.* <!-- warning: retcon allert -->\n",
        "\n",
        "*Note: much alike the above, many values that could be 'unsigned int' are written just as 'int' for readability. These kind of micro-optimizations are left to the reader.*\n",
        "\n",
        "*Note: nvcc treats all code as C++, not C. Still, this notebook uses as little as possible syntax from C++.*"
      ],
      "metadata": {
        "id": "t-inf5XRwYyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1**"
      ],
      "metadata": {
        "id": "1uFjY9mrbzGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your CUDA environment (compiler support and available GPU card):"
      ],
      "metadata": {
        "id": "_OpAjdQtb2W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNS2FA5BR0bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47d065a-1421-475e-95b2-bf97feae640d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "dOAtZ7-qFxfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3b74c2-8fec-457c-af40-9e276a7707dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1079b5db-aa98-6348-11c3-c91825cc7fa7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UdHPNHzlcGEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6f148e-407d-4366-93b8-6888ab81f7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 26 13:46:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know about the capabilities of you GPU, you can query the device properties and/or read the [compute capability specifications](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability).<br>\n",
        "Then, you should be able to answer the following questions:\n",
        "\n",
        "* What are the maximum dimensions of a thread block?\n",
        "* What is the maximum number of threads in a block?  \n",
        "* What is the maximum size of the GPU global memory?\n",
        "* What is the warp size?\n",
        "* How many Streaming Multiprocessors are available?\n",
        "* Is an execution configuration of <<<100, 1000>>> feasible on this device?\n",
        "* Suppose you have a CUDA program using multiple blocks, should each block contain 4x4, 8x8, or 30x30 threads?\n",
        "<!--\n",
        "Universal answer: it depends.\n",
        "In this case, depends on the size of the input, the amount of local variables used by each thread, and so on.\n",
        "-->"
      ],
      "metadata": {
        "id": "RJ_rxHDGcLbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deviceProperties.cpp\n",
        "//                           ^^^ The extension should be 'cu', here we use 'cpp' to the Colab's syntax highlight working...\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "  int deviceCount;\n",
        "  cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "  for (int dev = 0; dev < deviceCount; dev++){\n",
        "\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, dev);\n",
        "\n",
        "    printf(\"Device properties of GPU card ID %d (%s)\\n\", dev, deviceProp.name);\n",
        "    printf(\"  Compute capability: %d.%d\\n\", deviceProp.major, deviceProp.minor);\n",
        "    printf(\"  Warp size: %d\\n\", deviceProp.warpSize);\n",
        "    printf(\"  ------\\n\");\n",
        "    printf(\"  Maximum GPU global memory size: %zu bytes\\n\", deviceProp.totalGlobalMem);\n",
        "    printf(\"  Maximum GPU constants memory size: %zu bytes\\n\", deviceProp.totalConstMem);\n",
        "    printf(\"  Maximum shared memory available per block: %zu bytes\\n\", deviceProp.sharedMemPerBlock);\n",
        "    printf(\"  Maximum shared memory available per SM: %zu bytes\\n\", deviceProp.sharedMemPerMultiprocessor);\n",
        "    printf(\"  ------\\n\");\n",
        "    printf(\"  Maximum size of each dimension of a grid: %d x %d x %d\\n\", deviceProp.maxGridSize[0], deviceProp.maxGridSize[1], deviceProp.maxGridSize[2]);\n",
        "    printf(\"  Maximum size of each dimension of a block: %d x %d x %d\\n\", deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1], deviceProp.maxThreadsDim[2]);\n",
        "    printf(\"  Maximum number of threads per block: %d\\n\", deviceProp.maxThreadsPerBlock);\n",
        "    printf(\"  Number of 32-bit registers available per block: %d\\n\", deviceProp.regsPerBlock);\n",
        "    printf(\"  ------\\n\");\n",
        "    printf(\"  Number of Streaming Multiprocessors: %d\\n\", deviceProp.multiProcessorCount);\n",
        "    printf(\"  Maximum number of resident blocks per SM: %d\\n\", deviceProp.maxBlocksPerMultiProcessor);\n",
        "    printf(\"  Maximum resident threads per SM: %d\\n\", deviceProp.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  Number of 32-bit registers available per SM: %d\\n\", deviceProp.regsPerMultiprocessor);\n",
        "\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "z5t5V_0ic-iM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5b49fb-d505-407f-e50a-c811594ffb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deviceProperties.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv deviceProperties.cpp deviceProperties.cu\n",
        "!nvcc deviceProperties.cu -run"
      ],
      "metadata": {
        "id": "MIl3-VhBfYPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8813c2f3-27e7-48a0-a70d-e83985ded82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device properties of GPU card ID 0 (Tesla T4)\n",
            "  Compute capability: 7.5\n",
            "  Warp size: 32\n",
            "  ------\n",
            "  Maximum GPU global memory size: 15828320256 bytes\n",
            "  Maximum GPU constants memory size: 65536 bytes\n",
            "  Maximum shared memory available per block: 49152 bytes\n",
            "  Maximum shared memory available per SM: 65536 bytes\n",
            "  ------\n",
            "  Maximum size of each dimension of a grid: 2147483647 x 65535 x 65535\n",
            "  Maximum size of each dimension of a block: 1024 x 1024 x 64\n",
            "  Maximum number of threads per block: 1024\n",
            "  Number of 32-bit registers available per block: 65536\n",
            "  ------\n",
            "  Number of Streaming Multiprocessors: 40\n",
            "  Maximum number of resident blocks per SM: 16\n",
            "  Maximum resident threads per SM: 1024\n",
            "  Number of 32-bit registers available per SM: 65536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **BEST** way to write a CUDA program is to:\n",
        "- query the device for the available shared resource per block, warp size, etc.\n",
        "- pick your block size and amount of shared memory accordingly\n",
        "\n",
        "This way of doing things, however, gets quickly out of control in terms of added complexity.\n",
        "So, this notebook will stick to plain and simple hardcoded examples."
      ],
      "metadata": {
        "id": "6fYGjtgCMTPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2**"
      ],
      "metadata": {
        "id": "uhCMbl630JM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hardware questions, if a streaming multiprocessor (SM) has resource for:\n",
        "- 32 threads per warp <!--(SIMD with 32 execution context at once)-->\n",
        "- 2048 ($2^{11}$) threads\n",
        "- 65536 ($2^{16}$) 32-bit registers\n",
        "- at most 255 registers per thread\n",
        "\n",
        "At maximum occupancy, how many registers does each execution context have? <!--this question is more like \"what is an execution context\", after you understand that it is a thread, the answer is clearly 32--><br>\n",
        "Up to how many threads can execute the same instruction simultaneously (on different data)? <!--32, that's a warp-->\n",
        "\n",
        "*Note: use more registers than are left in the SM and it will cause spilling of registers to local memory, significantly slowing threads down.*\n",
        "\n",
        "---\n",
        "\n",
        "These above are the specs for an A100, that has 108 SMs.\n",
        "\n",
        "Suppose we have a kernel requesting 1000 blocks, each with 128 threads.\n",
        "How does it get executed?\n",
        "\n",
        "On current hardware, we would probably get 8-12 blocks running at the same time on each SM, and each block has 4 warps, resulting in 32-48 warps running on each SM.<br>\n",
        "The goal is to use all SMs evenly, since only a few warps run at once per SM (one per sub-core).<br>\n",
        "In any case, a SM can support at most 32 simultaneus blocks.\n",
        "\n",
        "---\n",
        "\n",
        "Assume now a device that allows up to 32 blocks per SM and 2048 threads per SM.\n",
        "Indicate which of the following assignments per SM are possible.\n",
        "In the cases in which it is possible, indicate the occupancy level.\n",
        "1) 8 blocks with 128 threads each <!--yes, 8*128/2048=0.5 occupancy-->\n",
        "2) 32 blocks with 32 threads each <!--yes, 32*32/2048=0.5 occupancy-->\n",
        "3) 64 blocks with 32 threads each <!--no, would have been 64*32/2048=1.0 occupancy-->\n",
        "4) 32 blocks with 64 threads each <!--yes, 64*32/2048=1.0 occupancy-->\n",
        "\n",
        "The same device has 65536 ($2^{16}$) registers per SM. Assume that the kernel being run uses 128 registers per thread and no additional memory.\n",
        "Indicate which of the following assignments per SM are possible.\n",
        "In the cases in which it is possible, indicate the occupancy level.\n",
        "1) 8 blocks with 128 threads each <!--no, requires 2^17 registers-->\n",
        "2) 16 blocks with 32 threads each <!--yes, 16*32/2048=0.25 occupancy-->\n",
        "3) 32 blocks with 32 threads each <!--no, requires 2^17 registers-->\n",
        "4) 8 blocks with 64 threads each <!--yes, 8*64/2048=0.25 occupancy-->\n",
        "\n",
        "*Note: underoccupancy can also arise from registers being oversubscribed. Registers are a fairly limited resource in modern GPUs!*"
      ],
      "metadata": {
        "id": "fFwpCMpl0LxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3**"
      ],
      "metadata": {
        "id": "s2ZLrun5GbZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**\n",
        "\n",
        "For a vector addition (element-wise sum), assume that the vector length is 8000, each thread calculates one output element, and the thread block size is 1024 threads. The programmer configures the kernel launch to have a minimal number of thread blocks to cover all output elements.\n",
        "\n",
        "How many threads will be in the grid?\n",
        "\n",
        "<!--\n",
        "8 blocks, 8192 threads.\n",
        "-->\n",
        "\n",
        "**Question 2:**\n",
        "\n",
        "We want to use each thread in a kernel to calculate two (adjacent) output elements of a vector addition. That is, we use half as many threads as vector elements.\n",
        "\n",
        "What would be the expression for mapping the thread/block indices to data index for the two elements to be added?\n",
        "\n",
        "<!--\n",
        "data_idx_1 = (blockIdx.x * blockDim.x + threadIdx.x) * 2\n",
        "data_idx_2 = (blockIdx.x * blockDim.x + threadIdx.x) * 2 + 1\n",
        "-->\n",
        "\n",
        "**Question 3:**\n",
        "\n",
        "We want to use each thread in a kernel to calculate two output elements of a vector addition. Each thread block processes 2*blockDim.x consecutive elements that form two sections. All threads in each block will first process a section, each processing one element. They will then all move to the next section, again each processing one element. You can assume blockDim.x is a multiple of two.\n",
        "\n",
        "What would be the expression for mapping the thread/block indices to data index for the two elements to be added?\n",
        "\n",
        "If 'n' is the length of the vector and the thread block size is 1024 threads, what does the execution configuration ('<<<...>>>') look like?\n",
        "\n",
        "<!--\n",
        "data_idx_1 = blockIdx.x * blockDim.x * 2 + threadIdx.x\n",
        "data_idx_2 = blockIdx.x * blockDim.x * 2 + threadIdx.x + blockDim.x / 2\n",
        "\n",
        "<<ceil(N/(1024*2)), 1024>>\n",
        "-->\n",
        "\n",
        "**Question 4:**\n",
        "\n",
        "A kernel is launched with '<<<(16, 16), (24, 12)>>>' on a device with warpSize = 32.\n",
        "\n",
        "Write down the threadIdx.x and threadIdx.y at which the 2$^{nd}$ and 3$^{rd}$ warp of each block start and end.\n",
        "\n",
        "<!--\n",
        "Indices are given starting from 0.\n",
        "warp 2: [2, 7] -> [4, 2]\n",
        "warp 3: [4, 3] -> [6, 11]\n",
        "warp 4: [7, 0] -> ...\n",
        "-->\n",
        "\n",
        "**Question 5:**\n",
        "\n",
        "A kernel is launched with '<<<(16, 8, 16), (6, 4, 8)>>>' on a device with warpSize = 32.\n",
        "\n",
        "Write down the threadIdx.x, threadIdx.y, and threadIdx.z at which the 1$^{st}$ and 2$^{nd}$ warp of each block start and end.\n",
        "\n",
        "*Recall: threads are always assigned to warps in row-major order, with x first, then y, then z.*\n",
        "\n",
        "<!--\n",
        "Indices are given starting from 0.\n",
        "warp 1: [0, 0, 0] -> [1, 1, 1]\n",
        "warp 2: [2, 1, 1] -> [3, 2, 2]\n",
        "warp 3: [4, 2, 2] -> ...\n",
        "-->\n",
        "\n",
        "<!--\n",
        "Reasoning to find the index:\n",
        "- where has left off the block before me?\n",
        "- plus what I need to work on in my block.\n",
        "\n",
        "Trick for blocks count:\n",
        "- take the input size, add to it the number of inputs each block handles, minus 1, divide by the inputs per block to get a quick and dirty ceiling\n",
        "-->"
      ],
      "metadata": {
        "id": "7Bjn38n7IMOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4**"
      ],
      "metadata": {
        "id": "-xaORW1DOgQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are to process an 800x600 picture (800 pixels in width/x, 600 pixels in height/y) with the following `dumbPictureKernel()`.<br>\n",
        "That is m = 600 and n = 800, with one thread per pixel.\n",
        "\n",
        "Assume it is executed with blocks of 16x16 threads and warps of 32 threads:\n",
        "1) How many warps will be generated during the execution of the kernel?\n",
        "2) How many warps will have control divergence?\n",
        "3) With m = 800 and n = 599, how many warps have control divergence?\n",
        "4) With m = 799 and n = 600, how many warps have control divergence?\n",
        "\n",
        "<!--\n",
        "1) each block has 256 threads, that is exactly 8 warps; we need ceil(800/16)*ceil(600/16)*8 blocks, thus 1900*8 = 15200.\n",
        "From another perspective, we issue 800x608 threads to cover the whole picture, exactly divided in warps of 32.\n",
        "2) threads are packed in warps in row-major order (x-first). The width is exactly divisible by 16, so it never triggers the boundary condition in any block nor warp. The height has the last row of blocks that are on the boundary. The 8 warps of a block span 2 rows each. From point (1) we know that we use 608 rows of threads, of which only the first 600 are inside the boundary. Therefore, 8 rows of threads are outside the boundary, that is exactly 4 warps per block in the last row of blocks. Hence, no control divergence, only inactive warps.\n",
        "3) Following from the answer to (2), there are now 9 rows of threads outside the boundary, resulting in 2 and 1/2 warps outside. Thus, exactly half of the threads in one warp diverge for every column of blocks, that is 50 diverged warps.\n",
        "4) From the reasoning in (2), the height does not cause any divergence, either a warp is entirely inside or outside the boundary. Along x, now, the last column of warps all diverge, and with a warp being 2 rows tall, that is 300 warps diverging for 2 threads out of 32. It is not 304 because the last 4 warps (or 8 rows) are in any case entirely outside the height's boundary.\n",
        "\n",
        "Note: 'n' is the bound along columns, and columns are along x.\n",
        "-->"
      ],
      "metadata": {
        "id": "PnRo7zkvOkDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void dumbPictureKernel(float* d_Pin, float* d_Pout, int n, int m) {\n",
        "  // calculate the row idx of the d_Pin and d_Pout element to process\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  // calculate the column idx of the d_Pin and d_Pout element to process\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  // each thread computes one element of d_Pout if in range\n",
        "  if ((row < m) && (col < n)) {\n",
        "    d_Pout[row*n + col] = 2*d_Pin[row*n + col];\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "zh0e8Y_iOjqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This example is more common than you may think:**\n",
        "\n",
        "A typical reason for using a control construct with thread divergence is handling boundary conditions when mapping threads to data.\n",
        "This is because the total number of threads needs to be a multiple of the block size whereas the size of the data can be an arbitrary number.\n",
        "\n",
        "Fortunately, as the input size grows (1D - linearly, 2D - quadratically, 3D - cubically), the boundary becomes smaller (1D - no growth, 2D - linearly, 3D - quadratically).\n",
        "Think how the perimeter of a square is always less than its area, even moreso when the sidelength grows.\n",
        "This means that, as we scale up, divergence affects a smaller fraction of warps, resulting in a lower overhead percentage."
      ],
      "metadata": {
        "id": "EHrfIrEM1iBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 5**"
      ],
      "metadata": {
        "id": "XOpnAE1XnMy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following vector addition program, estimate what impact will have the serialization due to control divergence (aka warp divergence). Assume warps of 32 threads.\n",
        "\n",
        "Quantify:\n",
        "- the percentage of warps encoutering divergence\n",
        "- the SIMD efficiency for the addition operation in the kernel (# threads doing it in parallel / # threads not doing it in parallel) in each diverged warp\n",
        "\n",
        "<!--\n",
        "We have 4 blocks of 256 threads, 1024 total threads for 1000 elements of input, so the boundary condition triggers only for those last 24 threads = 1 warp.\n",
        "Since 32 divides 256 exactly, in total we have 4*256/32 warps, the divergence percentage is 1/32.\n",
        "In that sole diverged warp, 8 out of 32 threads will run the sum, others will be idle, so the SIMD efficiency is 8/32.\n",
        "-->"
      ],
      "metadata": {
        "id": "-tN7sfkNnPDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd2.cpp\n",
        "#include<stdio.h>\n",
        "\n",
        "__global__\n",
        "void vecAddKernel(float* A, float* B, float* C, int n) {\n",
        "  int i = blockDim.x*blockIdx.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int n = 1000;\n",
        "\n",
        "  // timing\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  float *a;\n",
        "  float *b;\n",
        "  float *c;\n",
        "\n",
        "  cudaMallocManaged(&a, n*sizeof(float));\n",
        "  cudaMallocManaged(&b, n*sizeof(float));\n",
        "  cudaMallocManaged(&c, n*sizeof(float));\n",
        "\n",
        "  // arbitrary initialization\n",
        "  for (float i = 0; i < n; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i + 5;\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  vecAddKernel<<<4, 256>>>(a, b, c, n);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  // unfortunately, this won't pick up on the slight overhead of divergence...\n",
        "  printf(\"Execution time (ms): %f \\n\", milli);\n",
        "\n",
        "  cudaEventDestroy(stop);\n",
        "  cudaFree(a);\n",
        "  cudaFree(b);\n",
        "  cudaFree(c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "AsH0RImKn6XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b52f70-7875-46e1-ab43-810fa554c9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd2.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vecadd2.cpp vecadd2.cu\n",
        "!nvcc -arch=sm_75 vecadd2.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTCNZ6xqt9co",
        "outputId": "82a74f3c-b56f-48f2-978a-dd72fbda8254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time (ms): 0.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How divergence occurs:\n",
        "- small branch (like above): all threads execute both branches with masking\n",
        "- big branches (~10+ instructions each): the compiler inserts \"voting instructions\" to speedup the case where all threads go in the same branch, if that does not happen, the cost is again the sum of both branches\n",
        "\n",
        "Branches are handled per-warp, so the maximum throughput penalty is 1/32x on a A100 if ony one thread goes down a different path in every warp.\n",
        "\n",
        "Workarounds:\n",
        "- determine at compile time that all threads in the warp must go the same way (thank you very much)\n",
        "- if you can know a priori which part of the input takes which branch, create two or more kernels, each executing one branch, and launch them on their respective inputs\n",
        "\n",
        "Example:<br>\n",
        "Processing a long list of elements where, depending on run-time values, a few require very expensive processing.<br>\n",
        "GPU implementation:<br>\n",
        "First process the list to build two sub-lists of \"simple\" and \"expensive\" elements, then process the two sub-lists separately. You need 3 kernels, one per element type and one to filter the list.\n",
        "\n",
        "*Note: none of this is new, this is what we did 35 years ago on CRAY machines.*"
      ],
      "metadata": {
        "id": "C-WiW6j5jXt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 6**"
      ],
      "metadata": {
        "id": "INe2-F1gtBAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a vector, double the value of its elements in even position, while put to zero the elements in odd position if they are less than 1.0, otherwise leave them unchanged.\n",
        "\n",
        "This kernel implements the optimization proposed in Exercise 5.\n",
        "Therefore it replaces a divergence-inducing branch with two kernels, one operating on odd and one on even position elements.\n",
        "\n",
        "Questions:\n",
        "1) what would have been the impact of control divergence if we had only a single kernel doing both tasks with a `if` checking if the current index was odd/even? Assume warps of 32 threads.\n",
        "<!--\n",
        "Every single warp would have diverged.\n",
        "-->\n",
        "2) could we do the same \"split the kernel in one per branch\" trick to avoid divergence due to the `if(v[i] < 1.0)` check?\n",
        "<!--\n",
        "No, the trick is applicable only on boundary conditions, that is checks strictly dependent on the thread's id/position in the grid, block, or warp.\n",
        "-->"
      ],
      "metadata": {
        "id": "e8go_ZKXtHeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd2.cpp\n",
        "#include<stdio.h>\n",
        "\n",
        "__global__\n",
        "void evenElements(float* v, int n) {\n",
        "  int i = blockDim.x*2*blockIdx.x + threadIdx.x*2;\n",
        "  if (i < n)\n",
        "    v[i] = v[i]*2;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void oddElements(float* v, int n) {\n",
        "  int i = blockDim.x*2*blockIdx.x + threadIdx.x*2 + 1;\n",
        "  if (i < n)\n",
        "    if (v[i] < 1.0)\n",
        "      v[i] = 0.0;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // HP: let n always be even\n",
        "  int n = 1000;\n",
        "  int numThreads = (n + 1)/2;\n",
        "  int blockSize = 32;\n",
        "  int numBlocks = (numThreads + blockSize - 1)/blockSize;\n",
        "\n",
        "  float *v;\n",
        "  cudaMallocManaged(&v, n*sizeof(float));\n",
        "  for (float i = 0; i < n; i++)\n",
        "    v[i] = i/10;\n",
        "\n",
        "  evenElements<<<numBlocks, blockSize>>>(v, n);\n",
        "  oddElements<<<numBlocks, blockSize>>>(v, n);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaFree(v);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "d3bXuKiitHMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vecadd2.cpp vecadd2.cu\n",
        "!nvcc -arch=sm_75 vecadd2.cu -run"
      ],
      "metadata": {
        "id": "SL3v1yiKtSfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A big catch:\n",
        "\n",
        "Since per-thread work is extremely tiny, in this case the divergence penalty is very small. On the other hand, doing two global sweeps, complete with kernel launch and all, adds enough overhead to dominate over the benefit of not diverging.\n",
        "\n",
        "While this trick of creating multiple kernel is useful, reserve it for when the amount of work along either branch is quite substantial.\n",
        "\n",
        "*Note: no need to synchronize between kernel calls, all CUDA activity issued to a single device will not overlap in any way (unless using CUDA streams).*"
      ],
      "metadata": {
        "id": "nR5NqJr6zyLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 7**"
      ],
      "metadata": {
        "id": "Vg61LYb-DVxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following two kernels, estimate what impact will have the serialization due to control divergence (assume warps of 32 threads).\n",
        "\n",
        "In both kernels, we assign to an output element the maximum that can be found between its matching input position an a certain number of positions ahead.\n",
        "\n",
        "<!--\n",
        "1D case:\n",
        "We have 4 blocks each with 256 threads.\n",
        "All threads run the loop for the same number of iterations, except the last 16 of the very last block.\n",
        "So we have a 16/1024 diverged threads.\n",
        "Note however that each thread diverges by a different amount, the very last diverges for all iterations (\"wasting\" them), while the second-to-last only for one less iteration, and so on.\n",
        "A warp is 32 threads, and since 32 divides exactly the number of threads per block (256), diverged threads will be together in the last warp.\n",
        "Hence, we have 1/(4*(256/32)) divergence ration.\n",
        "\n",
        "3D case:\n",
        "We have 4*4*4 blocks each of 256*256*256 threads.\n",
        "Here, each thread looks at a \"cube\" DxDxD ahead of itself in all directions.\n",
        "We have divergence whenever a thread has its \"cube\" go out of the input tensor's boundary.\n",
        "Hence we will have a box of DxNxN threads diverging because of x, another identical box of diverging threads because of y, and so for z. Those boxed however overlap pair-wise and also all 3 in a corner, so we have actually 3*(D*N*N) - 3*(D*D*N) + (D*D*D) = 49549312 diverged threads.\n",
        "For warps, we have 256^3/32 of them per block, all unfolding along the x dimension (row-major), and since they divide blockDim.x exactly, no warp \"wraps back\" to a different y or z. This means that each warp is a thin box \"32x1x1\" and 4 of them on top of each other span the whole width of a block.\n",
        "So, 1xNxN warps will diverge because of x, N/32xDxN because of y, and N/32xNxD because of z, those boxes again overlap, so the final count is: 1*N*N + N/32*D*N + N/32*N*D - 1*D*N - 1*N*D - N/32*D*D + 1*D*D = 2056448 diverged warp out of N*N*N/32.\n",
        "-->"
      ],
      "metadata": {
        "id": "SEsfwEveDbsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define N = 1024\n",
        "#define D = 16\n",
        "\n",
        "__global__ void maxInNextElements(float* v, float* o, int n) {\n",
        "  int g_id = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int limit = min(D, gridDim.x*blockDim.x - g_id);\n",
        "\n",
        "  float res = 0.0;\n",
        "  for (int i = 0; i < limit; i += 1) {\n",
        "    res = max(res, v[g_id + i]);\n",
        "  }\n",
        "  o[g_id] = res;\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  float v[N], o[N];\n",
        "  //...\n",
        "  maxInNextElements<<<4, N/4>>>(v, o, N);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "LVv5bRmPDcBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define N = 1024\n",
        "#define D = 16\n",
        "\n",
        "__global__ void maxInNextElements(float* v, float* o, int n) {\n",
        "  int g_id_x = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int g_id_y = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int g_id_z = blockIdx.z*blockDim.z + threadIdx.z;\n",
        "  int limit_x = min(D, gridDim.x*blockDim.x - g_id_x);\n",
        "  int limit_y = min(D, gridDim.y*blockDim.y - g_id_y);\n",
        "  int limit_z = min(D, gridDim.z*blockDim.z - g_id_z);\n",
        "\n",
        "  float res = 0.0;\n",
        "  // this looks \"a cube\" ahead\n",
        "  for (int i = 0; i < limit_x; i += 1) {\n",
        "    int x = g_id_x + i;\n",
        "    for (int j = 0; j < limit_y; j += 1) {\n",
        "      int y = g_id_y + j;\n",
        "      for (int k = 0; k < limit_z; k += 1) {\n",
        "        int z = g_id_z + k;\n",
        "        res = max(res, v[x + y*N + z*N*N]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  o[g_id_x + g_id_y*N + g_id_z*N*N] = res;\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  float v[N*N*N], o[N*N*N];\n",
        "  //...\n",
        "  maxInNextElements<<<(4, 4, 4), (N/4, N/4, N/4)>>>(v, o, N);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "sfsYlupx4Y8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 8**"
      ],
      "metadata": {
        "id": "QE5eNfgYpPkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following kernel takes a vector and updates each element as the average of itself, its two following neighbors, and its two preceeding neighbors.\n",
        "If an element is on the boundary of the vector, outside elements are considered to be 0.\n",
        "\n",
        "Crucially, the kernel is sped-up by loading data in shared memory for reuse among threads in the same block.\n",
        "The canonical term to refer to reuse in this case is \"halo\", because if you imagine to run the kernel sequentially over the vector, left to right, as you move you would be continously reusing 4-out-of-5 elements that you just used for the immediately prior computation, that is, your input at each vector position continously overlaps with the \"lingering shadow\" of that of the previous one.\n",
        "\n",
        "Be wary that the shared memory allocation needs to be slightly larger than the number of elements handled by the block.\n",
        "It needs to store 4 extra values, 2 below and 2 above the current block."
      ],
      "metadata": {
        "id": "oSa3o5usPNoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "#include<stdio.h>\n",
        "\n",
        "__global__\n",
        "void neighborsAverage(const float* __restrict__ in, float* __restrict__ out, int n) {\n",
        "  extern __shared__ float s[];\n",
        "  int t_id = threadIdx.x;\n",
        "  int g_t_id = blockIdx.x*blockDim.x + t_id;\n",
        "\n",
        "  // load into shared memory with halo of 2 on each side\n",
        "  int l_idx = t_id + 2; // make space for the first two elements\n",
        "  if (g_t_id < n)\n",
        "    s[l_idx] = in[g_t_id];\n",
        "  else\n",
        "    s[l_idx] = 0.0f;\n",
        "\n",
        "  // load left halo\n",
        "  if (t_id < 2) {\n",
        "    int left_idx = g_t_id - 2 + t_id;\n",
        "    s[t_id] = (left_idx >= 0) ? in[left_idx] : 0.0f;\n",
        "  }\n",
        "  // load right halo\n",
        "  if (t_id >= blockDim.x - 2) {\n",
        "    int right_idx = g_t_id + (t_id - (blockDim.x - 2)) + 1;\n",
        "    s[l_idx + 2] = (right_idx < n) ? in[right_idx] : 0.0f;\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  // 5 elements average\n",
        "  if (g_t_id < n) {\n",
        "    out[g_t_id] = (s[l_idx-2] + s[l_idx-1] + s[l_idx] + s[l_idx+1] + s[l_idx+2]) / 5.0f;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int n = 4096;\n",
        "  float h_in[n], h_out[n];\n",
        "  for (int i = 0; i < n; i++)\n",
        "    h_in[i] = i + 1;\n",
        "\n",
        "  float *d_in, *d_out;\n",
        "  cudaMalloc(&d_in, n*sizeof(float));\n",
        "  cudaMalloc(&d_out, n*sizeof(float));\n",
        "  cudaMemcpy(d_in, h_in, n*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  int blockSize = 256;\n",
        "  int gridSize = (n + blockSize - 1)/blockSize;\n",
        "  size_t shmem = (blockSize + 4) * sizeof(float); // +4 for halos\n",
        "  neighborsAverage<<<gridSize, blockSize, shmem>>>(d_in, d_out, n);\n",
        "  cudaMemcpy(h_out, d_out, n*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i = 0; i < n; i++)\n",
        "    printf(\"%6.2f \", h_out[i]);\n",
        "  printf(\"\\n\");\n",
        "\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "yCHFiHVJPN6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If, as shown, we launch the kernel with blocks of 256 threads, and the vector has length 4096, what is the impact of control divergence? Assume 32 threads per warp.\n",
        "\n",
        "<!--\n",
        "We have 32 blocks, the division is exact, the 'g_t_id < n' condition does not cause divergence.\n",
        "In each block, the first two and last two threads take the \"left halo\" and \"right halo\" branches respectively, doing an extra shared memory load.\n",
        "So we have 4 diverged threads per block: 4*32/4096 thread divergence ratio.\n",
        "These threads end up in the first and last warp of each block.\n",
        "Each block has 256/32 = 8 warps, therefore we have a 2/8 warp divergence ratio.\n",
        "Moreover, each warp diverges by 2/32 threads.\n",
        "-->\n",
        "\n",
        "*Note: here divergence arises from the extra loads in shared memory!*"
      ],
      "metadata": {
        "id": "w6aG0owxR9g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 9**"
      ],
      "metadata": {
        "id": "cVRuiHZVwKKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following code snippet, representing a (sub-optimal) matrix multiplication kernel:"
      ],
      "metadata": {
        "id": "hlDLePkAwMvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "// HP: square matrices of side length 'n'\n",
        "__global__\n",
        "void MatrixMulKernel(float* A, float* B, float* C, int n) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  if ((row < n) && (col < n)) {\n",
        "    float partial_out = 0;\n",
        "    for (int k = 0; k < n; ++k) {\n",
        "      partial_out += A[row*n + k] * B[k*n + col];\n",
        "    }\n",
        "    C[row*n + col] = partial_out;\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "KIzS_lc5xQ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we launch the kernel with a block size of 16x16 on 1000x1000 matrices, how many warps will have control divergence?\n",
        "Assume warpSize = 64 (fictional, but you never know...).\n",
        "\n",
        "<!--\n",
        "The boundary condition is on the global 'row' and 'col' indices.\n",
        "ceil(1000/16) = 63 blocks, 63*16 = 1008 threads along both x and y.\n",
        "In each block we have 4 warps, each spanning 4 rows.\n",
        "Along y, only the last 8 rows are outside the boundary, that means 2 warps per column are fully outside, no divergence.\n",
        "Along x, the last column of blocks is vertically cut in half inside and half ouside the boundary, thus every warp is subject to divergence except warps that are also entirely outside along y too. With one warp every 4 rows, there are 252 warps in the last column of blocks, the last 2 are ouside the y boundary, so 250 warps will diverge.\n",
        "-->\n",
        "\n",
        "With an SM that can support up to 1,536 threads and up to 8 blocks, which of the following block configuration would result in the most number of threads in each SM: 64, 128, 512 or 1024 threads per block?\n",
        "\n",
        "<!--\n",
        "1536/64 = 24, not good, > 8\n",
        "1536/128 = 12, not good, > 8\n",
        "1536/512 = 3, the only viable option to use all threads\n",
        "1536/1024 = 1.5, not good, does not divide threads exactly (blocks are either assigned entirely or not assigned to a SM)\n",
        "-->"
      ],
      "metadata": {
        "id": "0w8wc5FBxaPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little visual help:\n",
        "\n",
        "<!--<img src=\"https://polimi365-my.sharepoint.com/:i:/r/personal/10669641_polimi_it/Documents/Images/blocks_over_input.png\" alt=\"2D visualization of the input, blocks, and warps.\" width=\"500\" border=\"2\">-->\n",
        "<img src=\"https://i.imgur.com/fNjRp2u.png\" alt=\"2D visualization of the input, blocks, and warps.\" width=\"500\" border=\"2\">\n",
        "\n",
        "*Note: ops, \"input\" should have been \"output\", well, too late for that.*"
      ],
      "metadata": {
        "id": "a6CidfW9oJ1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 10**"
      ],
      "metadata": {
        "id": "lOersQlA8xQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**\n",
        "\n",
        "Assume that each atomic operation in a DRAM system has a total latency of 100ns.\n",
        "What is the maximal throughput we can get for atomic operations on the same global memory variable?\n",
        "\n",
        "<!--\n",
        "Explanation: No other atomic operation can touch the same variable for the entire duration of 100ns.\n",
        "The maximal rate is 1/100ns = 0.01G atomic operations per second.\n",
        "-->\n",
        "\n",
        "Assuming now that the kernel does <!--(total over all threads)--> 5 floating-point operations per atomic operation, what is the maximal floating-point throughput of the kernel execution as limited by the throughput of the atomic operations?\n",
        "\n",
        "<!--\n",
        "The maximal is 5 operations every 100ns, 5/100ns = 0.05 GFLOPS.\n",
        "-->\n",
        "\n",
        "**Question 2:**\n",
        "\n",
        "For a GPU that supports atomic operations in L2 cache, assume that each atomic operation takes 4ns to complete in L2 cache and 100ns to complete in DRAM.\n",
        "Assume that 90% of the atomic operations hit in L2 cache. What is the approximate throughput for atomic operations on the same global memory variable?\n",
        "\n",
        "<!--\n",
        "The average latency is 4ns * 90% + 100ns * 10% = 13.6ns.\n",
        "The average throughput is approximately 1/13.6ns = 0.0735G atomic operations per second.\n",
        "-->\n",
        "\n",
        "**Question 3:**\n",
        "\n",
        "Continuing from Question 1.\n",
        "\n",
        "Assume that we privatized some global memory variables into shared memory variables, and the shared memory access latency is 1ns.\n",
        "For simplicity, assume that the additional global memory atomic operations for accumulating privatized variable into the global variable adds 10% to the total execution time.\n",
        "Assume that a kernel performs 5 floating-point operations per atomic operation. What is the maximal floating-point throughput of the kernel execution as limited by the throughput of the atomic operations?\n",
        "\n",
        "<!--\n",
        "The effective throughput without the final accumulation to the global variable is 5/1ns = 5 GFLOPS.\n",
        "Since the time is stretched by 10%, the final effective throughput is approximately 5/1.1 = 4.5 GFLOPS.\n",
        "-->"
      ],
      "metadata": {
        "id": "WXkGW8AO80qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 11**"
      ],
      "metadata": {
        "id": "oU-S1BxF-PEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following basic reduction kernel code fragment:"
      ],
      "metadata": {
        "id": "ue3VHRFD-R6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "int t = threadIdx.x;\n",
        "int start = 2*blockDim.x*blockIdx.x; // each block of threads starts with twice the elements as there are threads, that is to have one sum per thread in the first reduction round/level\n",
        "extern __shared__ float partial_sum[];\n",
        "partial_sum[t] = input[start + t]; // each thread sets up two values, ultimately all values are preloaded in 'partial_sum'\n",
        "partial_sum[blockDim.x + t] = input[start + blockDim.x + t];\n",
        "\n",
        "for (int stride = 1; stride <= blockDim.x; stride *= 2) {\n",
        "  __syncthreads();\n",
        "  if (t % stride == 0) // one thread every 'stride' sums its corresponding value and the next one from the previous round\n",
        "    partial_sum[2*t] += partial_sum[2*t + stride];\n",
        "}"
      ],
      "metadata": {
        "id": "F6Vv8zho-ntq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the block size be 1024 and the warp size be 32.\n",
        "Assume the length of 'input' to always be a multiple of the block size.\n",
        "\n",
        "Questions:\n",
        "1) how many warps in a block will have divergence during the iteration where stride = 1?\n",
        "<!--\n",
        "With stride = 1, the condition 't % stride == 0' is always true, no divergence occurs.\n",
        "From another perspective, the kernel is set up such that during the first iteration each thread executes exactly one of the partial sums in the first reduction level.\n",
        "-->\n",
        "2) how many warps in a block will have divergence during the iteration where stride is equal to 16?\n",
        "<!--\n",
        "This is the fifth iteration/reduction level.\n",
        "Only one in sixteen threads will enter the 'if' and perform the sum.\n",
        "For how the reduction is performed that is exactly one thread every sixteen going by thread id, therefore every warp will have two threads taking the branch and 30 not.\n",
        "Every warp diverges.\n",
        "-->\n",
        "3) how many warps in a block will have divergence during the iteration where stride is equal to 64?\n",
        "<!--\n",
        "This is the seventh iteration/reduction level.\n",
        "For the same reasoning as (2), one in 64 threads will take the branch, that is one thread every two warps.\n",
        "Hence, half of the warps diverge.\n",
        "-->"
      ],
      "metadata": {
        "id": "YdnwewD2-l5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 12**"
      ],
      "metadata": {
        "id": "srRrhR6G_CE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the following improved reduction kernel:"
      ],
      "metadata": {
        "id": "Axcdmw6U_IpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "int t = threadIdx.x;\n",
        "int start = 2*blockDim.x*blockIdx.x;\n",
        "extern __shared__ float partial_sum[];\n",
        "partial_sum[t] = input[start + t]; // omitting the boundary check...\n",
        "partial_sum[blockDim.x + t] = input[start + blockDim.x + t];\n",
        "\n",
        "for (int stride = blockDim.x; stride > 0; stride /= 2) { // be wary, here we halve the stride at every iteration, unlike in the previous exercise!\n",
        "  __syncthreads();\n",
        "  if (t < stride) // 'stride' threads sums their corresponding values with those one 'stride' ahead\n",
        "    partial_sum[t] += partial_sum[t + stride];\n",
        "}"
      ],
      "metadata": {
        "id": "wQd5N1p2_Mw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the block size be 1024 and the warp size be 32.\n",
        "Assume the length of 'input' to always be a multiple of the block size.\n",
        "\n",
        "1) how many warps in a block will have divergence during the iteration where stride is equal to 16?\n",
        "<!--\n",
        "This is the seventh iteration.\n",
        "For how this version is written (focus on the boundary condition), only the first 'stride' threads in a block take the branch.\n",
        "Therefore, only the first warp in a block will diverge, while also being the sole warp doing useful work.\n",
        "-->\n",
        "2) how many warps in a block will have divergence during the iteration where stride is equal to 64?\n",
        "<!--\n",
        "This is the fifth iteration.\n",
        "Following from (2), only the first 64 threads per block are active.\n",
        "Therefore, the first two warps in a block will fully take the branch, while the others don't do any useful work.\n",
        "No warp diverges.\n",
        "-->"
      ],
      "metadata": {
        "id": "Ons-nuws_R8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 13**"
      ],
      "metadata": {
        "id": "MDPuD2Qx_jJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the work efficient scan kernel based on reduction trees and inverse reduction trees, and focus on the following code fragment:"
      ],
      "metadata": {
        "id": "zuZPOum4_lFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "extern __shared__ float scan_sum[];\n",
        "scan_sum[threadIdx.x*2] = input[blockDim.x*blockIdx.x*2 + threadIdx.x*2]; // omitting the boundary checks...\n",
        "scan_sum[threadIdx.x*2 + 1] = input[blockDim.x*blockIdx.x*2 + threadIdx.x*2 + 1];\n",
        "__syncthreads();\n",
        "\n",
        "// === FOCUS ON THIS ===\n",
        "// == REDUCTION PHASE ==\n",
        "for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "  int index = (threadIdx.x + 1)*stride*2 - 1;\n",
        "  if (index < 2*blockDim.x)\n",
        "    scan_sum[index] += scan_sum[index - stride];\n",
        "  __syncthreads();\n",
        "}\n",
        "// =====================\n",
        "\n",
        "for (int stride = blockDim.x/2; stride > 0; stride /= 2) {\n",
        "  __syncthreads();\n",
        "  int index = (threadIdx.x + 1)*stride*2 - 1;\n",
        "  if (index + stride < 2*blockDim.x)\n",
        "    scan_sum[index + stride] += scan_sum[index];\n",
        "}\n",
        "__syncthreads();\n",
        "result[i] = scan_sum[threadIdx.x]; // omitting the boundary check..."
      ],
      "metadata": {
        "id": "fIy4SVl9_1hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that we give 2048 elements to each block. Each block thus has 1024 threads handling that section of elements.\n",
        "Let warp size be 32.\n",
        "\n",
        "How many warps in each block will have control divergence during the reduction tree phase iteration where stride is 16?\n",
        "\n",
        "<!--\n",
        "The stride doubles at every iteration, along with each thread's 'index'.\n",
        "When stride is 16, the first thread in a block will have index = 31, the second index = 63, and so on until the 32nd (threadIdx.x == 31) thread will have index = 1023.\n",
        "After the 32nd thread, all others will not take the branch.\n",
        "Therefore, not warp diverges, the first warp will do useful work, all others won't take the branch.\n",
        "-->"
      ],
      "metadata": {
        "id": "qad58MKv_4J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 14**"
      ],
      "metadata": {
        "id": "Zf9ZaB_e_-YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the work inefficient scan kernel based on interleaved reduction trees, and focus on the following code fragment:"
      ],
      "metadata": {
        "id": "3XQkk6gsABzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "extern __shared__ float scan_sum[];\n",
        "scan_sum[threadIdx.x] = input[blockDim.x*blockIdx.x + threadIdx.x]; // omitting the boundary checks...\n",
        "__syncthreads();\n",
        "\n",
        "for (int stride = 1; stride < blockDim.x; stride = stride*2) {\n",
        "    __syncthreads();\n",
        "    float tmp = threadIdx.x >= stride ? scan_sum[threadIdx.x - stride] : 0.0;\n",
        "    __syncthreads();\n",
        "    scan_sum[threadIdx.x] += tmp;\n",
        "}"
      ],
      "metadata": {
        "id": "6n7v52BxAKeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that we have 1024 elements in each block and warp size is 32, how many warps per block will have control divergence during the iteration where stride is 16?\n",
        "<!--\n",
        "The stride doubles at every iteration, each time deactivating an additional half of the threads.\n",
        "When stride is 16, all threads but the first 16 are doing active work. Therefore only the first warp per block diverges exactly by half of its threads.\n",
        "-->\n",
        "How many warps in each block will have control divergence during the iteration where stride is 64?\n",
        "<!--\n",
        "From the above, when stride is 64 the first 64 threads will be inactive. This amounts to the entirety of the first two warps, that will collectively take the inactive branch without diverging.\n",
        "-->"
      ],
      "metadata": {
        "id": "RdvT6kQPAnFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Host-Device Data Transfers**"
      ],
      "metadata": {
        "id": "jPRUKt--7zUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a profiler to analyze the performance of three different ways to transfer data from the host to the device:\n",
        "* Manual memory allocation\n",
        "* Unified Memory\n",
        "* Asynchronous memory prefetching\n",
        "* Pinned Memory\n",
        "\n",
        "For a deeper discussion, see https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#memory-optimizations."
      ],
      "metadata": {
        "id": "6nf65zt371Ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Memory Allocation (explicit `cudaMemcpy`)\n",
        "\n",
        "This is our classic baseline with explicit data transfers between host and device."
      ],
      "metadata": {
        "id": "OZaUNa3GP5ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd_manual.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// use '__device__' instead if the value is meant to be modifiable by the kernel\n",
        "__constant__ float extra[1];\n",
        "\n",
        "__global__\n",
        "void vectorAdd(float* add1, float* add2, float* result, int N) {\n",
        "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = id; i < N; i+= stride)\n",
        "    result[i] = add1[i] + add2[i] + *extra;\n",
        "}\n",
        "\n",
        "void initWith(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    vec[i] = number;\n",
        "}\n",
        "\n",
        "void checkResult(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    if (vec[i] != number) {\n",
        "      printf(\"Error at %d: %f != %f\\n\", i, vec[i], number);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 1 << 24;\n",
        "  int blockSize = 1024;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  // host side arrays\n",
        "  float *h_a;\n",
        "  float *h_b;\n",
        "  float *h_c;\n",
        "  float host_extra = 12.0f;\n",
        "\n",
        "  h_a = (float*)malloc(N*sizeof(float));\n",
        "  h_b = (float*)malloc(N*sizeof(float));\n",
        "  h_c = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  initWith(3.0f, h_a, N);\n",
        "  initWith(4.0f, h_b, N);\n",
        "  initWith(0.0f, h_c, N);\n",
        "\n",
        "  // device side arrays\n",
        "  float *d_a;\n",
        "  float *d_b;\n",
        "  float *d_c;\n",
        "\n",
        "  cudaMalloc((void**)&d_a, N*sizeof(float));\n",
        "  cudaMalloc((void**)&d_b, N*sizeof(float));\n",
        "  cudaMalloc((void**)&d_c, N*sizeof(float));\n",
        "\n",
        "  // manual copy\n",
        "  cudaMemcpy(d_a, h_a, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, h_b, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  gpuErrchk(cudaMemcpyToSymbol(extra, &host_extra, sizeof(float), 0, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // kernel\n",
        "  vectorAdd<<<numBlocks, blockSize>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "  //gpuErrchk(cudaPeekAtLastError());\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaMemcpy(h_c, d_c, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  checkResult(7.0f + 12.0f, h_c, N);\n",
        "  printf(\"Manual memory allocation: success!\\n\");\n",
        "\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  free(h_a);\n",
        "  free(h_b);\n",
        "  free(h_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "mv9DOPzB025q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722029dc-0928-48a9-e510-133ddc1809b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd_manual.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Unified Memory (`cudaMallocManaged`)\n",
        "\n",
        "No explicit copies needed, the GPU and CPU share the same allocation."
      ],
      "metadata": {
        "id": "meWx-rTRQS2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd_unified.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void vectorAdd(float* add1, float* add2, float* result, int N) {\n",
        "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = id; i < N; i += stride)\n",
        "    result[i] = add1[i] + add2[i];\n",
        "}\n",
        "\n",
        "void initWith(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    vec[i] = number;\n",
        "}\n",
        "\n",
        "void checkResult(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    if (vec[i] != number) {\n",
        "      printf(\"Error at %d: %f != %f\\n\", i, vec[i], number);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 1 << 24;\n",
        "  const int blockSize = 1024;\n",
        "  const int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  float *a, *b, *c;\n",
        "  cudaMallocManaged(&a, N * sizeof(float));\n",
        "  cudaMallocManaged(&b, N * sizeof(float));\n",
        "  cudaMallocManaged(&c, N * sizeof(float));\n",
        "\n",
        "  initWith(3.0f, a, N);\n",
        "  initWith(4.0f, b, N);\n",
        "  initWith(0.0f, c, N);\n",
        "\n",
        "  vectorAdd<<<numBlocks, blockSize>>>(a, b, c, N);\n",
        "\n",
        "  //gpuErrchk(cudaPeekAtLastError());\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  checkResult(7.0f, c, N);\n",
        "  printf(\"Unified Memory: success!\\n\");\n",
        "\n",
        "  cudaFree(a);\n",
        "  cudaFree(b);\n",
        "  cudaFree(c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D62hsOUP4c0",
        "outputId": "f351baed-47b6-4485-9858-639a09e3940a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd_unified.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unified Memory with Asynchronous Prefetching\n",
        "\n",
        "An improvement over the \"unified memory\" case.<br>\n",
        "Here we tell CUDA in advance to move the memory to the GPU, which avoids stalls during the kernel."
      ],
      "metadata": {
        "id": "jIr0L3IXQ34l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd_prefetch.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void vectorAdd(float* add1, float* add2, float* result, int N) {\n",
        "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = id; i < N; i += stride)\n",
        "    result[i] = add1[i] + add2[i];\n",
        "}\n",
        "\n",
        "void initWith(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    vec[i] = number;\n",
        "}\n",
        "\n",
        "void checkResult(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    if (vec[i] != number) {\n",
        "      printf(\"Error at %d: %f != %f\\n\", i, vec[i], number);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 1 << 24;\n",
        "  const int blockSize = 1024;\n",
        "  const int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  float *a, *b, *c;\n",
        "  cudaMallocManaged(&a, N * sizeof(float));\n",
        "  cudaMallocManaged(&b, N * sizeof(float));\n",
        "  cudaMallocManaged(&c, N * sizeof(float));\n",
        "\n",
        "  initWith(3.0f, a, N);\n",
        "  initWith(4.0f, b, N);\n",
        "  initWith(0.0f, c, N);\n",
        "\n",
        "  int device;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  // explicitly migrate data before the kernel\n",
        "  cudaMemPrefetchAsync(a, N * sizeof(float), device);\n",
        "  cudaMemPrefetchAsync(b, N * sizeof(float), device);\n",
        "\n",
        "  vectorAdd<<<numBlocks, blockSize>>>(a, b, c, N);\n",
        "\n",
        "  //gpuErrchk(cudaPeekAtLastError());\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  // prefetch back to host for validation\n",
        "  cudaMemPrefetchAsync(c, N * sizeof(float), cudaCpuDeviceId);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  checkResult(7.0f, c, N);\n",
        "  printf(\"Unified Memory + Prefetching: success!\\n\");\n",
        "\n",
        "  cudaFree(a);\n",
        "  cudaFree(b);\n",
        "  cudaFree(c);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSypDNOXQ4MF",
        "outputId": "5ae4d0a7-363d-45f0-b94e-2b4727c6fe77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd_prefetch.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinned Memory\n",
        "\n",
        "Pinned (or page-locked) host memory sits between the \"manual copy\" and \"unified memory\" approaches in complexity and performance.<br>\n",
        "It's still manually managed, but it allows asynchronous and faster host-device transfers, because the memory can be accessed directly by the GPU's DMA engine without an extra staging step."
      ],
      "metadata": {
        "id": "1hF7UbsJNYNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: pinned memory should be the fastest manual transfer method, especially when overlapping transfers and computation using streams. May still be inferior to the asynch method depending on the nature and frequency of the transfers.*\n",
        "\n",
        "*Note: pinned memory should not be overused. Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance.*"
      ],
      "metadata": {
        "id": "1louCyIfQypB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd_pinned.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) {\n",
        "   if (code != cudaSuccess) {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort)\n",
        "        exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void vectorAdd(float* a, float* b, float* c, int N) {\n",
        "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = id; i < N; i += stride)\n",
        "    c[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "void initWith(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    vec[i] = number;\n",
        "}\n",
        "\n",
        "void checkResult(float number, float* vec, int size) {\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    if (vec[i] != number) {\n",
        "      printf(\"Error at %d: %f != %f\\n\", i, vec[i], number);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 1 << 24;\n",
        "  const int blockSize = 256;\n",
        "  const int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  // allocate pinned memory\n",
        "  float *h_a, *h_b, *h_c;\n",
        "  gpuErrchk(cudaMallocHost((void**)&h_a, N * sizeof(float)));\n",
        "  gpuErrchk(cudaMallocHost((void**)&h_b, N * sizeof(float)));\n",
        "  gpuErrchk(cudaMallocHost((void**)&h_c, N * sizeof(float)));\n",
        "\n",
        "  initWith(3.0f, h_a, N);\n",
        "  initWith(4.0f, h_b, N);\n",
        "  initWith(0.0f, h_c, N);\n",
        "\n",
        "  // device arrays\n",
        "  float *d_a, *d_b, *d_c;\n",
        "  gpuErrchk(cudaMalloc((void**)&d_a, N * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&d_b, N * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&d_c, N * sizeof(float)));\n",
        "\n",
        "  // asynchronous memory copy using pinned host memory\n",
        "  cudaStream_t stream;\n",
        "  gpuErrchk(cudaStreamCreate(&stream));\n",
        "\n",
        "  gpuErrchk(cudaMemcpyAsync(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice, stream));\n",
        "  gpuErrchk(cudaMemcpyAsync(d_b, h_b, N * sizeof(float), cudaMemcpyHostToDevice, stream));\n",
        "\n",
        "  vectorAdd<<<numBlocks, blockSize, 0, stream>>>(d_a, d_b, d_c, N);\n",
        "  gpuErrchk(cudaGetLastError());\n",
        "\n",
        "  gpuErrchk(cudaMemcpyAsync(h_c, d_c, N * sizeof(float), cudaMemcpyDeviceToHost, stream));\n",
        "\n",
        "  gpuErrchk(cudaStreamSynchronize(stream));\n",
        "  gpuErrchk(cudaStreamDestroy(stream));\n",
        "\n",
        "  checkResult(7.0f, h_c, N);\n",
        "  printf(\"Pinned memory: success!\\n\");\n",
        "\n",
        "  // cleanup\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "  cudaFreeHost(h_a);\n",
        "  cudaFreeHost(h_b);\n",
        "  cudaFreeHost(h_c);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJj39hA6Oisp",
        "outputId": "a45d70ed-7e6d-4c31-d9a6-edfc50eba3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd_pinned.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run!"
      ],
      "metadata": {
        "id": "l7Nw2voUQxhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vecadd_manual.cpp vecadd_manual.cu\n",
        "!mv vecadd_unified.cpp vecadd_unified.cu\n",
        "!mv vecadd_prefetch.cpp vecadd_prefetch.cu\n",
        "!mv vecadd_pinned.cpp vecadd_pinned.cu\n",
        "print(\"\\nCompiling & running:\")\n",
        "!nvcc -arch=sm_75 vecadd_manual.cu -o vecadd_manual -run\n",
        "!nvcc -arch=sm_75 vecadd_unified.cu -o vecadd_unified -run\n",
        "!nvcc -arch=sm_75 vecadd_prefetch.cu -o vecadd_prefetch -run\n",
        "!nvcc -arch=sm_75 vecadd_pinned.cu -o vecadd_pinned -run"
      ],
      "metadata": {
        "id": "kyG7SGso1Szp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5bee61-ae86-47e1-80bf-7427d1a4da18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compiling & running:\n",
            "Manual memory allocation: success!\n",
            "Unified Memory: success!\n",
            "Unified Memory + Prefetching: success!\n",
            "Pinned memory: success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the most indicative metrics are under `cuda_gpu_mem_time_sum`.<br>\n",
        "Additionally, under `cuda_api_sum`, look out for:\n",
        "- `cudaMalloc` + `cudaMemcpy`\n",
        "- `cudaMallocManaged`\n",
        "- `cudaMallocManaged` + `cudaMemPrefetchAsync`\n",
        "\n",
        "Either look for their combined total time or combined percentage of time.<br>\n",
        "In theory, it should reduce as we go down the list."
      ],
      "metadata": {
        "id": "mPFuFKacXG1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n============= PROFILE: vecadd_manual =============\\n\")\n",
        "!nsys profile --stats=true ./vecadd_manual\n",
        "print(\"\\n============= PROFILE: vecadd_unified ============\\n\")\n",
        "!nsys profile --stats=true ./vecadd_unified\n",
        "print(\"\\n============ PROFILE: vecadd_prefetch ============\\n\")\n",
        "!nsys profile --stats=true ./vecadd_prefetch\n",
        "print(\"\\n============ PROFILE: vecadd_pinned ============\\n\")\n",
        "!nsys profile --stats=true ./vecadd_pinned"
      ],
      "metadata": {
        "id": "fheC99GlqS80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dcb9a0-762d-48b6-8dac-8cc7dc12a615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============= PROFILE: vecadd_manual =============\n",
            "\n",
            "Manual memory allocation: success!\n",
            "Generating '/tmp/nsys-report-1c62.qdstrm'\n",
            "[1/8] [========================100%] report1.nsys-rep\n",
            "[2/8] [========================100%] report1.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  -----------  ------------  ----------------------\n",
            "     87.0      452,215,583         13  34,785,814.1  2,391,646.0     50,053  351,681,820  96,066,107.9  poll                  \n",
            "     11.8       61,184,833        546     112,060.1     11,402.0        396   17,274,823     900,088.2  ioctl                 \n",
            "      0.5        2,606,854          1   2,606,854.0  2,606,854.0  2,606,854    2,606,854           0.0  pthread_cond_wait     \n",
            "      0.4        1,972,595         31      63,632.1     10,203.0      7,360    1,310,316     232,239.1  mmap64                \n",
            "      0.1          624,102          9      69,344.7     83,806.0     10,770       91,862      29,717.4  sem_timedwait         \n",
            "      0.1          384,572         49       7,848.4      6,930.0      2,475       16,105       2,835.5  open64                \n",
            "      0.0          233,718         18      12,984.3      5,894.5      1,685       86,082      19,615.2  mmap                  \n",
            "      0.0          229,230         40       5,730.8      3,481.0      1,558       29,606       6,295.5  fopen                 \n",
            "      0.0          113,937         12       9,494.8      6,346.5      2,340       44,603      11,205.5  write                 \n",
            "      0.0           83,763          2      41,881.5     41,881.5     39,227       44,536       3,754.0  pthread_create        \n",
            "      0.0           64,869         11       5,897.2      4,837.0      3,063       11,650       2,597.9  munmap                \n",
            "      0.0           53,775         33       1,629.5      1,111.0        749        6,113       1,252.5  fclose                \n",
            "      0.0           37,702         20       1,885.1         45.0         45       36,784       8,214.4  fgets                 \n",
            "      0.0           33,389         64         521.7        537.5        171        1,161         187.7  fcntl                 \n",
            "      0.0           32,482          6       5,413.7      5,301.0      2,352        8,518       2,220.8  open                  \n",
            "      0.0           25,586         15       1,705.7      1,536.0        531        3,127         676.2  read                  \n",
            "      0.0           21,056          2      10,528.0     10,528.0      6,946       14,110       5,065.7  socket                \n",
            "      0.0           16,118          2       8,059.0      8,059.0      2,027       14,091       8,530.5  pthread_cond_broadcast\n",
            "      0.0           13,504          3       4,501.3      5,196.0      2,715        5,593       1,559.7  pipe2                 \n",
            "      0.0           10,025          1      10,025.0     10,025.0     10,025       10,025           0.0  connect               \n",
            "      0.0            5,898          2       2,949.0      2,949.0      2,910        2,988          55.2  fwrite                \n",
            "      0.0            2,798          8         349.8        337.5        276          475          67.7  dup                   \n",
            "      0.0            1,306          1       1,306.0      1,306.0      1,306        1,306           0.0  bind                  \n",
            "      0.0            1,294          1       1,294.0      1,294.0      1,294        1,294           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  ------------  ----------------------\n",
            "     64.4       85,389,770          3  28,463,256.7      93,966.0      79,294  85,216,510  49,149,759.7  cudaMalloc            \n",
            "     32.0       42,393,003          3  14,131,001.0  14,100,880.0  14,078,317  14,213,806      72,593.2  cudaMemcpy            \n",
            "      1.8        2,423,744          3     807,914.7   1,064,215.0     242,198   1,117,331     490,644.3  cudaFree              \n",
            "      0.9        1,144,168          1   1,144,168.0   1,144,168.0   1,144,168   1,144,168           0.0  cudaMemcpyToSymbol    \n",
            "      0.9        1,132,120          1   1,132,120.0   1,132,120.0   1,132,120   1,132,120           0.0  cudaDeviceSynchronize \n",
            "      0.0           47,560          1      47,560.0      47,560.0      47,560      47,560           0.0  cudaLaunchKernel      \n",
            "      0.0            1,955          1       1,955.0       1,955.0       1,955       1,955           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                    Name                   \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  -----------------------------------------\n",
            "    100.0        1,128,807          1  1,128,807.0  1,128,807.0  1,128,807  1,128,807          0.0  vectorAdd(float *, float *, float *, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  ------------  ------------  ----------  ----------  -----------  ------------------\n",
            "     67.1       27,932,153      3   9,310,717.7  13,874,975.0         672  14,056,506  8,063,246.9  [CUDA memcpy HtoD]\n",
            "     32.9       13,714,915      1  13,714,915.0  13,714,915.0  13,714,915  13,714,915          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "    134.218      3    44.739    67.109     0.000    67.109       38.745  [CUDA memcpy HtoD]\n",
            "     67.109      1    67.109    67.109    67.109    67.109        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report1.nsys-rep\n",
            "    /home/cuda/report1.sqlite\n",
            "\n",
            "============= PROFILE: vecadd_unified ============\n",
            "\n",
            "Unified Memory: success!\n",
            "Generating '/tmp/nsys-report-de42.qdstrm'\n",
            "[1/8] [========================100%] report2.nsys-rep\n",
            "[2/8] [========================100%] report2.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  --------  -----------  ------------  ----------------------\n",
            "     92.9    1,003,784,743         44  22,813,289.6  10,080,465.5     2,548  320,359,645  51,842,279.0  poll                  \n",
            "      5.7       61,598,555        546     112,817.9      10,913.0       411   17,493,078     902,783.6  ioctl                 \n",
            "      1.0       11,020,750         24     459,197.9       9,530.0     1,694    4,367,512   1,220,864.5  mmap                  \n",
            "      0.2        2,148,233         31      69,297.8      12,716.0     7,949    1,374,009     243,430.5  mmap64                \n",
            "      0.1          597,965         10      59,796.5      71,152.0    12,527       88,961      30,514.6  sem_timedwait         \n",
            "      0.0          505,670          1     505,670.0     505,670.0   505,670      505,670           0.0  pthread_cond_wait     \n",
            "      0.0          384,038         49       7,837.5       6,964.0     2,239       16,693       2,886.3  open64                \n",
            "      0.0          278,983         40       6,974.6       4,123.5     1,512       32,070       7,604.1  fopen                 \n",
            "      0.0          110,418          2      55,209.0      55,209.0    45,917       64,501      13,140.9  pthread_create        \n",
            "      0.0           62,484         12       5,207.0       5,067.5     3,225        8,320       1,568.1  munmap                \n",
            "      0.0           60,854         12       5,071.2       5,550.5     1,555        8,256       2,487.1  write                 \n",
            "      0.0           52,321         33       1,585.5       1,094.0       805        5,807       1,167.4  fclose                \n",
            "      0.0           36,734         20       1,836.7          45.0        44       35,800       7,994.1  fgets                 \n",
            "      0.0           33,841         64         528.8         539.5       192        1,146         203.5  fcntl                 \n",
            "      0.0           31,456          6       5,242.7       4,864.0     2,275        8,714       2,637.9  open                  \n",
            "      0.0           26,013          2      13,006.5      13,006.5     7,900       18,113       7,221.7  socket                \n",
            "      0.0           24,455         15       1,630.3       1,479.0       479        3,981         879.4  read                  \n",
            "      0.0           19,258          3       6,419.3       7,147.0     3,508        8,603       2,624.3  pipe2                 \n",
            "      0.0           10,042          1      10,042.0      10,042.0    10,042       10,042           0.0  connect               \n",
            "      0.0            7,417          2       3,708.5       3,708.5     2,146        5,271       2,209.7  pthread_cond_broadcast\n",
            "      0.0            5,475          2       2,737.5       2,737.5     2,701        2,774          51.6  fwrite                \n",
            "      0.0            2,808          8         351.0         341.5       279          477          61.6  dup                   \n",
            "      0.0            1,243          1       1,243.0       1,243.0     1,243        1,243           0.0  bind                  \n",
            "      0.0            1,187          1       1,187.0       1,187.0     1,187        1,187           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  -----------  ------------  ----------------------\n",
            "     64.3      110,357,561          3  36,785,853.7      71,782.0      29,398  110,256,381  63,627,346.6  cudaMallocManaged     \n",
            "     29.1       49,976,682          1  49,976,682.0  49,976,682.0  49,976,682   49,976,682           0.0  cudaDeviceSynchronize \n",
            "      6.4       10,964,072          3   3,654,690.7   3,317,022.0   3,231,582    4,415,468     660,236.0  cudaFree              \n",
            "      0.1          201,970          1     201,970.0     201,970.0     201,970      201,970           0.0  cudaLaunchKernel      \n",
            "      0.0            1,487          1       1,487.0       1,487.0       1,487        1,487           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                    Name                   \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  -----------------------------------------\n",
            "    100.0       49,960,910          1  49,960,910.0  49,960,910.0  49,960,910  49,960,910          0.0  vectorAdd(float *, float *, float *, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
            " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
            "     81.3       25,299,815  3,577   7,072.9   3,007.0     2,463    82,398     13,457.6  [CUDA Unified Memory memcpy HtoD]\n",
            "     18.7        5,800,746    384  15,106.1   4,335.5     1,951    83,070     22,842.4  [CUDA Unified Memory memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
            "    201.327  3,577     0.056     0.008     0.004     0.983        0.166  [CUDA Unified Memory memcpy HtoD]\n",
            "     67.109    384     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report2.nsys-rep\n",
            "    /home/cuda/report2.sqlite\n",
            "\n",
            "============ PROFILE: vecadd_prefetch ============\n",
            "\n",
            "Unified Memory + Prefetching: success!\n",
            "Generating '/tmp/nsys-report-751e.qdstrm'\n",
            "[1/8] [========================100%] report3.nsys-rep\n",
            "[2/8] [========================100%] report3.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report3.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  --------  -----------  ------------  ----------------------\n",
            "     90.4      881,250,508         42  20,982,155.0  10,077,354.0    78,646  325,176,143  52,279,931.7  poll                  \n",
            "      7.8       75,919,758        549     138,287.4      11,259.0       456   17,284,791     958,891.8  ioctl                 \n",
            "      0.9        8,929,344         24     372,056.0       9,386.5     1,739    7,461,990   1,519,582.1  mmap                  \n",
            "      0.4        3,913,385          2   1,956,692.5   1,956,692.5   495,193    3,418,192   2,066,872.4  sem_wait              \n",
            "      0.2        2,021,743         31      65,217.5      12,484.0     8,067    1,326,626     234,974.5  mmap64                \n",
            "      0.1          849,123         10      84,912.3      58,440.0    33,704      323,818      85,043.5  sem_timedwait         \n",
            "      0.1          499,498          1     499,498.0     499,498.0   499,498      499,498           0.0  pthread_cond_wait     \n",
            "      0.0          421,740         49       8,606.9       7,511.0     2,583       17,632       3,318.9  open64                \n",
            "      0.0          255,471         40       6,386.8       3,749.0     1,378       30,316       6,719.6  fopen                 \n",
            "      0.0          255,338          3      85,112.7      71,731.0    52,138      131,469      41,323.8  pthread_create        \n",
            "      0.0           72,828         13       5,602.2       6,068.0     2,703        7,743       1,429.5  write                 \n",
            "      0.0           71,966         12       5,997.2       4,622.5     2,749       17,426       4,083.1  munmap                \n",
            "      0.0           54,766         33       1,659.6       1,342.0       857        5,992       1,135.5  fclose                \n",
            "      0.0           39,247         20       1,962.4          48.0        47       38,260       8,543.6  fgets                 \n",
            "      0.0           35,383         64         552.9         583.5       169        1,120         202.7  fcntl                 \n",
            "      0.0           35,050          6       5,841.7       5,678.0     1,809        9,565       2,688.0  open                  \n",
            "      0.0           28,774         16       1,798.4       1,595.0       898        3,567         742.3  read                  \n",
            "      0.0           24,503          2      12,251.5      12,251.5     7,489       17,014       6,735.2  socket                \n",
            "      0.0           17,803          3       5,934.3       6,077.0     3,288        8,438       2,578.0  pipe2                 \n",
            "      0.0           15,440          2       7,720.0       7,720.0     2,500       12,940       7,382.2  fwrite                \n",
            "      0.0           10,140          1      10,140.0      10,140.0    10,140       10,140           0.0  connect               \n",
            "      0.0            9,723          3       3,241.0       2,260.0     1,955        5,508       1,969.2  pthread_cond_broadcast\n",
            "      0.0            2,841          8         355.1         335.5       313          438          43.0  dup                   \n",
            "      0.0            1,239          1       1,239.0       1,239.0     1,239        1,239           0.0  bind                  \n",
            "      0.0              985          1         985.0         985.0       985          985           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  ---------  -----------  ------------  ----------------------\n",
            "     69.5      109,492,441          3  36,497,480.3      60,113.0     29,539  109,402,789  63,137,851.2  cudaMallocManaged     \n",
            "     13.7       21,646,749          2  10,823,374.5  10,823,374.5     14,808   21,631,941  15,285,621.3  cudaDeviceSynchronize \n",
            "      5.9        9,368,874          1   9,368,874.0   9,368,874.0  9,368,874    9,368,874           0.0  cudaLaunchKernel      \n",
            "      5.7        8,936,073          3   2,978,691.0     810,282.0    619,637    7,506,154   3,922,056.5  cudaFree              \n",
            "      5.2        8,181,250          3   2,727,083.3   1,934,506.0    681,676    5,565,068   2,536,338.6  cudaMemPrefetchAsync  \n",
            "      0.0            1,599          1       1,599.0       1,599.0      1,599        1,599           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                    Name                   \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  -----------------------------------------\n",
            "    100.0       21,622,307          1  21,622,307.0  21,622,307.0  21,622,307  21,622,307          0.0  vectorAdd(float *, float *, float *, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
            " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ---------------------------------\n",
            "     80.0       20,583,359  1,703   12,086.5    2,943.0     2,494   174,652     33,874.1  [CUDA Unified Memory memcpy HtoD]\n",
            "     20.0        5,141,208     32  160,662.8  160,668.0   160,540   160,732         48.2  [CUDA Unified Memory memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
            "    201.327  1,703     0.118     0.008     0.004     2.097        0.414  [CUDA Unified Memory memcpy HtoD]\n",
            "     67.109     32     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report3.nsys-rep\n",
            "    /home/cuda/report3.sqlite\n",
            "\n",
            "============ PROFILE: vecadd_pinned ============\n",
            "\n",
            "Pinned memory: success!\n",
            "Generating '/tmp/nsys-report-6bd1.qdstrm'\n",
            "[1/8] [========================100%] report4.nsys-rep\n",
            "[2/8] [========================100%] report4.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  --------  -----------  ------------  ----------------------\n",
            "     77.7      623,597,544         15  41,573,169.6  2,827,447.0    83,369  320,982,148  84,527,177.5  poll                  \n",
            "     19.3      155,347,640        560     277,406.5     11,674.0       504   27,307,311   2,179,531.4  ioctl                 \n",
            "      2.4       19,347,289         27     716,566.3     12,484.0     1,762    6,364,312   2,025,245.8  mmap                  \n",
            "      0.3        2,037,389         31      65,722.2     11,200.0     7,761    1,338,165     237,173.6  mmap64                \n",
            "      0.1          880,588         10      88,058.8     70,051.0    43,070      265,910      64,087.3  sem_timedwait         \n",
            "      0.1          550,823          1     550,823.0    550,823.0   550,823      550,823           0.0  pthread_cond_wait     \n",
            "      0.0          384,643         49       7,849.9      7,411.0     2,190       17,496       2,904.6  open64                \n",
            "      0.0          230,308         40       5,757.7      3,134.5     1,388       28,779       6,184.4  fopen                 \n",
            "      0.0          107,695          2      53,847.5     53,847.5    48,634       59,061       7,373.0  pthread_create        \n",
            "      0.0          103,075         12       8,589.6      5,812.0     3,100       37,054       9,111.7  write                 \n",
            "      0.0          102,004         17       6,000.2      5,337.0     3,576       11,861       2,402.4  munmap                \n",
            "      0.0           56,824         20       2,841.2         49.0        48       55,812      12,468.0  fgets                 \n",
            "      0.0           52,292         33       1,584.6      1,126.0       763        6,006       1,244.6  fclose                \n",
            "      0.0           33,610         64         525.2        562.5       177        1,025         189.0  fcntl                 \n",
            "      0.0           30,424          6       5,070.7      4,782.5     1,640        8,057       2,587.6  open                  \n",
            "      0.0           27,385         15       1,825.7      1,655.0       858        3,443         698.7  read                  \n",
            "      0.0           22,130          2      11,065.0     11,065.0     7,034       15,096       5,700.7  socket                \n",
            "      0.0           21,253          1      21,253.0     21,253.0    21,253       21,253           0.0  connect               \n",
            "      0.0           17,837          3       5,945.7      5,449.0     3,581        8,807       2,648.2  pipe2                 \n",
            "      0.0            7,938          2       3,969.0      3,969.0     2,169        5,769       2,545.6  pthread_cond_broadcast\n",
            "      0.0            5,550          2       2,775.0      2,775.0     2,478        3,072         420.0  fwrite                \n",
            "      0.0            2,847          8         355.9        334.5       299          539          78.0  dup                   \n",
            "      0.0            2,277          1       2,277.0      2,277.0     2,277        2,277           0.0  bind                  \n",
            "      0.0            1,283          1       1,283.0      1,283.0     1,283        1,283           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  -----------  ------------  ----------------------\n",
            "     78.9      173,225,490          3  57,741,830.0  27,841,413.0  27,473,421  117,910,656  52,108,056.7  cudaMallocHost        \n",
            "     12.0       26,415,611          3   8,805,203.7   8,207,252.0   8,157,082   10,051,277   1,079,422.7  cudaFreeHost          \n",
            "      5.0       10,953,111          1  10,953,111.0  10,953,111.0  10,953,111   10,953,111           0.0  cudaLaunchKernel      \n",
            "      2.7        5,972,092          1   5,972,092.0   5,972,092.0   5,972,092    5,972,092           0.0  cudaStreamSynchronize \n",
            "      1.1        2,431,187          3     810,395.7   1,038,842.0     271,102    1,121,243     468,855.8  cudaFree              \n",
            "      0.2          510,452          3     170,150.7     123,066.0      87,758      299,628     113,511.9  cudaMalloc            \n",
            "      0.0           47,396          3      15,798.7      10,191.0       4,801       32,404      14,631.0  cudaMemcpyAsync       \n",
            "      0.0           38,120          1      38,120.0      38,120.0      38,120       38,120           0.0  cudaStreamCreate      \n",
            "      0.0           21,220          1      21,220.0      21,220.0      21,220       21,220           0.0  cudaStreamDestroy     \n",
            "      0.0            1,518          1       1,518.0       1,518.0       1,518        1,518           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                    Name                   \n",
            " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -----------------------------------------\n",
            "    100.0          861,898          1  861,898.0  861,898.0   861,898   861,898          0.0  vectorAdd(float *, float *, float *, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     68.0       10,857,152      2  5,428,576.0  5,428,576.0  5,426,704  5,430,448      2,647.4  [CUDA memcpy HtoD]\n",
            "     32.0        5,099,449      1  5,099,449.0  5,099,449.0  5,099,449  5,099,449          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "    134.218      2    67.109    67.109    67.109    67.109        0.000  [CUDA memcpy HtoD]\n",
            "     67.109      1    67.109    67.109    67.109    67.109        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report4.nsys-rep\n",
            "    /home/cuda/report4.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Histogram**"
      ],
      "metadata": {
        "id": "Kr0MRAX5eKAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an array whose elements are all from a set known a priori, count the occurrencies of each element of the set in the array.\n",
        "\n",
        "In this example, let the set be the ASCII letters from 'a' to 'z', and the array length be 'n'.\n",
        "\n",
        "Let's also add the concept of \"bins\", these are subset of elements that count towards the same total.\n",
        "For us, letters will be binned in groups of four, that is, 'a-d' will count towards the same total, then 'e-h', and so on.\n",
        "\n",
        "Key issues to solve:\n",
        "- data access locality\n",
        "- output interference"
      ],
      "metadata": {
        "id": "YQmgIVH_ePXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrong (poor memory access efficiency - and data races)"
      ],
      "metadata": {
        "id": "GUFcfWgThYT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each thread is given a contigous section of the input.<br>\n",
        "The input array is evenly partitioned and given to threads, each thread then consumes its partition one element at a time.\n",
        "\n",
        "Issue:<br>\n",
        "At a given point in time, all threads will be asking to access values from memory that sit at very distant locations one from the other.\n",
        "We cannot bundle this data in a single DRAM transaction."
      ],
      "metadata": {
        "id": "ZAb9VCGRiby6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void histKernel(unsigned char *arr, unsigned int *hist, long n) {\n",
        "  int part_size = (n + gridDim.x*blockDim.x - 1)/gridDim.x*blockDim.x; // ceil(n / gridDim.x*blockDim.x)\n",
        "  int tid = (blockIdx.x*blockDim.x + threadIdx.x)*part_size;\n",
        "\n",
        "  for (int i = tid; i < tid + part_size && i < n; i++) {\n",
        "    int letter_idx = arr[i] - 97; // 97 is the ASCII code of 'a'\n",
        "    if (letter_idx >= 0 && letter_idx < 26)\n",
        "      hist[letter_idx/4] += 1; // divide by 4 to get bins\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // HP: less threads than elements\n",
        "  long n = 10000;\n",
        "  int blockSize = 16;\n",
        "  int numBlocks = 32;\n",
        "\n",
        "  unsigned char *arr;\n",
        "  unsigned int *hist;\n",
        "  cudaMallocManaged(&arr, n*sizeof(float));\n",
        "  cudaMallocManaged(&hist, 7*sizeof(float)); // 26 letters in bins of 4 -> 7 bins\n",
        "  for (int i = 0; i < n; i++)\n",
        "    arr[i] = (unsigned char)(i%26 + 97);\n",
        "  for (int i = 0; i < 7; i++)\n",
        "    hist[i] = 0;\n",
        "\n",
        "  histKernel<<<numBlocks, blockSize>>>(arr, hist, n);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaFree(arr);\n",
        "  cudaFree(hist);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "vELryHeThXvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrong (data races)"
      ],
      "metadata": {
        "id": "Gv-T1xlJeRQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Threads process the array in an interleaved fashion.<br>\n",
        "With the whole grid of threads being smaller than the array, the first thread in the grid processes the first element, the second the second, and so on. Then, the whole grid is \"moved\" to the right by its own size, therefore the first thread will process element |grid|+0, the second |grid|+1, and so on. They will then move to 2\\*|grid|+0, 2\\*|grid|+1, ...\n",
        "\n",
        "Why?<br>\n",
        "This way, at any point in time, all threads process a **contigous section of elements**. Therefore, **memory accesses are coalesced**, because threads step forward together, and ask for contigous data all at once.\n",
        "Ultimately, this gives better spatial locality -> multiple piece of data transferred in one DRAM transaction.\n",
        "\n",
        "There is still one issue: updates to the histogram can happen however they please. This means that two threads could read the same value for 'k = hist[x]' and thus both write 'k + 1', thereby loosing one of the two increments.<br>\n",
        "This is a classic problem of writing shared memory parallel code: the **data race**.\n",
        "\n",
        "*Coalesced: DRAM transactions are wide and can move multiple contigous elements at once. If contigous data is accessed by threads in the same **warp**, such data can be moved as part of one same, or very few, transactions.*\n",
        "\n",
        "*Note: more info on coalescence here: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-to-global-memory*"
      ],
      "metadata": {
        "id": "dCGWNbXEfJ8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void histKernel(unsigned char *arr, unsigned int *hist, long n) {\n",
        "  int tid = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x*gridDim.x;\n",
        "\n",
        "  for (int i = tid; i < size; i += stride) {\n",
        "    int letter_idx = arr[i] - 97;\n",
        "    if (letter_idx >= 0 && letter_idx < 26)\n",
        "      hist[letter_idx/4] += 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "// same main as above..."
      ],
      "metadata": {
        "id": "KGNPQDx3ex6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atomics"
      ],
      "metadata": {
        "id": "y8Tw87vzeZX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent concurrent updates (read-modify-write operations) to cause data races, we just need to give a thread exclusive access to the modified value from before it reads it until it finishes the update. Essentially, an atomic context.<br>\n",
        "CUDA gives use ready-to-use primitives for this, at least for most basic operations, therefore, instead of a normal '+1' on the histogram, we need to perform its atomic equivalent 'atomicAdd(...)'.\n",
        "\n",
        "Issue:<br>\n",
        "Atomic operations are semantically correct, but take ~1000 cycles to complete on DRAM, , this severely hurts performance.<br>\n",
        "So, could we get away without atomic operations while preserving correctness? Yes, and as it is often the case, we just need to postpone some part of the program! In this case, we will postpone the global accumulation of histogram bin counts."
      ],
      "metadata": {
        "id": "dIfxDAQ9oLyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void histKernel(unsigned char *arr, unsigned int *hist, long n) {\n",
        "  int tid = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x*gridDim.x;\n",
        "\n",
        "  for (int i = tid; i < size; i += stride) {\n",
        "    int letter_idx = arr[i] - 97;\n",
        "    if (letter_idx >= 0 && letter_idx < 26)\n",
        "      atomicAdd(&(hist[letter_idx/4]), 1);\n",
        "  }\n",
        "}\n",
        "\n",
        "// same main as above..."
      ],
      "metadata": {
        "id": "JaEHa0X5rPAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Privatization"
      ],
      "metadata": {
        "id": "fJPEJshGec_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a copy of the histogram counters per block, using shared memory.<br>\n",
        "Then, rely on atomics on shared memory, since they take ~10 cycles to complete on L2 cache, that is 100x better than DRAM.<br>\n",
        "As the very last step, accumulate the counters from all copies."
      ],
      "metadata": {
        "id": "jdr0UVCvoKP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void histKernel(unsigned char *arr, unsigned int *hist, long n, unsigned int num_bins) {\n",
        "  int tid = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x*gridDim.x;\n",
        "  extern __shared__ unsigned int hist_s[];\n",
        "\n",
        "  for(unsigned int bin_idx = threadIdx.x; bin_idx < num_bins; bin_idx += blockDim.x)\n",
        "    hist_s[bin_idx] = 0u;\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  // compute histogram in shared memory\n",
        "  for (unsigned int i = tid; i < n; i += stride) {\n",
        "    int letter_idx = arr[i] - 97;\n",
        "    if (letter_idx >= 0 && letter_idx < 26)\n",
        "      atomicAdd(&(hist_s[letter_idx/4]), 1);\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  // accumulate between blocks\n",
        "  for(unsigned int bin_idx = threadIdx.x; bin_idx < num_bins; bin_idx += blockDim.x)\n",
        "    atomicAdd(&(hist[bin_idx]), hist_s[bin_idx]);\n",
        "}\n",
        "\n",
        "// same main as above, just pass '7' as 'num_bins'."
      ],
      "metadata": {
        "id": "1zbIJmKkoJek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All major problems are now solved! The code could still see some minor optimization, like performing a reduction between the private histogram copies, instead of relying again on atomics, but that is left to the reader."
      ],
      "metadata": {
        "id": "AZ9JDsF2slAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stencil**"
      ],
      "metadata": {
        "id": "QLSAwTAYgLV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this simple \"5-point stencil\" kernel on a 2D grid that updates each cell with the average of its four neighbors (up, down, left, right).\n",
        "\n",
        "Visually:\n",
        "\n",
        "<img src=\"https://www.mdpi.com/electronics/electronics-09-01275/article_deploy/html/images/electronics-09-01275-g001-550.jpg\" alt=\"Stencil visualization.\" width=\"350\" border=\"2\">"
      ],
      "metadata": {
        "id": "RDOSkBqhgOKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void stencilKernel(float *input, float *out, long m, long n) {\n",
        "    int x = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "\n",
        "    extern __shared__ float tile[];\n",
        "\n",
        "    int sh_w = blockDim.x + 2;\n",
        "    // index inside shared memory\n",
        "    int sh_x = threadIdx.x + 1;\n",
        "    int sh_y = threadIdx.y + 1;\n",
        "\n",
        "    // helper function to cleanup index calculations\n",
        "    auto idx = [&](int yy, int xx) {\n",
        "      return yy*m + xx;\n",
        "    };\n",
        "\n",
        "    // load tile\n",
        "    if (x < m && y < n)\n",
        "        tile[sh_y*sh_w + sh_x] = input[idx(y, x)];\n",
        "    else\n",
        "        tile[sh_y*sh_w + sh_x] = 0.0f;\n",
        "\n",
        "    // load halo\n",
        "    if (threadIdx.x == 0 && x > 0)\n",
        "        tile[sh_y*sh_w + (sh_x - 1)] = input[idx(y, x - 1)];\n",
        "    if (threadIdx.x == blockDim.x - 1 && x < m - 1)\n",
        "        tile[sh_y*sh_w + (sh_x + 1)] = input[idx(y, x + 1)];\n",
        "    if (threadIdx.y == 0 && y > 0)\n",
        "        tile[(sh_y - 1) sh_w + sh_x] = input[idx(y - 1, x)];\n",
        "    if (threadIdx.y == blockDim.y - 1 && y < n - 1)\n",
        "        tile[(sh_y + 1)*sh_w + sh_x] = input[idx(y + 1, x)];\n",
        "\n",
        "    // corner halos (not used in 5-point)\n",
        "    /*\n",
        "    if (threadIdx.x == 0 && threadIdx.y == 0 && x > 0 && y > 0)\n",
        "        tile[(sh_y - 1)*sh_w + (sh_x - 1)] = input[idx(y - 1, x - 1)];\n",
        "    if (threadIdx.x == blockDim.x - 1 && threadIdx.y == 0 && x < m - 1 && y > 0)\n",
        "        tile[(sh_y - 1)*sh_w + (sh_x + 1)] = input[idx(y - 1, x + 1)];\n",
        "    if (threadIdx.x == 0 && threadIdx.y == blockDim.y - 1 && x > 0 && y < n - 1)\n",
        "        tile[(sh_y + 1)*sh_w + (sh_x - 1)] = input[idx(y + 1, x - 1)];\n",
        "    if (threadIdx.x == blockDim.x - 1 && threadIdx.y == blockDim.y - 1 && x < m - 1 && y < n - 1)\n",
        "        tile[(sh_y + 1)*sh_w + (sh_x + 1)] = input[idx(y + 1, x + 1)];\n",
        "    */\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // compute the stencil (5-point average)\n",
        "    if (x > 0 && x < m - 1 && y > 0 && y < n - 1) {\n",
        "        float c = tile[sh_y*sh_w + sh_x];\n",
        "        float l = tile[sh_y*sh_w + (sh_x - 1)];\n",
        "        float r = tile[sh_y*sh_w + (sh_x + 1)];\n",
        "        float u = tile[(sh_y - 1)*sh_w + sh_x];\n",
        "        float d = tile[(sh_y + 1)*sh_w + sh_x];\n",
        "        out[idx(y, x)] = (c + l + r + u + d)/5.0f;\n",
        "    } else if (x < m && y < n) {\n",
        "        out[idx(y, x)] = input[idx(y, x)];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  long m = 20000;\n",
        "  long n = 10000;\n",
        "  dim3 blockSize(32, 32);\n",
        "  dim3 numBlocks((m + blockSize.x - 1)/blockSize.x, (n + blockSize.y - 1)/blockSize.y);\n",
        "  size_t shmem = (blockSize.x + 2)*(blockSize.y + 2) * sizeof(float);\n",
        "\n",
        "  float *input, *out;\n",
        "  cudaMallocManaged(&input, m*n*sizeof(float));\n",
        "  cudaMallocManaged(&out, m*n*sizeof(float));\n",
        "  for (int i = 0; i < m*n; i++)\n",
        "    input[i] = (float)(i % 100);\n",
        "\n",
        "  stencilKernel<<<numBlocks, blockSize, shmem>>>(input, out, m, n);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaFree(input);\n",
        "  cudaFree(out);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "KWuk1QzGgxU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a clear opportunity to exploit shared memory to reuse neighbors inside a block.<br>\n",
        "Observe the following:\n",
        "- how many elements end up in the shared memory of each block, and why?\n",
        "<!--\n",
        "In general that is (blockSize.x + 2)*(blockSize.y + 2) because each block needs to include one more neighbor along the perimeter, creating an overlap between the shared memory of adjacent blocks (the \"halo\"). The reason is in the operation itself, each output elements wants four neighbors, regardless of its position.\n",
        "\n",
        "Note: there are indeed the \"corners\" in shared memory that don't get used, so we could have 4 less elements in the above calculation. But it's worth \"wasting\" them to simplify index calculations.\n",
        "-->\n",
        "- since the elements in the halo get accessed only by a single thread, could we not store the halo in shared memory and relegate these extra elements that are accessed once to global memory accesses? Would it be a good idea?\n",
        "<!--\n",
        "In terms of \"can\", it is absolutely possible.\n",
        "It is a bad idea, loosing a lot of performance because:\n",
        "- now threads that are on the edge of each block will diverge from the others in their warp; moreover it diverges over a very costly global memory access.\n",
        "- global memory accesses are slow and expensive, if we perform them in bulk during the preparation of shared memory we can save time as they get coalesced!\n",
        "\n",
        "Remember: shared memory isn't only for reuse, but also to improve memory access performance and exploit coalesced memory acceses on DRAM.\n",
        "\n",
        "Another observation: the \"wasted\" shared memory equals the perimeter of the input, this when the input size grows, it grows quadratically, while the wasted memory grows linearly with the perimeter, therefore at scale it is a negligible overhead.\n",
        "\n",
        "More info on coalescence here: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-to-global-memory\n",
        "-->\n",
        "- what would happen if we remove the `__synchthreads()` instruction?\n",
        "<!--\n",
        "Race conditions! We could have a thread in the block start reading addresses in shared memory that others had to write, before they have been written.\n",
        "\n",
        "In other words: we could violate a read-after-write data dependency.\n",
        "-->\n",
        "- what would we need to do if we wanted to have the input grid be updated in-place instead of relying on another output buffer?\n",
        "<!--\n",
        "Let each thread store its own result in a register, synch threads globally (incrementing with a CAS a global variable and spinning on it until it reaches the thread count) and only then let each thread overwrite the input.\n",
        "Improvement:\n",
        "The global variable could count blocks, not threads, and the first thread of each block can increment it as soon as its block has synched after preparing its shared memory. That is because after shared memory has been loaded in all blocks, subsequent reads will only hit shared memory, and there will be no more race conditions.\n",
        "-->\n",
        "\n",
        "*Note: the term to indicate the part of the input that overlaps between neighboring blocks is \"**halo**\". Yes Chief, like the videogame.*"
      ],
      "metadata": {
        "id": "jo8_rVXkg0bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MatMul**"
      ],
      "metadata": {
        "id": "ojawXVrOeH5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the classic idea of a matrix multiplication:\n",
        "\n",
        "<!--<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Matrix_Multiplication.png\" alt=\"Matrix multiplication overview.\" width=\"350\" border=\"0\">-->\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Matrix_multiplication_diagram.svg/1024px-Matrix_multiplication_diagram.svg.png\" alt=\"Matrix multiplication overview.\" width=\"350\" border=\"0\">"
      ],
      "metadata": {
        "id": "-n1iJAYocll8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key goal: parallelize the workload and exploit data reuse through **tiling**.\n",
        "\n",
        "The term tile draws on the analogy that a large wall (e.g., the global memory data) can be covered by small tiles (e.g., subsets that can each fit into the shared memory).\n",
        "An important criterion is that the kernel computation on these tiles can be done independently of each other.\n",
        "Note that, for this reason, not all data structures can be partitioned into tiles, given an arbitrary kernel function.\n",
        "\n",
        "In particular we will rely on:\n",
        "- tiling to use [for reuse] shared memory in the output;\n",
        "- strip mining to use [for reuse] shared memory while sliding over input operands by breaking the inner dimension's loop in tiles and go over them one by one;\n",
        "\n",
        "<br>For these examples, let us assume three square matrices of identical size: \"width\" in the code."
      ],
      "metadata": {
        "id": "yTqqWd9XCqiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No Tiling"
      ],
      "metadata": {
        "id": "kDLJcIXCncU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No optimization of sorts, this is just a basic matmul kernel very much like what you would write to run on CPU."
      ],
      "metadata": {
        "id": "eri7Gzmni412"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "__global__\n",
        "void matMul(float* M, float* N, float* P, int width) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y; // row of M and P\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x; // col of N and P\n",
        "\n",
        "  if ((row < width) && (col < width)) {\n",
        "    float acc = 0;\n",
        "    for (int k = 0; k < width; ++k) // k is the col of M and row of N\n",
        "      acc += M[row*width + k]*N[k*width + col];\n",
        "    P[row*width + col] = acc;\n",
        "  }\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  int width = ...;\n",
        "  int block_size = ...;\n",
        "  float M[width*width], N[width*width], P[width*width];\n",
        "  //...\n",
        "  matMul<<<dim3(block_size, block_size), dim3(width/block_size, width/block_size)>>>(M, N, P, width);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "VNeRx8B4heXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"tiled-mm\"></a>\n",
        "### Tiled Matrix Multiplication"
      ],
      "metadata": {
        "id": "yIOHuPqLnfJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Each block is assigned an output tile. Then, to move its threads along in the span of the other two operands in clear steps, progressively reducing the output elements in the assigned tile!<br>\n",
        "This is a technique called **strip-mining**, which takes a long-running loop and break it into phases.\n",
        "Each phase involves an inner loop that executes a few consecutive iterations of the original loop.\n",
        "The original loop becomes an outer loop whose role is to iteratively invoke the inner loop so that all the iterations of the original loop are executed in their original order.\n",
        "By adding barrier synchronizations before and after the inner loop, we force all threads in the same block to focus their work on the same section of input data during each phase.\n",
        "Strip-mining is an important means to creating the phases that are needed by tiling in data parallel programs.\n",
        "\n",
        "<img src=\"https://i.imgur.com/VGNlxxx.png\" alt=\"CUDA memory hierarchy overview.\" width=\"400\" border=\"2\">\n",
        "\n",
        "*Credit: \"Programming Massiveli Parallel Processors\" by W. Hwu, D. Kirk, I. Hajj*\n",
        "\n",
        "In other words, we have a one-thread-one-output-element relation, then each thread iterates over additional tiles formed along the width and height of the other two operands. For each tile, the block loads it in its shared memory and each threads reuses it. Continuing until after all tiles each output element has been fully reduced."
      ],
      "metadata": {
        "id": "v5O_6O_caVkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--<img src=\"https://polimi365-my.sharepoint.com/:i:/r/personal/10669641_polimi_it/Documents/Images/matmul_reuse.png\" alt=\"Reuse in shared memory for a tiled matrix multiplication.\" width=\"500\" border=\"2\">-->\n",
        "<img src=\"https://i.imgur.com/TdkJXHa.png\" alt=\"Reuse in shared memory for a tiled matrix multiplication.\" width=\"400\" border=\"2\">\n",
        "\n",
        "You should verify that the potential reduction in global memory traffic in a square matrix multiplication is proportional to the dimension of the blocks that are used.\n",
        "With $width{\\times}width$ blocks, the potential reduction of global memory traffic should be $width$.\n",
        "That is, if we use 16$\\times$16 blocks, we can potentially reduce the global memory traffic to 1/16 of the original level through collaboration between threads.\n",
        "<!--\n",
        "Another way to look at it: with tiles of 16x16, each value of those tiles is loaded once, and loaded again only 1/16th of the times it was before, because before needing to realod it the whole tile needs to have moved and come back.\n",
        "Alternatively, see it as the whole 16 accesses to a value that happen inside a tile now costing only 1 global memory access.\n",
        "-->\n",
        "\n",
        "A fairly universal approach to calculate the reduction in required global memory bandwidth is (# accesses to elements in the tile before its overwritten)/(# of elements in the tile). Where the denominator is the global accesses required to load the tile and the numerator the accesses that we would have needed if it were not for shared memory.\n",
        "\n",
        "*Note: tile size is by no means constrainted to be equal to block size. However, for sake of providing a simple example, the following code assumes that tile size = block size.*"
      ],
      "metadata": {
        "id": "7xRjsjAO2toZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define TILE_DIM 16\n",
        "\n",
        "__global__\n",
        "void matMul(float* M, float* N, float* P, int width) {\n",
        "  __shared__ float M_s[TILE_DIM][TILE_DIM];\n",
        "  __shared__ float N_s[TILE_DIM][TILE_DIM];\n",
        "\n",
        "  int row = blockIdx.y*TILE_DIM + threadIdx.y;\n",
        "  int col = blockIdx.x*TILE_DIM + threadIdx.x;\n",
        "\n",
        "  float acc = 0;\n",
        "  for (int t = 0; t < (width + TILE_DIM - 1)/TILE_DIM; t++) { // allow for 'width's not multiples of 'TILE_DIM' and viceversa\n",
        "    // shared memory loads\n",
        "    if (t*TILE_DIM + threadIdx.x < width && row < width)\n",
        "      M_s[threadIdx.y][threadIdx.x] = M[row*width + t*TILE_DIM + threadIdx.x];\n",
        "    else\n",
        "      M_s[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "    if (t*TILE_DIM + threadIdx.y < width && col < width)\n",
        "      N_s[threadIdx.y][threadIdx.x] = N[(t*TILE_DIM + threadIdx.y)*width + col];\n",
        "    else\n",
        "      N_s[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // compute\n",
        "    for (int k = 0; k < TILE_DIM; k++)\n",
        "      acc += M_s[threadIdx.y][k] * N_s[k][threadIdx.x];\n",
        "\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // write result\n",
        "  if (row < width && col < width)\n",
        "    C[row*width + col] = acc;\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  int width = ...;\n",
        "  float M[width*width], N[width*width], P[width*width];\n",
        "  //...\n",
        "  matMul<<<dim3(width/TILE_DIM, width/TILE_DIM), dim3(TILE_DIM TILE_DIM)>>>(M, N, P, width);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "ZJUmFXcyh45B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depth Parallel (tiling the inner dimension and reducing)"
      ],
      "metadata": {
        "id": "ldQoe-eAnhs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us look at the matrix multiplication as a 3D tensor, width and height are those of the output, while depth is the inner dimension, each \"cell\" in the tensor is a multiply-and-accumulate (pruduct + sum) to be done, while depth-wise 1x1 tensors pass through the values to be accumulated for one output.\n",
        "\n",
        "In practive, we expand spatial parallelism (one thread per output element) with depth parallelism (multiple threads collaborating along the inner accumulation dimension, i.e. 'k').\n",
        "\n",
        "Each output element is now computed by multiple threads, each handling a tile of the inner dimension (those that were strip-mined sequentially before), and the partial sums are then reduced efficiently within the shared memory of the cooperative thread group."
      ],
      "metadata": {
        "id": "dktDHSU7jO-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "#include <stdio.h>\n",
        "\n",
        "#define TILE_DIM 16 // tile size for output width and height dimensions\n",
        "#define DEPTH_THREADS 8 // number of tiles along the inner dimension\n",
        "\n",
        "__global__\n",
        "void matMul(float* M, float* N, float* C, int width) {\n",
        "    __shared__ float sh[TILE_DIM][TILE_DIM][DEPTH_THREADS];\n",
        "\n",
        "    int row = blockIdx.y*TILE_DIM + threadIdx.y;\n",
        "    int col = blockIdx.x*TILE_DIM + threadIdx.x;\n",
        "    int depth_id = threadIdx.z; // which slice of the inner dimension this thread handles\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    // each depth thread handles a subset of k values: strided by DEPTH_THREADS\n",
        "    for (int k = depth_id; k < width; k += DEPTH_THREADS) {\n",
        "        float a = (row < width && k < width) ? M[row*width + k] : 0.0f;\n",
        "        float b = (col < width && k < width) ? N[k*width + col] : 0.0f;\n",
        "        sum += a * b;\n",
        "    }\n",
        "\n",
        "    // store partial sum\n",
        "    if (row < width && col < width)\n",
        "        sh[threadIdx.y][threadIdx.x][depth_id] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // reduction across DEPTH_THREADS\n",
        "    if (depth_id == 0 && row < width && col < width) {\n",
        "        float acc = 0.0f;\n",
        "        for (int d = 0; d < DEPTH_THREADS; ++d)\n",
        "            acc += partial[threadIdx.y][threadIdx.x][d];\n",
        "        C[row * width + col] = acc;\n",
        "    }\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  int width = ...;\n",
        "  float M[width*width], N[width*width], P[width*width];\n",
        "  //...\n",
        "  dim3 block(TILE_DIM, TILE_DIM, DEPTH_THREADS);\n",
        "  dim3 grid((width + TILE_DIM - 1)/TILE_DIM, (width + TILE_DIM - 1)/TILE_DIM); // 2D grid of 3D blocks\n",
        "  matMul<<<grid, block>>>(M, N, P, width);\n",
        "  //...\n",
        "}\n"
      ],
      "metadata": {
        "id": "-L23I1xOwc3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately, this by itself is **far worse** than tiling!<br>\n",
        "Why: much less data reuse, each thread fetches from M and N independently, tiles are not shared.\n",
        "\n",
        "Modern libraries like cuBLAS combine a slight bit of this depth parallelism with the previous tiling method, mainly to hide the memory latency of loading data in shared memory by doing it alternatingly across a few different depth tiles (some load data while others compute)."
      ],
      "metadata": {
        "id": "fhFOzWi52jsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extras**"
      ],
      "metadata": {
        "id": "OhtRVdQqsxdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Occupancy Calculation"
      ],
      "metadata": {
        "id": "W7gpJuR0aLRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several API functions exist to assist programmers in choosing thread block size and cluster size based on register and shared memory requirements.\n",
        "\n",
        "Here we showcase the \"occupancy calculator\" API, `cudaOccupancyMaxActiveBlocksPerMultiprocessor`, can provide an occupancy prediction based on the block size and shared memory usage of a kernel. This function reports occupancy in terms of the number of concurrent thread blocks per multiprocessor."
      ],
      "metadata": {
        "id": "VghaJPc8fU98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile occupancy_calculator.cpp\n",
        "__global__\n",
        "void MyKernel(int *d, int *a, int *b) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  d[idx] = a[idx] * b[idx];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int numBlocks; // occupancy in terms of active blocks\n",
        "  int blockSize = 32;\n",
        "  // these variables are used to convert occupancy to warps\n",
        "  int device;\n",
        "  cudaDeviceProp prop;\n",
        "  int activeWarps;\n",
        "  int maxWarps;\n",
        "\n",
        "  cudaGetDevice(&device);\n",
        "  cudaGetDeviceProperties(&prop, device);\n",
        "  cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);\n",
        "\n",
        "  activeWarps = numBlocks * blockSize / prop.warpSize;\n",
        "  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n",
        "  std::cout << \"Occupancy: \" << (double)activeWarps / maxWarps * 100 << \"%\" << std::endl;\n",
        "\n",
        " return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "YR82LQChsznK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging CUDA with GDB"
      ],
      "metadata": {
        "id": "M6Fh_k6cn3LP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, everyone needs to debug sometimes! Here's how to do it when CUDA is in the picture as well:"
      ],
      "metadata": {
        "id": "vlXQeg2mn5su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y cuda-toolkit-12-5"
      ],
      "metadata": {
        "id": "BQfIXYGOup5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile red_flag.cpp\n",
        "#include<stdio.h>\n",
        "\n",
        "__global__\n",
        "void susKernel(const float* __restrict__ in, float* __restrict__ out, int n) {\n",
        "  extern __shared__ float s[];\n",
        "  int t_id = threadIdx.x;\n",
        "  int g_t_id = blockIdx.x*blockDim.x + t_id;\n",
        "\n",
        "  s[t_id] = in[g_t_id];\n",
        "  __syncthreads();\n",
        "\n",
        "  for (int stride = blockDim.x; stride > 0; stride /= 2) { // bug: stride should start at blockDim.x/2...\n",
        "    if (t_id < stride)\n",
        "      s[t_id] += s[t_id + stride];\n",
        "    __syncthreads();\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int n = 4096;\n",
        "  float h_in[n], h_out[n];\n",
        "  for (int i = 0; i < n; i++)\n",
        "    h_in[i] = i + 1;\n",
        "\n",
        "  float *d_in, *d_out;\n",
        "  cudaMalloc(&d_in, n*sizeof(float));\n",
        "  cudaMalloc(&d_out, n*sizeof(float));\n",
        "  cudaMemcpy(d_in, h_in, n*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  int blockSize = 256;\n",
        "  int gridSize = (n + blockSize - 1)/blockSize;\n",
        "  size_t shmem = blockSize * sizeof(float);\n",
        "  susKernel<<<gridSize, blockSize, shmem>>>(d_in, d_out, n);\n",
        "  cudaMemcpy(h_out, d_out, n*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzioPSUvoBjP",
        "outputId": "09139885-a920-4a8e-ea69-5c9e160ee4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing red_flag.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv red_flag.cpp red_flag.cu\n",
        "!nvcc -G -g red_flag.cu -o red_flag"
      ],
      "metadata": {
        "id": "zqCS8S0JphVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Colab does give easy access to an interactive console, so we need to \"pre-program\" our debug session...\n",
        "# This simple example just runs the debugger\n",
        "cat > gdbcmds.txt <<'EOF'\n",
        "run\n",
        "quit\n",
        "EOF\n",
        "\n",
        "set +e\n",
        "cuda-gdb --batch --command=gdbcmds.txt ./red_flag\n",
        "echo \"cuda-gdb exited with status $?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myVsa3rjppV7",
        "outputId": "99f1d54d-4475-40f8-d7a1-ee59ee17ce45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Thread debugging using libthread_db enabled]\n",
            "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
            "[New Thread 0x1555535ff000 (LWP 7486)]\n",
            "[New Thread 0x1555527ff000 (LWP 7487)]\n",
            "[Detaching after fork from child process 7488]\n",
            "[New Thread 0x155551383000 (LWP 7497)]\n",
            "[Thread 0x1555527ff000 (LWP 7487) exited]\n",
            "[Thread 0x1555535ff000 (LWP 7486) exited]\n",
            "[Thread 0x1555552cf000 (LWP 7482) exited]\n",
            "[Thread 0x155551383000 (LWP 7497) exited]\n",
            "[New process 7482]\n",
            "[Inferior 1 (process 7482) exited normally]\n",
            "cuda-gdb exited with status 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "warning: Cuda API error detected: cudaLaunchKernel returned (0xde)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aaaaand, this is as far as we can go on Colab.<br>\n",
        "To debug inside the kernel we would need a local machine to run the 'cuda-gdbserver' with access to the GPU.<br>\n",
        "If you are curious, the docs are here: https://docs.nvidia.com/cuda/cuda-gdb/"
      ],
      "metadata": {
        "id": "jArFN2oyuJta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yet Another Simple Exercise on Divergence"
      ],
      "metadata": {
        "id": "5ofCgbmhO5s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A [tiled matrix multiplication](#tiled-mm) kernel has many boundary condition checks, such as the ones to load M tiles in the following code snippet:"
      ],
      "metadata": {
        "id": "8_GHGeH1qEQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define TILE_WIDTH 16\n",
        "\n",
        "//...\n",
        "for (int t = 0; t < (width + TILE_DIM - 1)/TILE_DIM; t++) {\n",
        "  if (row < width && t*TILE_DIM + threadIdx.x < width) {\n",
        "    M_s[threadIdx.y][threadIdx.x] = M[row*width + p*TILE_DIM + threadIdx.x];\n",
        "  } else {\n",
        "    M_s[threadIdx.y][threadIdx.x] = 0.0;\n",
        "  }\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "bWEi4fsDqqfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming 100x100 square matrices, 16x16 square tiles, and 16x16 thread blocks, estimate what will be the overall impact of control divergence in this case.\n",
        "<!--\n",
        "Ah-aH! You wanted a solution huh? Sorry, it's 3am and I am too tired for this...\n",
        "Please try to build some confidence in yourself and trust your own solution.\n",
        "If self-confidence is not an option, try harder.\n",
        "If you already tried your hardest, send me and email and I will come up with the solution then ^-^!\n",
        "-->"
      ],
      "metadata": {
        "id": "SqM850wHrV-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tiled Arbitrary Matmul"
      ],
      "metadata": {
        "id": "XEM4xdZHaP5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of how to handle general rectangular matrices with shared memory.<br>\n",
        "Elements outside the not-fully overlapping tiles become 0.\n",
        "\n",
        "Source: https://stackoverflow.com/a/18856054/12501261\n",
        "\n",
        "*Note: on small matrices, between with or wihout shared memory we will not likely observe any improvement. That is because, for the GPU architecture we are running, the L1 cache already \"does the job\" of the shared memory. Remember that, apart from very old architectures, shared memory can be seen as a controlled cache.*"
      ],
      "metadata": {
        "id": "9HZ7Tp15aTHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile arbitrary_matmul.cpp\n",
        "#include<stdio.h>\n",
        "\n",
        "#define SCALE 1000000.0\n",
        "#define TILE_DIM 10\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// in reality ARows = CRows, ACols = BRows, BCols = CCols, but let's keep the separate for sake of clarity\n",
        "__global__\n",
        "void matMul(float* A, float* B, float* C, int ARows, int ACols, int BRows, int BCols, int CRows, int CCols) {\n",
        "  float CValue = 0;\n",
        "\n",
        "  int Row = blockIdx.y*TILE_DIM + threadIdx.y; // aka 'm'\n",
        "  int Col = blockIdx.x*TILE_DIM + threadIdx.x; // aka 'n'\n",
        "\n",
        "  __shared__ float As[TILE_DIM][TILE_DIM];\n",
        "  __shared__ float Bs[TILE_DIM][TILE_DIM];\n",
        "\n",
        "  // coalesce memory operations through tiled loads in shared memory\n",
        "  for (int k = 0; k < (TILE_DIM + ACols - 1)/TILE_DIM; k++) {\n",
        "    if (k*TILE_DIM + threadIdx.x < ACols && Row < ARows)\n",
        "      As[threadIdx.y][threadIdx.x] = A[Row*ACols + k*TILE_DIM + threadIdx.x];\n",
        "    else\n",
        "      As[threadIdx.y][threadIdx.x] = 0.0;\n",
        "\n",
        "    if (k*TILE_DIM + threadIdx.y < BRows && Col < BCols)\n",
        "      Bs[threadIdx.y][threadIdx.x] = B[(k*TILE_DIM + threadIdx.y)*BCols + Col];\n",
        "    else\n",
        "      Bs[threadIdx.y][threadIdx.x] = 0.0;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // compute\n",
        "    for (int k = 0; k < TILE_DIM; ++k)\n",
        "      CValue += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
        "\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // write result\n",
        "  if (Row < CRows && Col < CCols)\n",
        "    C[Row*CCols + Col] = CValue;\n",
        "}\n",
        "\n",
        "void initMatrixRand(float *M, int r, int c) {\n",
        "  for (int i = 0; i < r; i++)\n",
        "    for (int j = 0; j < c; j++)\n",
        "      M[i*c + j] = (float)rand() / SCALE;\n",
        "}\n",
        "\n",
        "void initMatrixValue(float v, float *M, int r, int c) {\n",
        "  for (int i = 0; i < r; i++)\n",
        "    for (int j = 0; j < c; j++)\n",
        "      M[i*c + j] = v;\n",
        "}\n",
        "\n",
        "bool checkMatMul(float *A, float *B, float *C, int m, int k, int n) {\n",
        "  for (int i = 0; i < m; i++) {\n",
        "    for (int j = 0; j < n; j++) {\n",
        "      float acc = 0.0;\n",
        "      for (int l = 0; l < k; l++)\n",
        "        acc += A[i*k + l] * B[l*n + j];\n",
        "      // floats are not associative, we need to allow some margin!\n",
        "      if (C[i*n + j] > acc*1.001f || C[i*n + j] < acc*0.999f) {\n",
        "        printf(\"Error in position (%d, %d), expected %f, got %f!\\n\", i, j, acc, C[i*n + j]);\n",
        "        return false;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int m = 1000; // ARows = CRows\n",
        "  int k = 500;  // ACols = BRows (inner dimension)\n",
        "  int n = 700;  // BCols = CCols\n",
        "  dim3 blockSize(TILE_DIM, TILE_DIM);\n",
        "  dim3 numBlocks((n + TILE_DIM - 1)/TILE_DIM, (m + TILE_DIM - 1)/TILE_DIM);\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  float *A;\n",
        "  float *B;\n",
        "  float *C;\n",
        "\n",
        "  // possible improvement: store \"B/Bs\" in column-major order, while \"A/As\"\n",
        "  // stays row-major, because the innermost loop is on \"k\"!\n",
        "  cudaMallocManaged(&A, m*k*sizeof(float));\n",
        "  cudaMallocManaged(&B, k*n*sizeof(float));\n",
        "  cudaMallocManaged(&C, m*n*sizeof(float));\n",
        "\n",
        "  initMatrixRand(A, m, k);\n",
        "  initMatrixRand(B, k, n);\n",
        "  initMatrixValue(0.0f, C, m, n);\n",
        "\n",
        "  int device;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  cudaMemPrefetchAsync(A, m*k*sizeof(float), device);\n",
        "  cudaMemPrefetchAsync(B, k*n*sizeof(float), device);\n",
        "  //cudaMemPrefetchAsync(C, m*n*sizeof(float), device);\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  matMul<<<numBlocks, blockSize>>>(A, B, C, m, k, k, n, m, n);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  cudaMemPrefetchAsync(C, m*n*sizeof(float), cudaCpuDeviceId);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  if (checkMatMul(A, B, C, m, k, n))\n",
        "    printf(\"Result OK!\\n\");\n",
        "  else\n",
        "    printf(\"Result NOT OK!\\n\");\n",
        "\n",
        "  printf(\"Execution time (ms): %f \\n\", milli);\n",
        "\n",
        "  cudaEventDestroy(stop);\n",
        "  cudaFree(A);\n",
        "  cudaFree(B);\n",
        "  cudaFree(C);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB3qapxzaTpX",
        "outputId": "c5958cb8-8e2b-4f40-df49-dde2b2682865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing arbitrary_matmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv arbitrary_matmul.cpp arbitrary_matmul.cu\n",
        "!nvcc -arch=sm_75 arbitrary_matmul.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3-W9JeqgVG9",
        "outputId": "aabba46d-9edf-4f30-9fa9-d20c7912f3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result OK!\n",
            "Execution time (ms): 4.256256 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Improbable Divergence Scenario"
      ],
      "metadata": {
        "id": "B01Tt39IzLBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following kernel, estimate what impact will have the serialization due to control divergence."
      ],
      "metadata": {
        "id": "b_evY5a0zREG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "__global__ void loopKernel(float* v, float* o, int n) {\n",
        "  int g_id = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  int delta = g_id % 2;\n",
        "\n",
        "  o[g_id] = 0;\n",
        "  for (int i = -delta; i <= delta; i += 1) {\n",
        "    o[g_id] += 2*v[g_id + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  float v[N], o[N];\n",
        "  //...\n",
        "  loopKernel<<<4, N/4>>>(v, o, N);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "xBLcQulvzRrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is very unlikely, but for the sake of an example, think of it as a weird take on a 1D convolution.\n",
        "\n",
        "How to improve it up is obvious: write two kernels, one for even and one for odd position elements!"
      ],
      "metadata": {
        "id": "X62Z756Dzoxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile only_for_highlight.cpp\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "__global__ void loopKernelEven(float* v, float* o, int n) {\n",
        "  int g_id = (blockIdx.x*blockDim.x + threadIdx.x)*2;\n",
        "\n",
        "  o[g_id] = 0;\n",
        "  for (int i = -1; i <= 1; i += 1) {\n",
        "    o[g_id] += 2*v[g_id + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void loopKernelOdd(float* v, float* o, int n) {\n",
        "  int g_id = (blockIdx.x*blockDim.x + threadIdx.x)*2 + 1;\n",
        "\n",
        "  o[g_id] += 2*v[g_id];\n",
        "}\n",
        "\n",
        "// launched with:\n",
        "int main() {\n",
        "  float v[N], o[N];\n",
        "  //...\n",
        "  loopKernelEven<<<4, N/8>>>(v, o, N);\n",
        "  loopKernelOdd<<<4, N/8>>>(v, o, N);\n",
        "  //...\n",
        "}"
      ],
      "metadata": {
        "id": "5c3vucvrzpBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warp Shuffles for Reduction"
      ],
      "metadata": {
        "id": "zOBN5HqsVx_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us improve the reduction!\n",
        "\n",
        "- classic version: each block moves a part of the input in shared memory, reduces it one logarithmic tree level at a time (with syncs in between), and then does the final reduction across blocks;\n",
        "- shuffle version: each block moves a part of the input in shared memory, each warp reduces internally its set of values, then we go at the block level, and finally across blocks;"
      ],
      "metadata": {
        "id": "UmzsHmb7X1rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduce.cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define WARP_SIZE 32\n",
        "\n",
        "// classic shared-memory reduction kernel\n",
        "__global__\n",
        "void blockReduceClassic(const float* __restrict__ g_in, float* __restrict__ g_out, size_t n) {\n",
        "  extern __shared__ float sdata[];\n",
        "  unsigned int tid = threadIdx.x;\n",
        "  unsigned int idx = blockIdx.x * blockDim.x * 2 + tid;\n",
        "\n",
        "  float val = 0.0f;\n",
        "  if (idx < n)\n",
        "    val = g_in[idx];\n",
        "  if (idx + blockDim.x < n)\n",
        "    val += g_in[idx + blockDim.x];\n",
        "\n",
        "  sdata[tid] = val;\n",
        "  __syncthreads();\n",
        "\n",
        "  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "    if (tid < s) // active threads: first 's' of each block\n",
        "      sdata[tid] += sdata[tid + s];\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  if (tid == 0)\n",
        "    g_out[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// warp shuffle helper\n",
        "__inline__ __device__\n",
        "float warpReduceSum(float val) {\n",
        "  unsigned mask = 0xffffffffu; // all 1s, all active\n",
        "  for (int offset = warpSize / 2; offset > 0; offset >>= 1)\n",
        "    val += __shfl_down_sync(mask, val, offset);\n",
        "  return val;\n",
        "}\n",
        "\n",
        "// shuffle-based reduction kernel\n",
        "__global__\n",
        "void blockReduceShuffle(const float* __restrict__ g_in, float* __restrict__ g_out, size_t n) {\n",
        "  extern __shared__ float sdata[];\n",
        "  unsigned int tid = threadIdx.x;\n",
        "  unsigned int lane = tid % warpSize; // position (lane) inside warp\n",
        "  unsigned int warp_id = tid / warpSize;\n",
        "\n",
        "  size_t idx = (size_t)blockIdx.x * blockDim.x * 2 + tid;\n",
        "\n",
        "  float sum = 0.0f;\n",
        "  if (idx < n)\n",
        "    sum = g_in[idx];\n",
        "  if (idx + blockDim.x < n)\n",
        "    sum += g_in[idx + blockDim.x];\n",
        "\n",
        "  sum = warpReduceSum(sum);\n",
        "\n",
        "  if (lane == 0)\n",
        "    sdata[warp_id] = sum;\n",
        "  __syncthreads();\n",
        "\n",
        "  if (warp_id == 0) {\n",
        "    float val = (lane < (blockDim.x / warpSize)) ? sdata[lane] : 0.0f;\n",
        "    val = warpReduceSum(val);\n",
        "    if (lane == 0)\n",
        "      g_out[blockIdx.x] = val;\n",
        "  }\n",
        "}\n",
        "\n",
        "// shuffle version (host entry point)\n",
        "float reduceShuffle(const float* d_in, size_t n) {\n",
        "  int threads = 1024;\n",
        "  int blocks = (int)((n + threads * 2 - 1) / (threads * 2)); // 2 elements per thread => each thread does 1 sum in the first level\n",
        "  float* d_out;\n",
        "  cudaMalloc(&d_out, blocks * sizeof(float));\n",
        "\n",
        "  size_t shmem = (threads / WARP_SIZE) * sizeof(float);\n",
        "  blockReduceShuffle<<<blocks, threads, shmem>>>(d_in, d_out, n);\n",
        "\n",
        "  float* h_out = (float*)malloc(blocks * sizeof(float));\n",
        "  cudaMemcpy(h_out, d_out, blocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float sum = 0.0f;\n",
        "  for (int i = 0; i < blocks; i++)\n",
        "    sum += h_out[i];\n",
        "\n",
        "  free(h_out);\n",
        "  cudaFree(d_out);\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "// classic version (host entry point)\n",
        "float reduceClassic(const float* d_in, size_t n) {\n",
        "  int threads = 1024;\n",
        "  int blocks = (int)((n + threads * 2 - 1) / (threads * 2));\n",
        "  float* d_out;\n",
        "  cudaMalloc(&d_out, blocks * sizeof(float));\n",
        "\n",
        "  size_t shmem = threads * sizeof(float);\n",
        "  blockReduceClassic<<<blocks, threads, shmem>>>(d_in, d_out, n);\n",
        "\n",
        "  float* h_out = (float*)malloc(blocks * sizeof(float));\n",
        "  cudaMemcpy(h_out, d_out, blocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float sum = 0.0f;\n",
        "  for (int i = 0; i < blocks; i++)\n",
        "    sum += h_out[i];\n",
        "\n",
        "  free(h_out);\n",
        "  cudaFree(d_out);\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  size_t N = 1 << 30;\n",
        "  float* h_in = (float*)malloc(N * sizeof(float));\n",
        "  for (size_t i = 0; i < N; i++)\n",
        "    h_in[i] = 1.0f;\n",
        "\n",
        "  float* d_in;\n",
        "  cudaMalloc(&d_in, N * sizeof(float));\n",
        "  cudaMemcpy(d_in, h_in, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // timing\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  float sumClassic = reduceClassic(d_in, N);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Execution classic time (ms): %f \\n\", milli);\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  float sumShuffle = reduceShuffle(d_in, N);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Execution shuffle time (ms): %f \\n\", milli);\n",
        "\n",
        "  printf(\"Computed values:\\nExpected: %.0f\\n\", (float)N);\n",
        "  printf(\"Classic:  %.0f\\n\", sumClassic);\n",
        "  printf(\"Shuffle:  %.0f\\n\", sumShuffle);\n",
        "\n",
        "  cudaEventDestroy(start);\n",
        "  cudaEventDestroy(stop);\n",
        "  cudaFree(d_in);\n",
        "  free(h_in);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYP4iJOOVxVh",
        "outputId": "adc20539-12cb-4b0b-b60f-5b1b2e0d6535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv reduce.cpp reduce.cu\n",
        "!nvcc -arch=sm_75 reduce.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRKX_DIcXyAM",
        "outputId": "c95964eb-179c-468e-e1fc-20c8b9584831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution classic time (ms): 85.974594 \n",
            "Execution shuffle time (ms): 35.604031 \n",
            "Computed values:\n",
            "Expected: 1073741824\n",
            "Classic:  1073741824\n",
            "Shuffle:  1073741824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jacobi Iteration Method for the Laplace Equation"
      ],
      "metadata": {
        "id": "NIIjR9Mm-9br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Laplace equation is a partial differential equation (PDE) in the form:\n",
        "\n",
        "$\\triangledown^2 u(x, y) = 0\\;\\; \\Leftrightarrow \\;\\;\\frac{\\delta^2d}{\\delta x^2} + \\frac{\\delta^2d}{\\delta y^2} = 0$\n",
        "\n",
        "And arises in many physical contexts, like steady-state heat conduction and fluid flow.\n",
        "\n",
        "We want to solve it numerically, and for that, we discretize $x$ and $y$ in a uniform grid with spacing $h$, and we approximate the laplacian at each interior point via finite differences w.r.t. its neighbors:\n",
        "\n",
        "$\\triangledown^2 u(x_i, y_j) \\approx \\frac{u_{i-1, j} + u_{i+1, j} + u_{i, j-1} + u_{i, j+1} - 4u_{i,j}}{h^2}$\n",
        "\n",
        "Imposing $\\triangledown^2 u(x, y) = 0$ thus gives us a simple dependency of each point on its four neighbors:\n",
        "\n",
        "$u_{i, j} = \\frac{1}{4} (u_{i-1, j} + u_{i+1, j} + u_{i, j-1} + u_{i, j+1})$\n",
        "\n",
        "In other words, we have a solution when the value at each interior point equals the average of its four neighbors.\n",
        "\n",
        "The **Jacobi method** is a simple iterative technique to approximate solutions of linear systems, ad we will use it to find the steady-state above.<br>\n",
        "Applied to the Laplace equation:\n",
        "- start with an initial guess $u^{(0)}$\n",
        "- at each iteration, compute a new solution $u^{(k + 1)}$ by replacing every interior grid point with the average of its neighbors from the previous iteration\n",
        "- keep the boundary values fixed (these are the Dirichlet boundary conditions)\n",
        "- continue until convergence to a steady-state\n",
        "\n"
      ],
      "metadata": {
        "id": "O-wwnpnq_FP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile jacobi.cpp\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "\n",
        "__global__\n",
        "void lap(int I, int J, const float* __restrict__ u1, float* __restrict__ u2) {\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int j = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "  if (i >= I || j >= J)\n",
        "    return;\n",
        "  int id = i + j * I;\n",
        "\n",
        "  if (i == 0 || i == I - 1 || j == 0 || j == J - 1) {\n",
        "  // boundary: Dirichlet condition, keep fixed\n",
        "    u2[id] = u1[id];\n",
        "  } else {\n",
        "    // interior: average of neighbors\n",
        "    u2[id] = 0.25f * (u1[id - 1] + u1[id + 1] +\n",
        "    u1[id - I] + u1[id + I]);\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int I = 128;        // grid width\n",
        "    const int J = 128;        // grid height\n",
        "    const int N = I * J;      // total points\n",
        "    const int max_iter = 500; // fixed iteration count\n",
        "\n",
        "    float *h_u = (float*)malloc(N * sizeof(float));\n",
        "\n",
        "    // initialization: boundary = 0, interior = random values\n",
        "    for (int j = 0; j < J; j++) {\n",
        "        for (int i = 0; i < I; i++) {\n",
        "            int id = i + j * I;\n",
        "            if (i == 0 || i == I-1 || j == 0 || j == J-1)\n",
        "                h_u[id] = 0.0f;\n",
        "            else\n",
        "                h_u[id] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float *d_u1, *d_u2;\n",
        "    cudaMalloc(&d_u1, N * sizeof(float));\n",
        "    cudaMalloc(&d_u2, N * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_u1, h_u, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(16, 16);\n",
        "    dim3 gridDim((I + blockDim.x - 1)/blockDim.x, (J + blockDim.y - 1)/blockDim.y);\n",
        "\n",
        "    // Jacobi iterations\n",
        "    for (int iter = 0; iter < max_iter; iter++) {\n",
        "        lap<<<gridDim, blockDim>>>(I, J, d_u1, d_u2);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // NOTE: we can't update locations in place, since other threads will be using them at the same time!\n",
        "        // => swap pointers (ping-pong)\n",
        "        float* tmp = d_u1;\n",
        "        d_u1 = d_u2;\n",
        "        d_u2 = tmp;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_u, d_u1, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Result snapshot:\\n\");\n",
        "    for (int j = 0; j < 5; j++) {\n",
        "        for (int i = 0; i < 5; i++) {\n",
        "            printf(\"%6.3f \", h_u[i + j * I]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    free(h_u);\n",
        "    cudaFree(d_u1);\n",
        "    cudaFree(d_u2);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "saQ74_uK_EkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv jacobi.cpp jacobi.cu\n",
        "!nvcc -arch=sm_75 jacobi.cu -run"
      ],
      "metadata": {
        "id": "xk_jNtarDCHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra:\n",
        "\n",
        "Check for convergence, every $K$ Jacobi iterations run a second kernel that checks for convergence between 'd_u1' and 'd_u2'.\n",
        "The check simply consist of finding, across all locations, the one with the maximum diference between 'd_u1' and 'd_u2', if that difference is less than a user-define tolerance, then terminate.\n",
        "In other words, this is just a reduce for the maximum value."
      ],
      "metadata": {
        "id": "oKwI49H1FgQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile highlight_only.cpp\n",
        "\n",
        "#define K 10\n",
        "\n",
        "__global__\n",
        "void max_diff(int N, const float* u1, const float* u2, float* d_out) {\n",
        "  __shared__ float sdata[256];\n",
        "  int tid = threadIdx.x;\n",
        "  int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "  float diff = 0.0f;\n",
        "  if (idx < N)\n",
        "    diff = fabsf(u1[idx] - u2[idx]);\n",
        "  sdata[tid] = diff;\n",
        "  __syncthreads();\n",
        "\n",
        "  // this is the classic shared memory reduce\n",
        "  for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "    if (tid < stride) {\n",
        "      if (sdata[tid + stride] > sdata[tid])\n",
        "        sdata[tid] = sdata[tid + stride];\n",
        "    }\n",
        "      __syncthreads();\n",
        "  }\n",
        "\n",
        "  if (tid == 0) d_out[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // ... skipping code from before ...\n",
        "\n",
        "  float tolerance = 1e-4f;\n",
        "  float *d_blockmax, *h_blockmax;\n",
        "  int blocks = (N + 255) / 256;\n",
        "  h_blockmax = (float*)malloc(blocks * sizeof(float));\n",
        "  cudaMalloc(&d_blockmax, blocks * sizeof(float));\n",
        "\n",
        "  for (int iter = 0; iter < max_iter; iter++) {\n",
        "    lap<<<gridDim, blockDim>>>(I, J, d_u1, d_u2);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // every K iterations, convergence check\n",
        "    if (iter % K == 0) {\n",
        "      max_diff<<<blocks, 256>>>(N, d_u1, d_u2, d_blockmax);\n",
        "      cudaMemcpy(h_blockmax, d_blockmax, blocks*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "      // finish the reduce on the host\n",
        "      float maxdiff = 0.0f;\n",
        "      for (int b = 0; b < blocks; b++)\n",
        "        if (h_blockmax[b] > maxdiff)\n",
        "          maxdiff = h_blockmax[b];\n",
        "\n",
        "      if (maxdiff < tolerance) {\n",
        "        printf(\"Converged at iteration %d with diff=%e\\n\", iter, maxdiff);\n",
        "        break;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // swap pointers\n",
        "    float* tmp = d_u1;\n",
        "    d_u1 = d_u2;\n",
        "    d_u2 = tmp;\n",
        "  }\n",
        "\n",
        "  // ... skipping code from before ...\n",
        "}"
      ],
      "metadata": {
        "id": "JDClYpQIGniK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab-to-PDF**"
      ],
      "metadata": {
        "id": "rX37hvYHvNav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to a graveyard of pain and suffering..."
      ],
      "metadata": {
        "id": "ufL9MBGVygt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q \"nbconvert[webpdf]\" playwright\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "id": "h8whq9HUsMJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GXrhZcN3pcT",
        "outputId": "821d523b-0d1f-4631-9b37-7a70925372e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple version\n",
        "\n",
        "# ISSUE: font is too large\n",
        "#!jupyter nbconvert --to webpdf \"CUDA.ipynb\" --allow-chromium-download --embed-images\n",
        "#files.download(\"CUDA.pdf\")\n",
        "\n",
        "# ISSUE: itemizes without an empty before them fail to get converted, same goes for HTML linebreaks \"<br>\"\n",
        "#!jupyter nbconvert --to latex \"CUDA.ipynb\" --embed-images\n",
        "#files.download(\"CUDA.tex\")\n",
        "\n",
        "# ISSUE: wants reveal.js\n",
        "#!jupyter nbconvert --to slides \"CUDA.ipynb\" --embed-images\n",
        "#files.download(\"CUDA.slides.html\")"
      ],
      "metadata": {
        "id": "mXfNH1srvSKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# webpdf version with images included\n",
        "# ISSUE: font is too large\n",
        "\n",
        "import nbformat, os, re, requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "notebook_path = \"CUDA.ipynb\"\n",
        "\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "def download_image(url, folder=\"images\"):\n",
        "    \"\"\"Download any image (even without file extension).\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        name = os.path.basename(urlparse(url).path) or \"image\"\n",
        "        if not re.search(r'\\.\\w{2,4}$', name):\n",
        "            ext = r.headers.get(\"content-type\", \"\").split(\"/\")[-1]\n",
        "            if ext in [\"jpeg\", \"jpg\", \"png\", \"gif\", \"svg\", \"webp\"]:\n",
        "                name += f\".{ext}\"\n",
        "            else:\n",
        "                name += \".png\"\n",
        "        local_path = os.path.join(folder, name)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "        return local_path\n",
        "    except Exception as e:\n",
        "        print(f\"Could not fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "for cell in nb.cells:\n",
        "    if cell.cell_type in (\"markdown\", \"raw\"):\n",
        "        soup = BeautifulSoup(cell.source, \"html.parser\")\n",
        "        modified = False\n",
        "        for img in soup.find_all(\"img\", src=True):\n",
        "            src = img[\"src\"]\n",
        "            if src.startswith(\"http\"):\n",
        "                local = download_image(src)\n",
        "                if local:\n",
        "                    img[\"src\"] = local\n",
        "                    modified = True\n",
        "        if modified:\n",
        "            cell.source = str(soup)\n",
        "\n",
        "local_nb = \"CUDA_embedded.ipynb\"\n",
        "with open(local_nb, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "!jupyter nbconvert --to webpdf \"CUDA_embedded.ipynb\" --allow-chromium-download\n",
        "\n",
        "files.download(\"CUDA_embedded.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "collapsed": true,
        "id": "8zodAan60AZI",
        "outputId": "c66420c3-f6cf-4371-e3da-d5e68d3f8c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "Could not fetch https://polimi365-my.sharepoint.com/:i:/r/personal/10669641_polimi_it/Documents/Images/CUDA_memory_model.PNG?csf=1&web=1&e=JIHi6R: 403 Client Error: FORBIDDEN for url: https://polimi365-my.sharepoint.com/personal/10669641_polimi_it/Documents/Images/CUDA_memory_model.PNG?csf=1&web=1&e=JIHi6R&CID=b54f4138-efcf-4a0b-9852-c03228147079\n",
            "Could not fetch https://polimi365-my.sharepoint.com/:i:/r/personal/10669641_polimi_it/Documents/Images/blocks_over_input.png: 403 Client Error: FORBIDDEN for url: https://polimi365-my.sharepoint.com/personal/10669641_polimi_it/Documents/Images/blocks_over_input.png?CID=0e146fc5-4a2d-4873-b0ac-e3efeb9bbbd0\n",
            "[NbConvertApp] Converting notebook CUDA_embedded.ipynb to webpdf\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 418575 bytes to CUDA_embedded.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d4879ece-5ffa-4c5a-9410-03927a590e1b\", \"CUDA_embedded.pdf\", 418575)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# latex (ALMOST) completely working version\n",
        "\n",
        "import nbformat, re, os, time, requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "notebook_path = \"CUDA.ipynb\"\n",
        "tmp_path = \"/content/CUDA_tmp\"\n",
        "images_dir = \"/content/images\"\n",
        "\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "for cell in nb.cells:\n",
        "    if cell.cell_type == \"markdown\":\n",
        "        text = cell.source\n",
        "\n",
        "        # replace HTML <br> tags with real newlines\n",
        "        text = re.sub(r'<br\\s*/?>', '\\n', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # convert <img> tags to LaTeX \\includegraphics blocks\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "        for img in soup.find_all(\"img\"):\n",
        "            src = img.get(\"src\", \"\")\n",
        "            alt = img.get(\"alt\", \"\")\n",
        "            width = img.get(\"width\")\n",
        "\n",
        "            local_path = None\n",
        "            # try downloading the image\n",
        "            if src.startswith(\"http\"):\n",
        "                try:\n",
        "                    file_name = os.path.basename(src.split(\"?\")[0]) or \"image.png\"\n",
        "                    if not re.search(r'\\.(png|jpg|jpeg|gif|pdf|svg)$', file_name, re.IGNORECASE):\n",
        "                        file_name += \".png\"\n",
        "                    local_path = os.path.join(images_dir, file_name)\n",
        "                    r = requests.get(src, stream=True, timeout=10)\n",
        "                    if r.status_code == 200:\n",
        "                        with open(local_path, \"wb\") as f_img:\n",
        "                            for chunk in r.iter_content(1024):\n",
        "                                f_img.write(chunk)\n",
        "                except Exception as e:\n",
        "                    print(f\"WARNING: could not download image {src}: {e}\")\n",
        "                files.download(local_path)\n",
        "            latex_snippet = \"% Image from: \" + src + \"\\n\\\\begin{center}\\n\\\\includegraphics\"\n",
        "            if width:\n",
        "                try:\n",
        "                    latex_snippet += f\"[width={int(width)}pt]\"\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            if local_path:\n",
        "                latex_snippet += f\"{{{local_path}}}\\n\"\n",
        "            else:\n",
        "                latex_snippet += f\"{{{src}}}\\n\"\n",
        "\n",
        "            if alt:\n",
        "                latex_snippet += f\"\\\\\\\\\\\\textit{{{alt}}}\\n\"\n",
        "            latex_snippet += \"\\\\end{center}\"\n",
        "            img.replace_with(latex_snippet)\n",
        "\n",
        "        text = str(soup)\n",
        "\n",
        "        # ensure blank lines before list items\n",
        "        lines = text.splitlines()\n",
        "        new_lines = []\n",
        "        for i, line in enumerate(lines):\n",
        "            if re.match(r'^\\s*[-*+]\\s', line):\n",
        "                if i > 0 and lines[i-1].strip() != \"\":\n",
        "                    new_lines.append(\"\")\n",
        "            new_lines.append(line)\n",
        "        cell.source = \"\\n\".join(new_lines)\n",
        "\n",
        "with open(tmp_path + \".ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "!jupyter nbconvert --to latex \"{tmp_path}\" --LatexPreprocessor.title \"CUDA Exercises\" --LatexPreprocessor.author_names \"M. Ronzani\"\n",
        "time.sleep(1.5)\n",
        "\n",
        "# replace escaped CUDA symbols\n",
        "with open(tmp_path + \".tex\", \"r\", encoding=\"utf-8\") as f:\n",
        "    tex_content = f.read()\n",
        "tex_content = tex_content.replace(\"\\\\textgreater\\\\textgreater\\\\textgreater\", \">{}>{}>\")\n",
        "tex_content = tex_content.replace(\"\\\\textless\\\\textless\\\\textless\", \"<{}<{}<\")\n",
        "with open(tmp_path + \".tex\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(tex_content)\n",
        "\n",
        "while not os.path.exists(tmp_path + \".tex\"):\n",
        "    print(\"Waiting for file write...\")\n",
        "    time.sleep(2)\n",
        "files.download(tmp_path + \".tex\")\n",
        "\n",
        "os.remove(tmp_path + \".ipynb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "k2zQF48b86-K",
        "outputId": "b06f5095-8540-4e33-f9ce-e4525564a747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/CUDA_tmp.ipynb to latex\n",
            "[NbConvertApp] Writing 249826 bytes to /content/CUDA_tmp.tex\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b52787c5-93d9-4a32-a362-ec3b369414e7\", \"CUDA_tmp.tex\", 249532)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}